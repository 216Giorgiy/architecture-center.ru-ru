---
title: Создание конвейера CI/CD для микрослужб в Kubernetes
description: Описывается конвейер CI/CD с пример развертывания микрослужб для службы Azure Kubernetes (AKS).
author: MikeWasson
ms.date: 04/11/2019
ms.topic: guide
ms.service: architecture-center
ms.subservice: reference-architecture
ms.custom: microservices
ms.openlocfilehash: e7da8a1b4111fb7856b919b40d033833a4e475e0
ms.sourcegitcommit: d58e6b2b891c9c99e951c59f15fce71addcb96b1
ms.translationtype: MT
ms.contentlocale: ru-RU
ms.lasthandoff: 04/12/2019
ms.locfileid: "59533225"
---
<!-- markdownlint-disable MD040 -->

# <a name="building-a-cicd-pipeline-for-microservices-on-kubernetes"></a>Создание конвейера CI/CD для микрослужб в Kubernetes

Он может оказаться сложной задачей для создания надежного процесса CI/CD для архитектуры микрослужб. Отдельные команды должен иметь возможность выпуска службы быстро и надежно, без нарушения работы других команд или нарушать стабильной работы приложения в целом.

В этой статье описывается пример конвейера CI/CD, для развертывания микрослужб для службы Azure Kubernetes (AKS). Каждой команды и проекта отличается, поэтому в этой статье не принимают в качестве набора hard-and-fast правил. Вместо этого она создана для стать отправной точкой для разработки собственного процесса непрерывной Интеграции и Развертывания.

Этот пример конвейера, описанных здесь была создана для реализации микрослужб вызывается приложения доставки с помощью Дронов, который можно найти на [GitHub][ri]. Сценарий приложения описан [здесь](./design/index.md#reference-implementation).

Цели конвейер можно интерпретировать следующим образом:

- Команды можно создавать и развертывать свои службы независимо друг от друга.
- Изменения кода, прошедшие процесс непрерывной Интеграции, автоматически развертываются в рабочей среде.
- Системы контроля качества применяются на каждом этапе конвейера.
- Новую версию службы можно развернуть параллельно с предыдущей версии.

Дополнительные сведения см. в разделе [CI/CD для архитектур микрослужб](./ci-cd.md).

## <a name="assumptions"></a>Предположения

Для целей этого примера ниже приведены некоторые предположения о группе разработчиков и код базового.

- Репозиторий кода — monorepo, с папками микрослужб.
- Стратегия создания ветви команды основывается на [разработке на основе магистрали](https://trunkbaseddevelopment.com/).
- Команды используют [ветвей выпуска](/azure/devops/repos/git/git-branching-guidance?view=azure-devops#manage-releases) к управлению выпусками. Для каждой микрослужбы создании отдельных выпусков.
- Процесс Непрерывной интеграции и использует [конвейеры Azure](/azure/devops/pipelines/?view=azure-devops) для создания, тестирования и развертывания микрослужб для AKS.
- Образы контейнеров для каждой микрослужбы, хранятся в [реестр контейнеров Azure](/azure/container-registry/).
- Группа использует диаграмм Helm для упаковки каждой микрослужбы.

Эти предположения диска многие из конкретных особенностей конвейера CI/CD. Однако базовый подход, описанный здесь быть адаптированы для других процессов, средств и служб, таких как Jenkins или Docker Hub.

## <a name="validation-builds"></a>Проверка сборок

Предположим, что разработчик работает в микрослужбе, вызывает службу доставки. При разработке новых компонентов разработчик проверяет код в ветви компонентов. По соглашению компоненты ветвей носят имя `feature/*`.

![Рабочий процесс CI/CD](./images/aks-cicd-1.png)

Файл определения сборки содержит триггер, который фильтрует, укажите имя ветви и исходный путь:

```yaml
trigger:
  batch: true
  branches:
    include:
    - master
    - feature/*
    - topic/*

    exclude:
    - feature/experimental/*
    - topic/experimental/*

  paths:
     include:
     - /src/shipping/delivery/
```

&#11162;См. в разделе [исходный файл](https://github.com/mspnp/microservices-reference-implementation/blob/master/src/shipping/delivery/azure-pipelines-validation.yml).

Благодаря этому подходу каждая команда может иметь свой собственный конвейер сборки. Только код, который возвращается в `/src/shipping/delivery` папки активирует сборку службы доставки. Отправив фиксации в ветви, соответствующие критерию фильтра активирует сборку непрерывной Интеграции. На этом этапе в рабочем процессе сборка CI выполняет некоторые минимальные проверки кода.

1. Создавайте код.
1. Выполнение модульных тестов.

Целью является для сохранения времени построения короткий, поэтому разработчик может получить быструю обратную связь. Когда компонент готов выполнить слияние в главную ветвь, разработчик Открывает запрос на Вытягивание Этим активируется другая сборка CI, которая выполняет некоторые дополнительные проверки.

1. Создавайте код.
1. Выполнение модульных тестов.
1. Создание образа среды выполнения контейнера.
1. Выполните сканирование уязвимостей в образе.

![Рабочий процесс CI/CD](./images/aks-cicd-2.png)

> [!NOTE]
> В репозитории Azure DevOps, вы можете определить [политики](/azure/devops/repos/git/branch-policies) для защиты ветвей. Например, для слияния в главную ветвь политика может требовать успешную сборку CI, а также утверждение от утверждающего.

## <a name="full-cicd-build"></a>Полная сборка CI/CD

На некотором этапе группа готова к развертыванию новой версии службы доставки. Руководитель выпуска создается ветвь с основного сервера с такому шаблону именования: `release/<microservice name>/<semver>`. Например, `release/delivery/v1.0.2`.

![Рабочий процесс CI/CD](./images/aks-cicd-3.png)

Создание этой ветви триггеров полная сборка непрерывной Интеграции, выполняет все предыдущие шаги плюс:

1. Отправьте образ контейнера в реестр контейнеров Azure. Образ отмечен номером версии, взятым из имени ветви.
2. Запустите `helm package` упаковка чарта Helm для службы. Диаграммы также по значку с номером версии.
3. Передача пакета Helm в реестр контейнеров.

При условии, что эта сборка завершается успешно, запускает процесс развертывания (CD) с помощью конвейеров Azure [конвейер выпуска](/azure/devops/pipelines/release/what-is-release-management). Этот конвейер содержит следующие действия:

1. Развертывание чарта Helm в среде контроля Качества.
1. Утверждающий утверждает, прежде чем пакет переместится в рабочую среду. Дополнительные сведения см. в статье [Release deployment control using approvals](/azure/devops/pipelines/release/approvals/approvals) (Управление развертыванием выпуска с помощью утверждений).
1. Повторной образа Docker для пространства имен production в реестре контейнеров Azure. Например, если текущий маркер – `myrepo.azurecr.io/delivery:v1.0.2`, рабочим маркером будет `myrepo.azurecr.io/prod/delivery:v1.0.2`.
1. Развертывание чарта Helm в рабочую среду.

Даже в monorepo эти задачи могут быть распространены на отдельные микрослужбы, таким образом, чтобы команды можно развернуть с высокой скоростью. Процесс состоит из ряда операций вручную. утверждение PR, создание ветви выпуска и утверждение развертывания в рабочем кластере. Эти действия выполняются вручную политикой &mdash; они можно автоматизировать, если является предпочтительным для организации.

## <a name="isolation-of-environments"></a>Изоляция сред

У вас будет несколько сред, в которых вы развертываете службы, включая среды для разработки, тестирования проверки сборки, интеграционного и нагрузочного тестирования и, наконец, рабочую среду. Эти среды нуждаются в некоторой степени изоляции. В Kubernetes у вас есть выбор между физической и логической изоляцией. Физическая изоляция означает развертывание в отдельных кластерах. При логической изоляции используются пространства имен и политики, как описано выше.

Рекомендуется создать выделенный рабочий кластер вместе с отдельным кластером для сред разработки и тестирования. Используйте логическую изоляцию для разделения сред в кластере разработки и тестирования. У служб, развернутых в кластере разработки и тестирования, никогда не должно быть доступа к хранилищам данных, в которых хранятся бизнес-данные.

## <a name="build-process"></a>Процесс сборки

Если это возможно, упакуйте процесс сборки в контейнер Docker. Который позволяет создавать артефакты кода, с помощью Docker, без обязательной настройки среды сборки на каждом компьютере сборки. Процесс построения контейнерных упрощает масштабное развертывание конвейера непрерывной Интеграции путем добавления новых агентов сборки. Кроме того любой разработчик в группе можно создавать код, просто запустив контейнер сборки.

С помощью многоэтапная сборка в Docker, можно определить среды сборки и образ среды выполнения в одном файле Dockerfile. Например ниже приведен файл Dockerfile, который выполняет построение приложения ASP.NET Core.

```
FROM microsoft/dotnet:2.2-runtime AS base
WORKDIR /app

FROM microsoft/dotnet:2.2-sdk AS build
WORKDIR /src/Fabrikam.Workflow.Service

COPY Fabrikam.Workflow.Service/Fabrikam.Workflow.Service.csproj .
RUN dotnet restore Fabrikam.Workflow.Service.csproj

COPY Fabrikam.Workflow.Service/. .
RUN dotnet build Fabrikam.Workflow.Service.csproj -c Release -o /app

FROM build AS publish
RUN dotnet publish Fabrikam.Workflow.Service.csproj -c Release -o /app

FROM base AS final
WORKDIR /app
COPY --from=publish /app .
ENTRYPOINT ["dotnet", "Fabrikam.Workflow.Service.dll"]
```

&#11162;См. в разделе [исходный файл](https://github.com/mspnp/microservices-reference-implementation/blob/master/src/shipping/workflow/Dockerfile).

Этот файл Dockerfile определяет несколько этапов сборки. Обратите внимание, что рабочей области с именем `base` использует среда выполнения ASP.NET Core, в рабочей области с именем `build` использует полный пакет SDK для ASP.NET Core. `build` Этап служит для построения проекта ASP.NET Core. Но создается контейнер окончательной среды выполнения из `base`, который содержит только среду выполнения и значительно меньше, чем полный образ пакета SDK.

### <a name="building-a-test-runner"></a>Создание тестов

Еще один полезный прием — для запуска модульных тестов в контейнере. Например ниже приведен в файле Docker, который создает средство выполнения тестов:

```
FROM build AS testrunner
WORKDIR /src/tests

COPY Fabrikam.Workflow.Service.Tests/*.csproj .
RUN dotnet restore Fabrikam.Workflow.Service.Tests.csproj

COPY Fabrikam.Workflow.Service.Tests/. .
ENTRYPOINT ["dotnet", "test", "--logger:trx"]
```

Разработчик может использовать этот файл Docker для локального запуска тестов:

```bash
docker build . -t delivery-test:1 --target=testrunner
docker run -p 8080:8080 delivery-test:1
```

Конвейер непрерывной Интеграции также следует выполнить тесты как часть этапа проверки построения.

Обратите внимание, что этот файл используется Docker `ENTRYPOINT` команду, чтобы запустить тесты, не Docker `RUN` команды.

- Если вы используете `RUN` команды выполнения тестов при сборке образа. С помощью `ENTRYPOINT`, тесты входят в. Они выполняются только в том случае, если явно целью `testrunner` рабочей области.
- Непройденного теста не вызывает Docker `build` к сбою команды. Таким образом, можно отличить контейнер сборки ошибки от сбоев тестов.
- На подключенный том можно сохранить результаты теста.

### <a name="container-best-practices"></a>Рекомендации по обеспечению контейнера

Ниже приведены некоторые другие рекомендации для контейнеров.

- Определите действующие для всей организации соглашения для тегов контейнеров, управления версиями и соглашения об именовании ресурсов, развернутых в кластере (модули, службы и т. д.). В результате этого будет легче диагностировать проблемы с развертыванием.

- Во время цикла разработки и тестирования процесс непрерывной интеграции и поставки включает в себя сборку многих образов контейнера. Только некоторые из этих образов являются кандидатами на выпуск, а затем только некоторые из этих релиз-кандидаты будет получить повышаются до рабочей среды. Иметь четкую стратегию управления версиями, вы узнаете, какие образы сейчас развернуты в рабочей среде и выполнить откат до предыдущей версии при необходимости.

- Всегда следует развертывать как теги контейнеров для конкретной версии, не `latest`.

- Используйте [пространства имен](/azure/container-registry/container-registry-best-practices#repository-namespaces) в реестре контейнеров Azure, чтобы изолировать образов, утвержденные для рабочей среды на основе образов, по-прежнему тестируемые. Не перемещайте изображение в рабочей среде пространство имен, пока не будете готовы к его развертыванию в рабочей среде. При использовании такого подхода с семантическим управлением версиями образов контейнеров можно уменьшить вероятность случайного развертывания версии, не утвержденной для выпуска.

- Следуйте принципу наименьших прав доступа, выполнив контейнеры как со стороны непривилегированных пользователей. В Kubernetes, можно создать политику безопасности pod, который предотвращает запуск в качестве контейнеров *корневой*. См. в разделе [предотвратить запуск с правами привилегированного пользователя модулей](https://docs.bitnami.com/kubernetes/how-to/secure-kubernetes-cluster-psp/).

## <a name="helm-charts"></a>Чарты Helm

С помощью Helm можно управлять созданием и развертыванием служб. Вот некоторые возможности Helm, которые помогут в Непрерывной интеграции и:

- Часто одна микрослужба определяется несколько объектов Kubernetes. Helm позволяет этим объектам быть упакована в одной диаграмме Helm.
- Диаграмму можно развернуть с помощью одной команды Helm, а не ряд команд kubectl.
- Диаграммы, явным образом. Используйте Helm для выпустить версию и просмотреть выпуски откат к предыдущей версии. Отслеживание обновлений и исправлений с использованием семантических версий, а также возможность отката к предыдущей версии.
- Чарты helm используйте шаблоны, чтобы избежать дублирующейся информации, таких как метки и селекторы, в нескольких файлах.
- Helm можно управлять зависимостями между диаграммы.
- Диаграммы можно хранить в репозитории Helm, например реестр контейнеров Azure и интегрировать в конвейер сборки.

Дополнительные сведения об использовании Реестра контейнеров Helm см. в статье [Использование Реестра контейнеров Azure в качестве репозитория Helm для диаграмм приложения](/azure/container-registry/container-registry-helm-repos).

Одна микрослужба может включать несколько файлов конфигурации Kubernetes. Обновление службы может означать касается всех этих файлов для обновления селекторы, метки и теги изображений. Helm обрабатывает их как единый пакет диаграмму и дает возможность легко обновить файлы yaml-ФАЙЛ с помощью переменных. Helm использует язык шаблона (на основе Go шаблонов) позволяет создавать параметризованные файлы конфигурации YAML.

Например Вот часть yaml-файл, который определяет развертывания:

```yaml
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: {{ include "package.fullname" . | replace "." "" }}
  labels:
    app.kubernetes.io/name: {{ include "package.name" . }}
    app.kubernetes.io/instance: {{ .Release.Name }}
  annotations:
    kubernetes.io/change-cause: {{ .Values.reason }}

...

  spec:
      containers:
      - name: &package-container_name fabrikam-package
        image: {{ .Values.dockerregistry }}/{{ .Values.image.repository }}:{{ .Values.image.tag }}
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        env:
        - name: LOG_LEVEL
          value: {{ .Values.log.level }}
```

&#11162;См. в разделе [исходный файл](https://github.com/mspnp/microservices-reference-implementation/blob/master/charts/package/templates/package-deploy.yaml).

Вы увидите, что имя развертывания, метки и контейнер по спецификациям все параметры шаблона использования, которые предоставляются во время развертывания. Например из командной строки:

```bash
helm install $HELM_CHARTS/package/ \
     --set image.tag=0.1.0 \
     --set image.repository=package \
     --set dockerregistry=$ACR_SERVER \
     --namespace backend \
     --name package-v0.1.0
```

Несмотря на то, что ваш конвейер CI/CD можно установить диаграммы непосредственно в Kubernetes, рекомендуется создать в архиве диаграммы (TGZ-файла) и помещает диаграммы в репозиторий Helm, такие как реестр контейнеров Azure. Дополнительные сведения см. в разделе [приложения на основе Docker пакета в чарты Helm в Azure конвейерах](/azure/devops/pipelines/languages/helm?view=azure-devops).

Рассмотрите возможность развертывания Helm в своем собственном пространстве имен и с помощью управления доступом на основе ролей (RBAC) для ограничения пространства имен которого его можно развернуть. Дополнительные сведения см. в разделе [управления доступом на основе роли](https://helm.sh/docs/using_helm/#helm-and-role-based-access-control) в документации по Helm.

### <a name="revisions"></a>Исправления

Диаграммы helm всегда имеют номер версии, который необходимо использовать [семантического управления версиями](https://semver.org/). Диаграмма может также иметь `appVersion`. Это поле является необязательным и не должен быть связан с версии диаграммы. Некоторые команды может возникнуть необходимость версии приложения отдельно от обновлений к диаграммам. Но более простой подход использовать один номер версии, поэтому отношение 1:1 между диаграммы версии и версии приложения. Таким образом, можно хранить одну диаграмму по выпускам продукта и легко развернуть нужного выпуска:

```bash
helm install <package-chart-name> --version <desiredVersion>
```

Рекомендуется также для предоставления аннотацию причина изменения в шаблон развертывания:

```yaml
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: {{ include "delivery.fullname" . | replace "." "" }}
  labels:
     ...
  annotations:
    kubernetes.io/change-cause: {{ .Values.reason }}
```

Это позволяет просматривать поле Причина изменений для каждой версии с помощью `kubectl rollout history` команды. В предыдущем примере причину изменения предоставляется как параметр диаграмма Helm.

```bash
$ kubectl rollout history deployments/delivery-v010 -n backend
deployment.extensions/delivery-v010
REVISION  CHANGE-CAUSE
1         Initial deployment
```

Можно также использовать `helm list` команду, чтобы просмотреть журнал изменений:

```bash
~$ helm list
NAME            REVISION    UPDATED                     STATUS        CHART            APP VERSION     NAMESPACE
delivery-v0.1.0 1           Sun Apr  7 00:25:30 2019    DEPLOYED      delivery-v0.1.0  v0.1.0          backend
```

> [!TIP]
> Используйте `--history-max` флаг при инициализации Helm. Этот параметр ограничивает число исправлений, которые Tiller сохраняет в журнале. Tiller сохраняет журнал редакций в configmaps. Если часто выпускаем обновления, если не ограничить размер журнала configmaps может стать очень большим.

## <a name="azure-devops-pipeline"></a>Azure конвейера DevOps

В конвейерах Azure делятся на конвейеры *создавать конвейеры* и *конвейеры выпуска*. Конвейер сборки выполняется процесс непрерывной Интеграции и создает артефакты сборки. Для архитектуры микрослужб в Kubernetes эти артефакты — это образы контейнеров и чарты Helm, которые определяют каждой микрослужбы. Этот процесс компакт-диска, который развертывает микрослужбы в кластере выполнение конвейера выпуска.

Исходя из потока непрерывной Интеграции, описанные ранее в этой статье, конвейер сборки может состоять из следующих задач:

1. Создание контейнера средство выполнения тестов.

    ```yaml
    - task: Docker@1
      inputs:
        azureSubscriptionEndpoint: $(AzureSubscription)
        azureContainerRegistry: $(AzureContainerRegistry)
        arguments: '--pull --target testrunner'
        dockerFile: $(System.DefaultWorkingDirectory)/$(dockerFileName)
        imageName: '$(imageName)-test'
    ```

1. Выполните тесты, путем вызова в контейнере средство выполнения тестов "run" docker.

    ```yaml
    - task: Docker@1
      inputs:
        azureSubscriptionEndpoint: $(AzureSubscription)
        azureContainerRegistry: $(AzureContainerRegistry)
        command: 'run'
        containerName: testrunner
        volumes: '$(System.DefaultWorkingDirectory)/TestResults:/app/tests/TestResults'
        imageName: '$(imageName)-test'
        runInBackground: false
    ```

1. Публиковать результаты тестов. См. в разделе [интеграции сборки и задачи тестирования](/azure/devops/pipelines/languages/docker?view=azure-devops&tabs=yaml#integrate-build-and-test-tasks).

    ```yaml
    - task: PublishTestResults@2
      inputs:
        testResultsFormat: 'VSTest'
        testResultsFiles: 'TestResults/*.trx'
        searchFolder: '$(System.DefaultWorkingDirectory)'
        publishRunAttachments: true
    ```

1. Сборки среды выполнения контейнера.

    ```yaml
    - task: Docker@1
      inputs:
        azureSubscriptionEndpoint: $(AzureSubscription)
        azureContainerRegistry: $(AzureContainerRegistry)
        dockerFile: $(System.DefaultWorkingDirectory)/$(dockerFileName)
        includeLatestTag: false
        imageName: '$(imageName)'
    ```

1. Отправьте контейнер для реестра контейнеров Azure (или других реестра контейнеров).

    ```yaml
    - task: Docker@1
      inputs:
        azureSubscriptionEndpoint: $(AzureSubscription)
        azureContainerRegistry: $(AzureContainerRegistry)
        command: 'Push an image'
        imageName: '$(imageName)'
        includeSourceTags: false
    ```

1. Пакет чарта Helm.

    ```yaml
    - task: HelmDeploy@0
      inputs:
        command: package
        chartPath: $(chartPath)
        chartVersion: $(Build.SourceBranchName)
        arguments: '--app-version $(Build.SourceBranchName)'
    ```

1. Отправьте пакет Helm в реестр контейнеров Azure (или другой репозиторий Helm).

    ```yaml
    task: AzureCLI@1
      inputs:
        azureSubscription: $(AzureSubscription)
        scriptLocation: inlineScript
        inlineScript: |
        az acr helm push $(System.ArtifactsDirectory)/$(repositoryName)-$(Build.SourceBranchName).tgz --name $(AzureContainerRegistry);
    ```

&#11162;См. в разделе [исходный файл](https://github.com/mspnp/microservices-reference-implementation/blob/master/src/shipping/delivery/azure-pipelines-ci.yml).

Выходные данные из конвейера непрерывной Интеграции — это образ контейнера готовый к выпуску и обновленного чарта Helm для микрослужбы. На этом этапе конвейера выпуска можно перехватить. Он выполняет следующие действия:

- Развертывание в средах разработки, контроля Качества и промежуточной.
- Дождитесь утверждающему утвердить или отклонить развертывание.
- Повторной образ контейнера для выпуска
- Передача тегом выпуска в реестр контейнеров.
- Обновите диаграмму Helm в рабочем кластере.

Дополнительные сведения о создании конвейера выпуска, см. в разделе [конвейеры выпуска черновик выпусков и вариантов выпуска](/azure/devops/pipelines/release/?view=azure-devops).

В примере ниже показан процесс Непрерывной интеграции и end-to-end, описанный в этой статье:

![CD/Непрерывной поставки](./images/aks-cicd-flow.png)

## <a name="next-steps"></a>Дальнейшие действия

В этой статье был основан на реализации, которую можно найти на [GitHub][ri].

[ri]: https://github.com/mspnp/microservices-reference-implementation