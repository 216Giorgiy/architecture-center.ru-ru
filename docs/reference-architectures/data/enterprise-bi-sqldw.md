---
title: Корпоративная бизнес-аналитика
titleSuffix: Azure Reference Architectures
description: Использование Azure для получения бизнес-информацию на основе реляционных данных, хранимых локально.
author: MikeWasson
ms.date: 11/06/2018
ms.custom: seodec18
ms.openlocfilehash: 656bf6f1bd342856fd8a2d2aa0b62a9dd4d4f87f
ms.sourcegitcommit: 88a68c7e9b6b772172b7faa4b9fd9c061a9f7e9d
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 12/08/2018
ms.locfileid: "53120090"
---
# <a name="enterprise-bi-in-azure-with-sql-data-warehouse"></a>Корпоративная бизнес-аналитика в Azure с использованием Хранилища данных SQL

Эта эталонная архитектура реализует конвейер [ELT](../../data-guide/relational-data/etl.md#extract-load-and-transform-elt) (извлечение — загрузка — преобразование), который перемещает данные из локальной базы данных SQL Server в хранилище данных SQL и преобразует данные для анализа.

Эталонную реализацию для этой архитектуры можно найти на сайте [GitHub][github-folder].

![Схема архитектуры для корпоративной бизнес-аналитики в Azure с использованием Хранилища данных SQL](./images/enterprise-bi-sqldw.png)

**Сценарий**. Организация имеет большой набор данных OLTP, хранящийся локально в базе данных SQL Server. Организация хочет использовать хранилище данных SQL для проведения анализа с использованием Power BI.

Эта эталонная архитектура предназначена для однократных заданий или заданий по требованию. Если необходимо регулярно перемещать данные (ежечасно или ежедневно), рекомендуется использовать фабрику данных Azure для автоматизации рабочего процесса. Описание эталонной архитектуры, в которой используется Фабрика данных, см. в статье [Автоматизированная корпоративная бизнес-аналитика с использованием Хранилища данных SQL и Фабрики данных Azure][adf-ra].

## <a name="architecture"></a>Архитектура

Архитектура состоит из следующих компонентов:

### <a name="data-source"></a>Источник данных

**SQL Server.** Исходные данные размещаются локально в базе данных SQL Server. Чтобы имитировать локальную среду, скрипты развертывания для этой архитектуры предоставляют виртуальную машину в Azure с установленной платформой SQL Server. В качестве исходных данных используется [пример базы данных OLTP Wide World Importers][wwi].

### <a name="ingestion-and-data-storage"></a>Прием и хранение данных

**Хранилище больших двоичных объектов**. Хранилище больших двоичных объектов используется в качестве промежуточной области для копирования данных перед загрузкой в хранилище данных SQL.

**Хранилище данных Azure SQL.** [Хранилище данных SQL](/azure/sql-data-warehouse/) — распределенная система, предназначенная для анализа больших объемов данных. Она поддерживает массовую параллельную обработку (MPP), что делает ее пригодной для запуска высокопроизводительной аналитики.

### <a name="analysis-and-reporting"></a>Анализ и создание отчетов

**Службы Azure Analysis Services**. [Analysis Services](/azure/analysis-services/) — полностью управляемая служба, которая предоставляет возможности моделирования данных. Используйте службу Analysis Services для создания семантической модели, которую могут запрашивать пользователи. Служба Analysis Services особенно полезна в сценарии информационной панели бизнес-аналитики. В этой архитектуре служба Analysis Services считывает данные из хранилища данных для обработки семантической модели и эффективно обслуживает запросы информационной панели. Она также поддерживает гибкий параллелизм путем масштабирования реплик, чтобы быстрее обрабатывать запросы.

В настоящее время Azure Analysis Services поддерживает табличные модели, но не поддерживает многомерные. В табличных моделях используются конструкции реляционного моделирования (таблицы и столбцы), тогда как в многомерных моделях используются моделирующие конструкции OLAP (кубы, размеры и меры). Если требуются многомерные модели, используйте SQL Server Analysis Services (SSAS). Дополнительные сведения см. в разделе [Сравнение табличных и многомерных решений](/sql/analysis-services/comparing-tabular-and-multidimensional-solutions-ssas).

**Power BI**. Power BI — набор средств бизнес-аналитики для анализа информации о бизнесе. В этой архитектуре он запрашивает семантическую модель, хранящуюся в службе Analysis Services.

### <a name="authentication"></a>Authentication

**Azure Active Directory** (Azure AD) аутентифицирует пользователей, которые подключаются к серверу Analysis Services через Power BI.

## <a name="data-pipeline"></a>Конвейер данных

В этой эталонной архитектуре в качестве источника данных используется база данных [WorldWideImporters](/sql/sample/world-wide-importers/wide-world-importers-oltp-database). Конвейер данных состоит из перечисленных ниже этапов.

1. Экспортирование данных из SQL Server в неструктурированные файлы (утилита bcp).
2. Копирование неструктурированных файлов в хранилище больших двоичных объектов Azure (AzCopy)
3. Загрузка данных в хранилище данных SQL (PolyBase).
4. Преобразование данных в схему типа "звезда" (T-SQL).
5. Загрузка семантической модели в службу Analysis Services (средства SQL Server Data Tools).

![Схема конвейера корпоративной бизнес-аналитики](./images/enterprise-bi-sqldw-pipeline.png)

> [!NOTE]
> Для шагов 1 &ndash; 3 рассмотрите использование Redgate Data Platform Studio. Так как к Data Platform Studio применены наиболее подходящие оптимизации и исправления совместимости, это самый быстрый способ начать работу с хранилищем данных SQL. Дополнительную информацию см. в разделе [Загрузка данных с помощью Redgate Data Platform Studio](/azure/sql-data-warehouse/sql-data-warehouse-load-with-redgate).
>

Следующие разделы описывают эти этапы более подробно.

### <a name="export-data-from-sql-server"></a>Экспорт данных из SQL Server

Программа массового копирования [bcp](/sql/tools/bcp-utility) — это быстрый способ создания неструктурированных текстовых файлов из таблиц SQL. На этом шаге выбираются столбцы, которые нужно экспортировать, но данные не надо преобразовывать. Любые преобразования данных должны происходить в хранилище данных SQL.

**рекомендации**;

Если возможно, планируйте извлечение данных в часы с наименьшей загрузкой, чтобы свести к минимуму конфликт ресурсов в рабочей среде.

Избегайте запуска bcp на сервере базы данных. Вместо этого запустите его с другого компьютера. Записывайте файлы на локальный диск. Убедитесь, что имеется достаточно ресурсов ввода-вывода для обработки параллельных операций записи. Для обеспечения максимальной производительности экспортируйте файлы в специализированные высокоскоростные хранилища.

Вы можете ускорить сетевой перенос, сохранив экспортированные данные в сжатом формате Gzip. Однако загрузка сжатых файлов в хранилище происходит медленнее, чем загрузка несжатых файлов, поэтому существует компромисс между более быстрой передачей сети и более быстрой загрузкой. Если решено использовать сжатие Gzip, не создавайте отдельные файлы Gzip. Вместо этого разделите данные на несколько сжатых файлов.

### <a name="copy-flat-files-into-blob-storage"></a>Копирование неструктурированных файлов в хранилище больших двоичных объектов

Утилита [AzCopy](/azure/storage/common/storage-use-azcopy) предназначена для высокопроизводительного копирования данных в хранилище больших двоичных объектов Azure.

**рекомендации**;

Создайте учетную запись хранения в регионе, расположенном рядом с исходными данными. Разверните учетную запись хранения и экземпляр хранилища данных SQL в том же регионе.

Не запускайте AzCopy на том же компьютере, на котором выполняются производственные рабочие нагрузки, поскольку потребление процессора и ввода-вывода может влиять на рабочую нагрузку.

Сначала проверьте загрузку, чтобы изучить скорость загрузки. Чтобы указать количество параллельных операций копирования, в AzCopy можно использовать параметр /NC. Чтобы настроить производительность, начните со значения по умолчанию и затем поэкспериментируйте с этим параметром. В среде с низкой пропускной способностью слишком много параллельных операций могут привести к сбою сетевого соединения и не допустить успешного завершения операций.

AzCopy перемещает данные в хранилище через общий доступ в Интернет. Если это недостаточно быстро, рассмотрите создание схемы [ExpressRoute](/azure/expressroute/). ExpressRoute — это служба, которая направляет данные с помощью выделенного частного подключения в Azure. Если ваше сетевое соединение слишком медленное, то другой вариант заключается в физической отправке данных на диск в центр данных Azure. Дополнительные сведения см. в разделе [Передача данных в Azure и обратно](/azure/architecture/data-guide/scenarios/data-transfer).

Во время операции копирования AzCopy создает временный файл журнала, который позволяет AzCopy перезапустить операцию, если она прервется (например, из-за сетевой ошибки). Убедитесь, что для хранения файлов журнала на диске достаточно свободного места. Чтобы указать, где записываются файлы журнала, можно использовать параметр /Z.

### <a name="load-data-into-sql-data-warehouse"></a>Загрузка данных в хранилище данных SQL

Используйте [PolyBase](/sql/relational-databases/polybase/polybase-guide), чтобы загрузить файлы из хранилища больших двоичных объектов в хранилище данных. PolyBase предназначен для использования архитектуры MPP (массовой параллельной обработки) хранилища данных SQL, которая делает ее самым быстрым способом загрузки данных в хранилище данных SQL.

Загрузка данных — это двухэтапный процесс:

1. Создание набора внешних таблиц для данных. Внешняя таблица — это определение таблицы, которое указывает на данные, хранящиеся вне хранилища &mdash; в этом случае, неструктурированные файлы в хранилище памяти. Этот этап не перемещает никаких данных в хранилище.
2. Создание промежуточных таблиц и загрузка данных в промежуточные таблицы. Этот этап копирует данные в хранилище.

**рекомендации**;

Если имеется большой объем данных (более 1 ТБ) и используется рабочая нагрузка аналитики, которая выиграет от параллелизма, то рассмотрите возможности хранилища данных SQL. Хранилище данных SQL не подходит для рабочих нагрузок OLTP или малых наборов данных (<250 ГБ). Для наборов данных менее 250 ГБ рассмотрите базу данных Azure SQL или SQL Server. Дополнительные сведения см. в разделе [Хранение данных](../../data-guide/relational-data/data-warehousing.md).

Создайте промежуточные таблицы в виде таблиц без кластеризованных индексов, которые не индексируются. Запросы, которые создают производственные таблицы, приведут к полному сканированию таблицы, поэтому нет причин индексировать промежуточные таблицы.

PolyBase автоматически использует преимущества параллелизма в хранилище. Производительность нагрузки масштабируется при увеличении DWU. Для достижения оптимальной производительности используйте операции одинарной загрузки. Разбивка входных данных на фрагменты и выполнение нескольких одновременных нагрузок не повышает производительность.

PolyBase может читать данные, сжатые с помощью GZip. Тем не менее для сжатого файла используется только один модуль чтения, потому что распаковка файла является однопоточной операцией. Поэтому следует избегать загрузки одного большого сжатого файла. Вместо этого разбейте данные на несколько сжатых файлов, чтобы воспользоваться преимуществами параллелизма. 

Следует учитывать следующие ограничения.

- PolyBase поддерживает максимальный размер столбца из `varchar(8000)`, `nvarchar(4000)` или `varbinary(8000)`. Если имеются данные, превышающие эти лимиты, то при экспорте следует разбить данные на блоки, а затем собрать эти блоки после импорта.

- PolyBase использует фиксированный признак конца строки \n или новой строки. Если в исходных данных появляются символы новой строки, это может вызвать проблемы.

- Схема исходных данных может содержать типы данных, которые не поддерживаются в хранилище данных SQL.

Чтобы обойти эти ограничения, можно создать хранимую процедуру, которая выполнит необходимые преобразования. При запуске bcp найдите по ссылке эту хранимую процедуру. Как альтернатива [Redgate Data Platform Studio](/azure/sql-data-warehouse/sql-data-warehouse-load-with-redgate) автоматически преобразует типы данных, которые не поддерживаются в хранилище данных SQL.

Дополнительные сведения см. в следующих статьях:

- [Рекомендации по загрузке данных в хранилище данных SQL Azure](/azure/sql-data-warehouse/guidance-for-loading-data).
- [Перенос схем в хранилище данных SQL](/azure/sql-data-warehouse/sql-data-warehouse-migrate-schema).
- [Руководство по определению типов данных для таблиц в хранилище данных SQL](/azure/sql-data-warehouse/sql-data-warehouse-tables-data-types).

### <a name="transform-the-data"></a>Преобразование данных

Преобразование данных и перемещение их в производственные таблицы. На этом этапе данные преобразуются в схему типа "звезда" с таблицами измерений и таблицами фактов, подходящими для семантического моделирования.

Создайте рабочие таблицы с кластерными индексами columnstore, которые обеспечивают наилучшую общую производительность запросов. Индексы columnstore оптимизированы для запросов, которые сканируют большое количество записей. Индексы columnstore также не работают для отдельных поисковых запросов (которые просматривают одну строку). Если необходимо выполнять частые отдельные запросы, можно добавить некластеризованный индекс в таблицу. Выполнение отдельных запросов происходит значительно быстрее с помощью некластеризованного индекса. Однако отдельный поиск обычно менее распространен в сценариях хранилища данных, чем рабочие нагрузки OLTP. Дополнительные сведения см. в разделе [Индексирование таблиц в хранилище данных SQL](/azure/sql-data-warehouse/sql-data-warehouse-tables-index).

> [!NOTE]
> Кластерные таблицы columnstore не поддерживают типы данных `varchar(max)`, `nvarchar(max)` или `varbinary(max)`. В этом случае рассмотрим кучу или кластеризованный индекс. Эти столбцы можно поместить в отдельную таблицу.

Поскольку база данных примеров не очень велика, были созданы реплицированные таблицы без разделов. Для производственных нагрузок использование распределенных таблиц, вероятно, улучшит производительность запросов. См. [Руководство по проектированию распределенных таблиц в хранилище данных SQL Azure](/azure/sql-data-warehouse/sql-data-warehouse-tables-distribute). В примерах скрипты запускают запросы, используя статический [класс ресурсов](/azure/sql-data-warehouse/resource-classes-for-workload-management).

### <a name="load-the-semantic-model"></a>Загрузка семантической модели

Загрузка данных в табличную модель в Azure Analysis Services. На этом этапе создается семантическая модель данных с помощью средства SQL Server Data Tools (SSDT). Можно также создать модель путем импорта из файла Power BI Desktop. Поскольку хранилище данных SQL не поддерживает внешние ключи, необходимо добавить связи для семантической модели, чтобы можно было присоединиться к таблицам.

### <a name="use-power-bi-to-visualize-the-data"></a>Использование Power BI для визуализации данных

Существует два варианта подключения Power BI к Azure Analysis Services:

- Импорт. Данные импортируются в модель Power BI.
- Активное подключение. Данные извлекаются непосредственно из Analysis Services.

Корпорация Майкрософт рекомендует активное подключение, так как оно не требует копирования данных в модели Power BI. Кроме того, использование DirectQuery гарантирует, что результаты всегда соответствуют последним исходным данным. Дополнительные сведения см. в разделе [Подключение с помощью Power BI](/azure/analysis-services/analysis-services-connect-pbi).

**рекомендации**;

Избегайте запуска запросов панели управления бизнес-аналитики непосредственно к хранилищу данных. Информационные панели бизнес-аналитики требуют очень малого времени отклика, которого может не хватить на прямые запросы к хранилищу. Кроме того, при обновлении информационной панели будет учитываться количество одновременных запросов, что может повлиять на производительность.

Azure Analysis Services предназначена для обработки требований к запросу информационной панели бизнес-аналитики, поэтому рекомендуемая практика заключается в том, чтобы запрашивать Analysis Services из Power BI.

## <a name="scalability-considerations"></a>Вопросы масштабируемости

### <a name="sql-data-warehouse"></a>Хранилище данных SQL

С хранилищем данных SQL можно по требованию масштабировать вычислительные ресурсы. Механизм запросов оптимизирует запросы для параллельной обработки на основе количества вычислительных узлов и перемещает данные между узлами по мере необходимости. Дополнительные сведения см. в разделе [Управление вычислительными ресурсами в хранилище данных SQL Azure](/azure/sql-data-warehouse/sql-data-warehouse-manage-compute-overview).

### <a name="analysis-services"></a>Analysis Services

Для рабочих нагрузок рекомендуется уровень "Стандартный" для служб Azure Analysis Services, так как он поддерживает секционирование и DirectQuery. В пределах уровня размер экземпляра определяет память и мощность обработки. Вычислительная мощность измеряется в единицах обработки запроса (QPUs). Контролируйте использование QPU, чтобы выбрать необходимый размер. Дополнительные сведения см. в разделе [Мониторинг производительности сервера](/azure/analysis-services/analysis-services-monitor).

При высокой нагрузке производительность запросов может ухудшиться из-за параллелизма запросов. Чтобы одновременно выполнять больше запросов, можно масштабировать службы Analysis Services, создавая пул реплик для обработки запросов. Работа по обработке модели данных всегда происходит на основном сервере. Основной сервер по умолчанию также обрабатывает запросы. При необходимости можно назначить сервер-источник исключительно для обработки, чтобы пул запросов обрабатывал все запросы. В случае высоких требований к обработке нужно отделить обработку от пула запросов. В случае высокой нагрузки запросов и относительно несложной обработки можно включить сервер-источник в пул запросов. Дополнительные сведения см. в разделе [Горизонтальное масштабирование служб Azure Analysis Services](/azure/analysis-services/analysis-services-scale-out).

Чтобы уменьшить объем ненужной обработки, рассмотрите возможность использования секций для разделения табличной модели на логические части. Каждая секция может обрабатываться отдельно. Дополнительные сведения см. в разделе [Секция](/sql/analysis-services/tabular-models/partitions-ssas-tabular).

## <a name="security-considerations"></a>Вопросы безопасности

### <a name="ip-whitelisting-of-analysis-services-clients"></a>Список разрешенных IP-адресов клиентов службы Analysis Services

Рассмотрите возможность использования функции брандмауэра службы Analysis Services белого списка клиентских IP-адресов. Если параметр включен, брандмауэр блокирует все клиентские соединения, которые отличаются от указанных в правилах брандмауэра. Стандартные правила присваивают белый список службе Power BI, но при необходимости можно отключить это правило. Дополнительные сведения см. в разделе [Усиление защиты Azure Analysis Services благодаря новым возможностям брандмауэра](https://azure.microsoft.com/blog/hardening-azure-analysis-services-with-the-new-firewall-capability/).

### <a name="authorization"></a>Авторизация

Azure Analysis Services использует Azure Active Directory (Azure AD) для аутентификации пользователей, подключающихся к серверу служб Analysis Services. Создавая роли и затем назначая их пользователям или группам Azure AD, можно ограничить данные, которые может просматривать конкретный пользователь. Для каждой роли можно сделать следующее. 

- Защитить таблицы или отдельные столбцы. 
- Защитить отдельные строки на основе выражения фильтра. 

Дополнительные сведения см. в разделе [Управление ролями и пользователями базы данных](/azure/analysis-services/analysis-services-database-users).

## <a name="deploy-the-solution"></a>Развертывание решения

Чтобы выполнить развертывание и запуск эталонной реализации, выполните действия, описанные в [файле сведений на GitHub][github-folder]. Он позволяет развернуть следующее:

- Виртуальную машину Windows для имитации локального сервера базы данных. Она включает SQL Server 2017 и связанные с ним инструменты, а также Power BI Desktop.
- Учетная запись хранения Azure, которая обеспечивает хранилище больших двоичных объектов для хранения данных, экспортированных из базы данных SQL Server.
- Экземпляр хранилища данных SQL Azure.
- Экземпляр службы Azure Analysis Services.

## <a name="next-steps"></a>Дополнительная информация

- Для автоматизации этого конвейера используйте Фабрика данных Azure. См. сведения об [автоматизированной корпоративной бизнес-аналитике с использованием Хранилища данных SQL и Фабрики данных Azure][adf-ra].

<!-- links -->

[adf-ra]: ./enterprise-bi-adf.md
[github-folder]: https://github.com/mspnp/reference-architectures/tree/master/data/enterprise_bi_sqldw
[wwi]: /sql/sample/world-wide-importers/wide-world-importers-oltp-database
