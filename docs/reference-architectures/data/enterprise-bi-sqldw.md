---
title: Корпоративная бизнес-аналитика с использованием хранилища данных SQL
description: Используйте Azure для получения информации о бизнесе из реляционных данных, хранящихся локально
author: MikeWasson
ms.date: 07/01/2018
ms.openlocfilehash: e3542e40b4b6d1f604f93bb21528f34ba7f22fc6
ms.sourcegitcommit: 58d93e7ac9a6d44d5668a187a6827d7cd4f5a34d
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 07/02/2018
ms.locfileid: "37142341"
---
# <a name="enterprise-bi-with-sql-data-warehouse"></a>Корпоративная бизнес-аналитика с использованием хранилища данных SQL

Эта эталонная архитектура реализует конвейер [ELT](../../data-guide/relational-data/etl.md#extract-load-and-transform-elt) (извлечение-загрузка-преобразование), который перемещает данные из локальной базы данных SQL Server в хранилище данных SQL и преобразует данные для анализа. [**Разверните это решение**.](#deploy-the-solution)

![](./images/enterprise-bi-sqldw.png)

**Сценарий**. Организация имеет большой набор данных OLTP, хранящийся локально в базе данных SQL Server. Организация хочет использовать хранилище данных SQL для проведения анализа с использованием Power BI. 

Эта эталонная архитектура предназначена для однократных заданий или заданий по требованию. Если необходимо регулярно перемещать данные (ежечасно или ежедневно), рекомендуется использовать фабрику данных Azure для автоматизации рабочего процесса. Описание эталонной архитектуры, в которой используется Фабрика данных, см. в статье [Automated enterprise BI with SQL Data Warehouse and Azure Data Factory](./enterprise-bi-adf.md) (Автоматизированная корпоративная бизнес-аналитика с использованием Хранилища данных SQL и Фабрики данных).

## <a name="architecture"></a>Архитектура

Архитектура состоит из следующих компонентов:

### <a name="data-source"></a>Источник данных

**SQL Server.** Исходные данные размещаются локально в базе данных SQL Server. Чтобы имитировать локальную среду, скрипты развертывания для этой архитектуры предоставляют виртуальную машину в Azure с установленной платформой SQL Server. В качестве исходных данных используется [пример базы данных OLTP Wide World Importers][wwi].

### <a name="ingestion-and-data-storage"></a>Прием и хранение данных

**Хранилище больших двоичных объектов**. Хранилище больших двоичных объектов используется в качестве промежуточной области для копирования данных перед загрузкой в хранилище данных SQL.

**Хранилище данных Azure SQL.** [Хранилище данных SQL](/azure/sql-data-warehouse/) — распределенная система, предназначенная для анализа больших объемов данных. Она поддерживает массовую параллельную обработку (MPP), что делает ее пригодной для запуска высокопроизводительной аналитики. 

### <a name="analysis-and-reporting"></a>Анализ и создание отчетов

**Службы Azure Analysis Services**. [Analysis Services](/azure/analysis-services/) — полностью управляемая служба, которая предоставляет возможности моделирования данных. Используйте службу Analysis Services для создания семантической модели, которую могут запрашивать пользователи. Служба Analysis Services особенно полезна в сценарии информационной панели бизнес-аналитики. В этой архитектуре служба Analysis Services считывает данные из хранилища данных для обработки семантической модели и эффективно обслуживает запросы информационной панели. Она также поддерживает гибкий параллелизм путем масштабирования реплик, чтобы быстрее обрабатывать запросы.

В настоящее время Azure Analysis Services поддерживает табличные модели, но не поддерживает многомерные. В табличных моделях используются конструкции реляционного моделирования (таблицы и столбцы), тогда как в многомерных моделях используются моделирующие конструкции OLAP (кубы, размеры и меры). Если требуются многомерные модели, используйте SQL Server Analysis Services (SSAS). Дополнительные сведения см. в разделе [Сравнение табличных и многомерных решений](/sql/analysis-services/comparing-tabular-and-multidimensional-solutions-ssas).

**Power BI**. Power BI — набор средств бизнес-аналитики для анализа информации о бизнесе. В этой архитектуре он запрашивает семантическую модель, хранящуюся в службе Analysis Services.

### <a name="authentication"></a>Authentication

**Azure Active Directory** (Azure AD) аутентифицирует пользователей, которые подключаются к серверу Analysis Services через Power BI.

## <a name="data-pipeline"></a>Конвейер данных
 
В этой эталонной архитектуре в качестве источника данных используется база данных [WorldWideImporters](/sql/sample/world-wide-importers/wide-world-importers-oltp-database). Конвейер данных состоит из перечисленных ниже этапов.

1. Экспортирование данных из SQL Server в неструктурированные файлы (утилита bcp).
2. Копирование неструктурированных файлов в хранилище больших двоичных объектов Azure (AzCopy)
3. Загрузка данных в хранилище данных SQL (PolyBase).
4. Преобразование данных в схему типа "звезда" (T-SQL).
5. Загрузка семантической модели в службу Analysis Services (средства SQL Server Data Tools).

![](./images/enterprise-bi-sqldw-pipeline.png)
 
> [!NOTE]
> Для шагов 1 &ndash; 3 рассмотрите использование Redgate Data Platform Studio. Так как к Data Platform Studio применены наиболее подходящие оптимизации и исправления совместимости, это самый быстрый способ начать работу с хранилищем данных SQL. Дополнительную информацию см. в разделе [Загрузка данных с помощью Redgate Data Platform Studio](/azure/sql-data-warehouse/sql-data-warehouse-load-with-redgate). 

Следующие разделы описывают эти этапы более подробно.

### <a name="export-data-from-sql-server"></a>Экспорт данных из SQL Server

Программа массового копирования [bcp](/sql/tools/bcp-utility) — это быстрый способ создания неструктурированных текстовых файлов из таблиц SQL. На этом шаге выбираются столбцы, которые нужно экспортировать, но данные не надо преобразовывать. Любые преобразования данных должны происходить в хранилище данных SQL.

**рекомендации**;

Если возможно, планируйте извлечение данных в часы с наименьшей загрузкой, чтобы свести к минимуму конфликт ресурсов в рабочей среде. 

Избегайте запуска bcp на сервере базы данных. Вместо этого запустите его с другого компьютера. Записывайте файлы на локальный диск. Убедитесь, что имеется достаточно ресурсов ввода-вывода для обработки параллельных операций записи. Для обеспечения максимальной производительности экспортируйте файлы в специализированные высокоскоростные хранилища.

Вы можете ускорить сетевой перенос, сохранив экспортированные данные в сжатом формате Gzip. Однако загрузка сжатых файлов в хранилище происходит медленнее, чем загрузка несжатых файлов, поэтому существует компромисс между более быстрой передачей сети и более быстрой загрузкой. Если решено использовать сжатие Gzip, не создавайте отдельные файлы Gzip. Вместо этого разделите данные на несколько сжатых файлов.

### <a name="copy-flat-files-into-blob-storage"></a>Копирование неструктурированных файлов в хранилище больших двоичных объектов

Утилита [AzCopy](/azure/storage/common/storage-use-azcopy) предназначена для высокопроизводительного копирования данных в хранилище больших двоичных объектов Azure.

**рекомендации**;

Создайте учетную запись хранения в регионе, расположенном рядом с исходными данными. Разверните учетную запись хранения и экземпляр хранилища данных SQL в том же регионе. 

Не запускайте AzCopy на том же компьютере, на котором выполняются производственные рабочие нагрузки, поскольку потребление процессора и ввода-вывода может влиять на рабочую нагрузку. 

Сначала проверьте загрузку, чтобы изучить скорость загрузки. Чтобы указать количество параллельных операций копирования, в AzCopy можно использовать параметр /NC. Чтобы настроить производительность, начните со значения по умолчанию и затем поэкспериментируйте с этим параметром. В среде с низкой пропускной способностью слишком много параллельных операций могут привести к сбою сетевого соединения и не допустить успешного завершения операций.  

AzCopy перемещает данные в хранилище через общий доступ в Интернет. Если это недостаточно быстро, рассмотрите создание схемы [ExpressRoute](/azure/expressroute/). ExpressRoute — это служба, которая направляет данные с помощью выделенного частного подключения в Azure. Если ваше сетевое соединение слишком медленное, то другой вариант заключается в физической отправке данных на диск в центр данных Azure. Дополнительные сведения см. в разделе [Передача данных в Azure и обратно](/azure/architecture/data-guide/scenarios/data-transfer).

Во время операции копирования AzCopy создает временный файл журнала, который позволяет AzCopy перезапустить операцию, если она прервется (например, из-за сетевой ошибки). Убедитесь, что для хранения файлов журнала на диске достаточно свободного места. Чтобы указать, где записываются файлы журнала, можно использовать параметр /Z.

### <a name="load-data-into-sql-data-warehouse"></a>Загрузка данных в хранилище данных SQL

Используйте [PolyBase](/sql/relational-databases/polybase/polybase-guide), чтобы загрузить файлы из хранилища больших двоичных объектов в хранилище данных. PolyBase предназначен для использования архитектуры MPP (массовой параллельной обработки) хранилища данных SQL, которая делает ее самым быстрым способом загрузки данных в хранилище данных SQL. 

Загрузка данных — это двухэтапный процесс:

1. Создание набора внешних таблиц для данных. Внешняя таблица — это определение таблицы, которое указывает на данные, хранящиеся вне хранилища &mdash; в этом случае, неструктурированные файлы в хранилище памяти. Этот этап не перемещает никаких данных в хранилище.
2. Создание промежуточных таблиц и загрузка данных в промежуточные таблицы. Этот этап копирует данные в хранилище.

**рекомендации**;

Если имеется большой объем данных (более 1 ТБ) и используется рабочая нагрузка аналитики, которая выиграет от параллелизма, то рассмотрите возможности хранилища данных SQL. Хранилище данных SQL не подходит для рабочих нагрузок OLTP или малых наборов данных (<250 ГБ). Для наборов данных менее 250 ГБ рассмотрите базу данных Azure SQL или SQL Server. Дополнительные сведения см. в разделе [Хранение данных](../../data-guide/relational-data/data-warehousing.md).

Создайте промежуточные таблицы в виде таблиц без кластеризованных индексов, которые не индексируются. Запросы, которые создают производственные таблицы, приведут к полному сканированию таблицы, поэтому нет причин индексировать промежуточные таблицы.

PolyBase автоматически использует преимущества параллелизма в хранилище. Производительность нагрузки масштабируется при увеличении DWU. Для достижения оптимальной производительности используйте операции одинарной загрузки. Разбивка входных данных на фрагменты и выполнение нескольких одновременных нагрузок не повышает производительность.

PolyBase может читать данные, сжатые с помощью GZip. Тем не менее для сжатого файла используется только один модуль чтения, потому что распаковка файла является однопоточной операцией. Поэтому следует избегать загрузки одного большого сжатого файла. Вместо этого разбейте данные на несколько сжатых файлов, чтобы воспользоваться преимуществами параллелизма. 

Следует учитывать следующие ограничения.

- PolyBase поддерживает максимальный размер столбца из `varchar(8000)`, `nvarchar(4000)` или `varbinary(8000)`. Если имеются данные, превышающие эти лимиты, то при экспорте следует разбить данные на блоки, а затем собрать эти блоки после импорта. 

- PolyBase использует фиксированный признак конца строки \n или новой строки. Если в исходных данных появляются символы новой строки, это может вызвать проблемы.

- Схема исходных данных может содержать типы данных, которые не поддерживаются в хранилище данных SQL.

Чтобы обойти эти ограничения, можно создать хранимую процедуру, которая выполнит необходимые преобразования. При запуске bcp найдите по ссылке эту хранимую процедуру. Как альтернатива [Redgate Data Platform Studio](/azure/sql-data-warehouse/sql-data-warehouse-load-with-redgate) автоматически преобразует типы данных, которые не поддерживаются в хранилище данных SQL.

Дополнительные сведения см. в следующих статьях:

- [Рекомендации по загрузке данных в хранилище данных SQL Azure](/azure/sql-data-warehouse/guidance-for-loading-data).
- [Перенос схем в хранилище данных SQL](/azure/sql-data-warehouse/sql-data-warehouse-migrate-schema).
- [Руководство по определению типов данных для таблиц в хранилище данных SQL](/azure/sql-data-warehouse/sql-data-warehouse-tables-data-types).

### <a name="transform-the-data"></a>Преобразование данных

Преобразование данных и перемещение их в производственные таблицы. На этом этапе данные преобразуются в схему типа "звезда" с таблицами измерений и таблицами фактов, подходящими для семантического моделирования.

Создайте рабочие таблицы с кластерными индексами columnstore, которые обеспечивают наилучшую общую производительность запросов. Индексы columnstore оптимизированы для запросов, которые сканируют большое количество записей. Индексы columnstore также не работают для отдельных поисковых запросов (которые просматривают одну строку). Если необходимо выполнять частые отдельные запросы, можно добавить некластеризованный индекс в таблицу. Выполнение отдельных запросов происходит значительно быстрее с помощью некластеризованного индекса. Однако отдельный поиск обычно менее распространен в сценариях хранилища данных, чем рабочие нагрузки OLTP. Дополнительные сведения см. в разделе [Индексирование таблиц в хранилище данных SQL](/azure/sql-data-warehouse/sql-data-warehouse-tables-index).

> [!NOTE]
> Кластерные таблицы columnstore не поддерживают типы данных `varchar(max)`, `nvarchar(max)` или `varbinary(max)`. В этом случае рассмотрим кучу или кластеризованный индекс. Эти столбцы можно поместить в отдельную таблицу.

Поскольку база данных примеров не очень велика, были созданы реплицированные таблицы без разделов. Для производственных нагрузок использование распределенных таблиц, вероятно, улучшит производительность запросов. См. [Руководство по проектированию распределенных таблиц в хранилище данных SQL Azure](/azure/sql-data-warehouse/sql-data-warehouse-tables-distribute). В примерах скрипты запускают запросы, используя статический [класс ресурсов](/azure/sql-data-warehouse/resource-classes-for-workload-management).

### <a name="load-the-semantic-model"></a>Загрузка семантической модели

Загрузка данных в табличную модель в Azure Analysis Services. На этом этапе создается семантическая модель данных с помощью средства SQL Server Data Tools (SSDT). Можно также создать модель путем импорта из файла Power BI Desktop. Поскольку хранилище данных SQL не поддерживает внешние ключи, необходимо добавить связи для семантической модели, чтобы можно было присоединиться к таблицам.

### <a name="use-power-bi-to-visualize-the-data"></a>Использование Power BI для визуализации данных

Существует два варианта подключения Power BI к Azure Analysis Services:

- Импорт. Данные импортируются в модель Power BI.
- Активное подключение. Данные извлекаются непосредственно из Analysis Services.

Корпорация Майкрософт рекомендует активное подключение, так как оно не требует копирования данных в модели Power BI. Кроме того, использование DirectQuery гарантирует, что результаты всегда соответствуют последним исходным данным. Дополнительные сведения см. в разделе [Подключение с помощью Power BI](/azure/analysis-services/analysis-services-connect-pbi).

**рекомендации**;

Избегайте запуска запросов панели управления бизнес-аналитики непосредственно к хранилищу данных. Информационные панели бизнес-аналитики требуют очень малого времени отклика, которого может не хватить на прямые запросы к хранилищу. Кроме того, при обновлении информационной панели будет учитываться количество одновременных запросов, что может повлиять на производительность. 

Azure Analysis Services предназначена для обработки требований к запросу информационной панели бизнес-аналитики, поэтому рекомендуемая практика заключается в том, чтобы запрашивать Analysis Services из Power BI.

## <a name="scalability-considerations"></a>Вопросы масштабируемости

### <a name="sql-data-warehouse"></a>Хранилище данных SQL

С хранилищем данных SQL можно по требованию масштабировать вычислительные ресурсы. Механизм запросов оптимизирует запросы для параллельной обработки на основе количества вычислительных узлов и перемещает данные между узлами по мере необходимости. Дополнительные сведения см. в разделе [Управление вычислительными ресурсами в хранилище данных SQL Azure](/azure/sql-data-warehouse/sql-data-warehouse-manage-compute-overview).

### <a name="analysis-services"></a>Analysis Services

Для рабочих нагрузок рекомендуется уровень "Стандартный" для служб Azure Analysis Services, так как он поддерживает секционирование и DirectQuery. В пределах уровня размер экземпляра определяет память и мощность обработки. Вычислительная мощность измеряется в единицах обработки запроса (QPUs). Контролируйте использование QPU, чтобы выбрать необходимый размер. Дополнительные сведения см. в разделе [Мониторинг производительности сервера](/azure/analysis-services/analysis-services-monitor).

При высокой нагрузке производительность запросов может ухудшиться из-за параллелизма запросов. Чтобы одновременно выполнять больше запросов, можно масштабировать службы Analysis Services, создавая пул реплик для обработки запросов. Работа по обработке модели данных всегда происходит на основном сервере. Основной сервер по умолчанию также обрабатывает запросы. При необходимости можно назначить сервер-источник исключительно для обработки, чтобы пул запросов обрабатывал все запросы. В случае высоких требований к обработке нужно отделить обработку от пула запросов. В случае высокой нагрузки запросов и относительно несложной обработки можно включить сервер-источник в пул запросов. Дополнительные сведения см. в разделе [Горизонтальное масштабирование служб Azure Analysis Services](/azure/analysis-services/analysis-services-scale-out). 

Чтобы уменьшить объем ненужной обработки, рассмотрите возможность использования секций для разделения табличной модели на логические части. Каждая секция может обрабатываться отдельно. Дополнительные сведения см. в разделе [Секция](/sql/analysis-services/tabular-models/partitions-ssas-tabular).

## <a name="security-considerations"></a>Вопросы безопасности

### <a name="ip-whitelisting-of-analysis-services-clients"></a>Список разрешенных IP-адресов клиентов службы Analysis Services

Рассмотрите возможность использования функции брандмауэра службы Analysis Services белого списка клиентских IP-адресов. Если параметр включен, брандмауэр блокирует все клиентские соединения, которые отличаются от указанных в правилах брандмауэра. Стандартные правила присваивают белый список службе Power BI, но при необходимости можно отключить это правило. Дополнительные сведения см. в разделе [Усиление защиты Azure Analysis Services благодаря новым возможностям брандмауэра](https://azure.microsoft.com/blog/hardening-azure-analysis-services-with-the-new-firewall-capability/).

### <a name="authorization"></a>Авторизация

Azure Analysis Services использует Azure Active Directory (Azure AD) для аутентификации пользователей, подключающихся к серверу служб Analysis Services. Создавая роли и затем назначая их пользователям или группам Azure AD, можно ограничить данные, которые может просматривать конкретный пользователь. Для каждой роли можно сделать следующее. 

- Защитить таблицы или отдельные столбцы. 
- Защитить отдельные строки на основе выражения фильтра. 

Дополнительные сведения см. в разделе [Управление ролями и пользователями базы данных](/azure/analysis-services/analysis-services-database-users).

## <a name="deploy-the-solution"></a>Развертывание решения

Пример развертывания для этой архитектуры можно найти на портале [GitHub][ref-arch-repo-folder]. Он позволяет развернуть следующее:

  * Виртуальную машину Windows для имитации локального сервера базы данных. Она включает SQL Server 2017 и связанные с ним инструменты, а также Power BI Desktop.
  * Учетная запись хранения Azure, которая обеспечивает хранилище больших двоичных объектов для хранения данных, экспортированных из базы данных SQL Server.
  * Экземпляр хранилища данных SQL Azure.
  * Экземпляр службы Azure Analysis Services.

### <a name="prerequisites"></a>предварительным требованиям

[!INCLUDE [ref-arch-prerequisites.md](../../../includes/ref-arch-prerequisites.md)]

### <a name="deploy-the-simulated-on-premises-server"></a>Развертывание имитации локального сервера

Сначала надо развернуть виртуальную машину в качестве имитируемого локального сервера, который включает SQL Server 2017 и связанные с ним инструменты. На этом этапе в SQL Server также загружается [база данных OLTP Wide World Importers][wwi].

1. Перейдите в папку `data\enterprise_bi_sqldw\onprem\templates` в репозитории.

2. В файле `onprem.parameters.json` замените значения для `adminUsername` и `adminPassword`. Измените также значения в разделе `SqlUserCredentials` в соответствии с именем пользователя и паролем. Обратите внимание на префикс `.\\` в свойстве userName.
    
    ```bash
    "SqlUserCredentials": {
      "userName": ".\\username",
      "password": "password"
    }
    ```

3. Для развертывания локального сервера выполните `azbb`, как показано ниже.

    ```bash
    azbb -s <subscription_id> -g <resource_group_name> -l <region> -p onprem.parameters.json --deploy
    ```

    Укажите регион, в котором поддерживается Хранилище данных SQL Azure и Azure Analysis Services. Ознакомьтесь со статьей [Доступность продуктов по регионам](https://azure.microsoft.com/global-infrastructure/services/).

4. Для завершения развертывания потребуется от 20 до 30 минут, включая запуск сценария [DSC](/powershell/dsc/overview) для установки инструментов и восстановления базы данных. Проверьте развертывание на портале Azure, просмотрев ресурсы в группе ресурсов. Вы должны увидеть виртуальную машину `sql-vm1` и связанные с ней ресурсы.

### <a name="deploy-the-azure-resources"></a>Развертывание ресурсов Azure

На этом шаге подготавливается Хранилище данных SQL и службы Azure Analysis Services вместе с учетной записью хранения. Если требуется, можно выполнить этот шаг параллельно с предыдущим шагом.

1. Перейдите в папку `data\enterprise_bi_sqldw\azure\templates` в репозитории.

2. Выполните следующую команду в Azure CLI, чтобы создать группу ресурсов. Вы можете выполнить развертывание в группу ресурсов, отличную от указанной на предыдущем шаге, но необходимо выбрать тот же регион. 

    ```bash
    az group create --name <resource_group_name> --location <region>  
    ```

3. Выполните следующую команду в Azure CLI, чтобы развернуть ресурсы Azure. Замените значения параметров, отображаемые в угловых скобках. 

    ```bash
    az group deployment create --resource-group <resource_group_name> \
     --template-file azure-resources-deploy.json \
     --parameters "dwServerName"="<server_name>" \
     "dwAdminLogin"="<admin_username>" "dwAdminPassword"="<password>" \ 
     "storageAccountName"="<storage_account_name>" \
     "analysisServerName"="<analysis_server_name>" \
     "analysisServerAdmin"="user@contoso.com"
    ```

    - Параметр `storageAccountName` должен следовать [правилам именования](../../best-practices/naming-conventions.md#naming-rules-and-restrictions) для учетных записей хранения.
    - Для параметра `analysisServerAdmin` используйте имя участника-пользователя Azure Active Directory (UPN).

4. Проверьте развертывание на портале Azure, просмотрев ресурсы в группе ресурсов. Вы должны увидеть учетную запись хранения, экземпляр хранилища данных Azure SQL и экземпляр службы Analysis Services.

5. Используйте портал Azure, чтобы получить ключ доступа для учетной записи хранения. Выберите учетную запись хранения, чтобы открыть ее. В разделе **Параметры** выберите **Ключи доступа**. Скопируйте значение первичного ключа. Он понадобится на следующем шаге.

### <a name="export-the-source-data-to-azure-blob-storage"></a>Экспорт исходных данных в хранилище больших двоичных объектов Azure 

На этом этапе вы запустите сценарий PowerShell, который использует bcp для экспорта базы данных SQL в неструктурированные файлы на виртуальной машине, а затем использует AzCopy для копирования этих файлов в хранилище больших двоичных объектов Azure.

1. Используйте удаленный рабочий стол для подключения к моделируемой виртуальной машине.

2. При входе в виртуальную машину запустите следующие команды из окна PowerShell.  

    ```powershell
    cd 'C:\SampleDataFiles\reference-architectures\data\enterprise_bi_sqldw\onprem'

    .\Load_SourceData_To_Blob.ps1 -File .\sql_scripts\db_objects.txt -Destination 'https://<storage_account_name>.blob.core.windows.net/wwi' -StorageAccountKey '<storage_account_key>'
    ```

    Для параметра `Destination` замените `<storage_account_name>` на имя созданной ранее учетной записи хранения. Для параметра `StorageAccountKey` используйте ключ доступа для этой учетной записи хранения.

3. Перейдя к учетной записи хранения, выбрав службу BLOB-объектов и открыв контейнер `wwi` на портале Azure, убедитесь, что исходные данные были скопированы в хранилище больших двоичных объектов. Вы должны увидеть список таблиц, которые начинаются с `WorldWideImporters_Application_*`.

### <a name="run-the-data-warehouse-scripts"></a>Выполнение скриптов хранилища данных

1. Во время сеанса удаленного рабочего стола запустите SQL Server Management Studio (SSMS). 

2. Подключение к хранилищу данных SQL

    - Тип сервера: ядро СУБД
    
    - Имя сервера: `<dwServerName>.database.windows.net`, где `<dwServerName>` — имя, указанное при развертывании ресурсов Azure. Его можно получить на портале Azure.
    
    - Аутентификация: проверка подлинности SQL Server. Используйте учетные данные, указанные при развертывании ресурсов Azure, в параметрах `dwAdminLogin` и `dwAdminPassword`.

2. Перейдите к папке `C:\SampleDataFiles\reference-architectures\data\enterprise_bi_sqldw\azure\sqldw_scripts` на виртуальной машине. Вы будете исполнять скрипты в этой папке в порядке от `STEP_1` до `STEP_7`.

3. Выберите в SSMS базу данных `master` и откройте скрипт `STEP_1`. Измените значение пароля в следующей строке, а затем выполните сценарий.

    ```sql
    CREATE LOGIN LoaderRC20 WITH PASSWORD = '<change this value>';
    ```

4. Выберите в SSMS базу данных `wwi`. Откройте сценарий `STEP_2` и выполните его. Если возникает сообщение об ошибке, убедитесь, что выполняется сценарий базы данных `wwi`, а не `master`.

5. Откройте новое соединение с хранилищем данных SQL, используя пользователя `LoaderRC20` и пароль, указанный в сценарии `STEP_1`.

6. С помощью этого подключения откройте сценарий `STEP_3`. Установите в сценарии следующие значения:

    - SECRET: введите ключ доступа к учетной записи хранения.
    - LOCATION: используйте имя учетной записи хранения следующим образом: `wasbs://wwi@<storage_account_name>.blob.core.windows.net`.

7. Используя это же подключение, последовательно выполняйте сценарии от `STEP_4` до `STEP_7`. Перед запуском следующего сценария убедитесь, что каждый сценарий выполняется успешно.

В SMSS вы должны увидеть набор таблиц `prd.*` в базе данных `wwi`. Чтобы убедиться, что данные были созданы, выполните следующий запрос: 

```sql
SELECT TOP 10 * FROM prd.CityDimensions
```

## <a name="build-the-analysis-services-model"></a>Создание модели Analysis Services

На этом этапе вы создадите табличную модель, которая импортирует данные из хранилища данных. Затем вы развернете модель в Azure Analysis Services.

1. Во время сеанса удаленного рабочего стола запустите средства SQL Server Data Tools 2015.

2. Выберите **Файл** > **Создать** > **Проект**.

3. В диалоговом окне **Создать проект** в разделе **Шаблоны** выберите **Бизнес-аналитика** > **Analysis Services** > **Табличный проект служб Analysis Services**. 

4. Назовите проект и нажмите кнопку **ОК**.

5. В диалоговом окне **Конструктор табличных моделей** выберите **Интегрированная рабочая область** и установите **Уровень совместимости** на `SQL Server 2017 / Azure Analysis Services (1400)`. Последовательно выберите **ОК**.

6. В окне **Обозреватель табличных моделей** щелкните правой кнопкой мыши проект и выберите **Импорт из источника данных**.

7. Выберите **Хранилище данных SQL Azure** и нажмите **Подключить**.

8. Для **Server** введите полное имя сервера хранилища данных SQL Azure. Для **Database** введите `wwi`. Последовательно выберите **ОК**.

9. В следующем диалоговом окне выберите аутентификацию **базы данных**, введите имя пользователя и пароль хранилища данных SQL Azure и нажмите **ОК**.

10. В диалоговом окне **Навигатор** установите флажки для **prd.CityDimensions**, **prd.DateDimensions** и **prd.SalesFact**. 

    ![](./images/analysis-services-import.png)

11. Нажмите кнопку **Загрузить**. После завершения обработки нажмите кнопку **Закрыть**. Теперь вы должны увидеть табличное представление данных.

12. В окне **Обозреватель табличных моделей** нажмите правой кнопкой мыши проект и выберите **Представление модели** > **Представление диаграммы**.

13. Чтобы создать связь, перетащите поле **[prd.SalesFact].[WWI City ID]** в поле **[prd.CityDimensions].[WWI City ID]**.  

14. Перетащите поле **[prd.SalesFact].[Invoice Date Key]** в поле **[prd.DateDimensions].[Date]**.  
    ![](./images/analysis-services-relations.png)

15. В меню **Файл** выберите **Сохранить все**.  

16. В **обозревателе решений** щелкните правой кнопкой мыши проект и выберите **Свойства**. 

17. В разделе **Сервер** введите URL-адрес своего экземпляра службы Azure Analysis Services. Его можно получить на портале Azure. На портале выберите ресурс службы Analysis Services, нажмите панель "Обзор" и найдите свойство **Server Name**. Он будет выглядеть аналогично `asazure://westus.asazure.windows.net/contoso`. Последовательно выберите **ОК**.

    ![](./images/analysis-services-properties.png)

18. В **обозревателе решений** щелкните правой кнопкой мыши проект и выберите пункт **Развернуть**. При появлении запроса войдите в Azure. После завершения обработки нажмите кнопку **Закрыть**.

19. На портале Azure просмотрите сведения о вашем экземпляре Azure Analysis Services. Убедитесь, что ваша модель отображается в списке моделей.

    ![](./images/analysis-services-models.png)

## <a name="analyze-the-data-in-power-bi-desktop"></a>Анализ данных в Power BI Desktop

На этом этапе вы будете использовать Power BI для создания отчета из данных в Analysis Services.

1. Во время сеанса удаленного рабочего стола запустите Power BI Desktop.

2. На экране приветствия нажмите **Получение данных**.

3. Выберите **Azure** > **База данных Azure Analysis Services**. Добавьте новый отчет, щелкнув **Подключить**

    ![](./images/power-bi-get-data.png)

4. Введите URL-адрес экземпляра службы Analysis Services, а затем нажмите кнопку **ОК**. При появлении запроса войдите в Azure.

5. В диалоговом окне **Навигатор** разверните проект таблицы, который вы развертывали, выберите созданную модель и нажмите кнопку **ОК**.

2. На панели **визуализации** выберите значок **Линейчатая диаграмма с накоплением**. В представлении "Отчет" измените размер визуализации, чтобы ее увеличить.

6. В области **Поля** разверните **prd.CityDimensions**.

7. Перетащите **prd.CityDimensions** > **Идентификатор города WWI** в область **Ось**.

8. Перетащите **prd.CityDimensions** > **Город** в **Условные обозначения**.

9. В области **Поля** разверните узел **prd.SalesFact**.

10. Перетащите **prd.SalesFact** > **Общая сумма без учета налогов** в **Значение**.

    ![](./images/power-bi-visualization.png)

11. В **фильтрах уровня визуальных элементов** выберите **идентификатор города WWI**.

12. Задайте **Тип фильтра** как `Top N` и установите **Показать элементы** как `Top 10`.

13. Перетащите **prd.SalesFact** > **Общая сумма без учета налога** в **По значению**.

    ![](./images/power-bi-visualization2.png)

14. Нажмите кнопку **Применить фильтр**. Визуализация показывает 10 лучших объемов продаж по городу.

    ![](./images/power-bi-report.png)

Дополнительные сведения о Power BI Desktop см. в статье [Начало работы с Power BI Desktop](/power-bi/desktop-getting-started).

## <a name="next-steps"></a>Дополнительная информация

- Для получения дополнительных сведений об этой эталонной архитектуре посетите наш [репозиторий GitHub][ref-arch-repo-folder].
- Узнать больше о [стандартных блоках Azure][azbb-repo].

<!-- links -->

[azure-cli-2]: /azure/install-azure-cli
[azbb-repo]: https://github.com/mspnp/template-building-blocks
[azbb-wiki]: https://github.com/mspnp/template-building-blocks/wiki/Install-Azure-Building-Blocks
[github-folder]: https://github.com/mspnp/reference-architectures/tree/master/data/enterprise_bi_sqldw
[ref-arch-repo]: https://github.com/mspnp/reference-architectures
[ref-arch-repo-folder]: https://github.com/mspnp/reference-architectures/tree/master/data/enterprise_bi_sqldw
[wwi]: /sql/sample/world-wide-importers/wide-world-importers-oltp-database
