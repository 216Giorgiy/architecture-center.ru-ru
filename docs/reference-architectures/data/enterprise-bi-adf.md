---
title: Автоматизированная корпоративная бизнес-аналитика с использованием Хранилища данных SQL и Фабрики данных Azure
description: Сведения о том, как автоматизировать рабочий процесс ELT в Azure с помощью Фабрики данных Azure
author: MikeWasson
ms.date: 11/06/2018
ms.openlocfilehash: 3fedcd08572a9fe1fc610f5fbab12f8ff0d53073
ms.sourcegitcommit: 19a517a2fb70768b3edb9a7c3c37197baa61d9b5
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 11/26/2018
ms.locfileid: "52295636"
---
# <a name="automated-enterprise-bi-with-sql-data-warehouse-and-azure-data-factory"></a>Автоматизированная корпоративная бизнес-аналитика с использованием Хранилища данных SQL и Фабрики данных Azure

На примере этой эталонной архитектуры показано, как выполнять добавочную нагрузку в конвейере [ELT](../../data-guide/relational-data/etl.md#extract-load-and-transform-elt) (извлечение, загрузка и преобразование). Для автоматизации этого конвейера используется Фабрика данных Azure. Конвейер поэтапно перемещает последние данные OLTP из локальной базы данных SQL Server в Хранилище данных SQL. Данные о транзакциях преобразуются в табличную модель для анализа.

> [!VIDEO https://www.microsoft.com/en-us/videoplayer/embed/RE2Gnz2]

Эталонную реализацию для этой архитектуры можно найти на сайте [GitHub][github].

![](./images/enterprise-bi-sqldw-adf.png)

Эта архитектура создана на основе архитектуры, описанной в статье [Корпоративная бизнес-аналитика с использованием хранилища данных SQL](./enterprise-bi-sqldw.md), но с некоторыми дополнительными функциями, требуемыми для хранения корпоративных данных.

-   Автоматизация конвейера с помощью Фабрики данных.
-   Добавочная загрузка.
-   Интеграция нескольких источников данных.
-   Загрузка таких двоичных данных, как геопространственные данные и изображения.

## <a name="architecture"></a>Архитектура

Архитектура состоит из следующих компонентов:

### <a name="data-sources"></a>Источники данных

**Локальный сервер SQL Server**. Исходные данные размещаются локально в базе данных SQL Server. Чтобы имитировать локальную среду, сценарии развертывания для этой архитектуры предоставляют виртуальную машину в Azure с установленным SQL Server. В качестве базы данных-источника используется [пример базы данных OLTP Wide World Importers][wwi].

**Внешние данные**. Распространенный сценарий для хранилищ данных — выполнение интеграции нескольких источников данных. Для этой эталонной архитектуры загружается набор внешних данных о численности населения города по годам, интегрируемый с данными из базы данных OLTP. Эта данные можно использовать для получения полезных сведений. Например, чтобы узнать, соответствуют ли показатели роста продаж в каждом регионе показателям роста населения или превышают их.

### <a name="ingestion-and-data-storage"></a>Прием и хранение данных

**Хранилище больших двоичных объектов**. Хранилище больших двоичных объектов используется в качестве промежуточной области для исходных данных перед их загрузкой в Хранилище данных SQL.

**Хранилище данных Azure SQL.** [Хранилище данных SQL](/azure/sql-data-warehouse/) — распределенная система, предназначенная для анализа больших объемов данных. Она поддерживает массовую параллельную обработку (MPP), что делает ее пригодной для запуска высокопроизводительной аналитики. 

**Фабрика данных Azure**. [Фабрика данных][adf] — это управляемая служба, которая координирует и автоматизирует перемещение и преобразование данных. В этой архитектуре она координирует разные этапы процесса ELT.

### <a name="analysis-and-reporting"></a>Анализ и создание отчетов

**Службы Azure Analysis Services**. [Analysis Services](/azure/analysis-services/) — полностью управляемая служба, которая предоставляет возможности моделирования данных. Семантическая модель загружается в службы Analysis Services.

**Power BI**. Power BI — набор средств бизнес-аналитики для анализа информации о бизнесе. В этой архитектуре он запрашивает семантическую модель, хранящуюся в службе Analysis Services.

### <a name="authentication"></a>Authentication

**Azure Active Directory** (Azure AD) аутентифицирует пользователей, которые подключаются к серверу Analysis Services через Power BI.

В Фабрике данных также можно использовать Azure AD для аутентификации в хранилище данных SQL с использованием субъекта-службы или Управляемого удостоверения службы (MSI). Для простоты в примере развертывания используется аутентификация SQL Server.

## <a name="data-pipeline"></a>Конвейер данных

Конвейер в [Фабрике данных Azure][adf] — это логическая группа действий, используемых для координации задачи (в нашем примере это загрузка и преобразование данных в Хранилище данных SQL). 

В этой эталонной архитектуре определяется основной конвейер, который запускает последовательность дочерних конвейеров. Каждый дочерний конвейер загружает данные в одну или несколько таблиц в хранилище данных.

![](./images/adf-pipeline.png)

## <a name="incremental-loading"></a>Добавочная загрузка

При выполнении автоматизированного процесса ETL или ELT гораздо эффективнее загружать только те данные, которые изменились с момента предыдущего выполнения. Это называется *добавочной загрузкой*. Этот процесс отличается от полной загрузки, при которой загружаются все данные. Чтобы выполнить добавочную загрузку, нужно выбрать метод определения измененных данных. Для этого чаще всего используется значение *верхнего предела*. То есть отслеживается последнее значение в некотором столбце исходной таблицы, например по столбцу даты и времени или по столбцу уникальных целых чисел. 

Начиная с версии SQL Server 2016, вы можете использовать [темпоральные таблицы](/sql/relational-databases/tables/temporal-tables). Это таблицы с системным управлением версиями, в которых хранится полный журнал изменения данных. Ядро СУБД автоматически записывает каждое изменение в отдельную таблицу журнала. Чтобы запросить данные журнала, добавьте в запрос предложение FOR SYSTEM_TIME. На внутреннем уровне ядро СУБД отправляет запрос к таблице журнала, но это происходит незаметно для приложения. 

> [!NOTE]
> Для более ранних версий SQL Server можно использовать функцию [отслеживания измененных данных](/sql/relational-databases/track-changes/about-change-data-capture-sql-server) (CDC). Этот метод не такой удобный, как темпоральные таблицы, так как вам нужно запросить отдельную таблицу изменений, и изменения отслеживаются по регистрационному номеру транзакции (LSN) в журнале, а не по метке времени. 

Темпоральные таблицы удобно использовать для данных измерений, которые могут изменяться со временем. В таблице фактов обычно представлены неизменяемые транзакции, например при продаже. В этом случае ведение журнала версий системы не имеет смысла. Вместо этого для транзакций обычно присутствует столбец, в котором представлена дата транзакции, что может использоваться в качестве значения верхнего предела. Например, в базе данных OLTP Wide World Importers таблицы Sales.Invoices и Sales.InvoiceLines имеют поле `LastEditedWhen` со значением по умолчанию `sysdatetime()`. 

Ниже приведена стандартная последовательность действий для конвейера ELT:

1. Для каждой таблицы базы данных-источника отслеживается пороговое значение времени, когда запускается последнее задание ELT. Сохраните эту информацию в хранилище данных. (При начальной настройке для всех значений времени указано 1-1-1900.)

2. На этапе экспорта данных пороговое значение времени передается в качестве параметра в набор хранимых процедур в базе данных-источника. Эти хранимые процедуры запрашивают любые записи, которые были изменены или созданы после этого значения времени. Для таблицы фактов Sales используется столбец `LastEditedWhen`. Для данных измерения используются темпоральные таблицы с системным управлением версиями.

3. Когда перенос данных завершится, обновите таблицу, в которой хранятся пороговые значения времени.

Также полезно вести *журнал преобразований* для каждого выполнения ELT. В таком журнале определенная запись связывается с выполнением ELT, при котором создаются данные. При каждом выполнении ETL создается запись журнала преобразований для каждой таблицы. В этой записи указано время начала и завершения загрузки. Ключи для каждой записи журнала преобразований хранятся в таблицах фактов и измерений.

![](./images/city-dimension-table.png)

Когда новый пакет данных загрузится в хранилище, обновите табличную модель служб Analysis Services. Дополнительные сведения см. в статье [Асинхронное обновление с помощью REST API](/azure/analysis-services/analysis-services-async-refresh).

## <a name="data-cleansing"></a>Очистка данных

Очистка данных должна выполняться в рамках процесса ELT. В этой эталонной архитектуре есть один источник с поврежденными данными — таблица численности населения по городам, где некоторые города имеют нулевое значение, возможно, из-за недоступности данных. Во время обработки в конвейере ELT такие города удаляются из таблицы численности населения по городам. Очистку данных следует выполнять с промежуточными таблицами, а не внешними.

Ниже приводится хранимая процедура, которая удаляет города с нулевым значением численности населения из соответствующей таблицы. (Исходный файл можно найти [здесь](https://github.com/mspnp/reference-architectures/blob/master/data/enterprise_bi_sqldw_advanced/azure/sqldw_scripts/citypopulation/%5BIntegration%5D.%5BMigrateExternalCityPopulationData%5D.sql).) 

```sql
DELETE FROM [Integration].[CityPopulation_Staging]
WHERE RowNumber in (SELECT DISTINCT RowNumber
FROM [Integration].[CityPopulation_Staging]
WHERE POPULATION = 0
GROUP BY RowNumber
HAVING COUNT(RowNumber) = 4)
```

## <a name="external-data-sources"></a>Внешние источники данных

В хранилищах часто объединяются данные из нескольких источников. Эта эталонная архитектура позволяет загрузить внешний источник данных, содержащий демографические данные. Этот набор данных доступен в хранилище BLOB-объектов Azure как часть примера [WorldWideImportersDW](https://github.com/Microsoft/sql-server-samples/tree/master/samples/databases/wide-world-importers/sample-scripts/polybase).

В Фабрике данных Azure можно копировать данные прямо из хранилища BLOB-объектов с помощью [соответствующего соединителя](/azure/data-factory/connector-azure-blob-storage). Но соединителю требуется строка подключения или подписанный URL-адрес, поэтому вы не сможете использовать соединитель для копирования BLOB-объекта с общим доступом на чтение. В качестве решения с помощью PolyBase создайте внешнюю таблицу в хранилище BLOB-объектов и скопируйте внешние таблицы в Хранилище данных SQL. 

## <a name="handling-large-binary-data"></a>Обработка больших двоичных данных 

В базе данных-источника таблица Cities содержит столбец Location, в котором представлены данные пространственного типа — [geography](/sql/t-sql/spatial-geography/spatial-types-geography). Хранилище данных SQL изначально не поддерживает тип данных **geography**, поэтому при загрузке такие поля преобразуются в тип **varbinary**. Дополнительные сведения см. в разделе [Обходные решения для неподдерживаемых типов данных](/azure/sql-data-warehouse/sql-data-warehouse-tables-data-types#unsupported-data-types).

Но в PolyBase размер столбца не должен превышать значение `varbinary(8000)`. Это означает, что некоторые данные могут быть усечены. Чтобы решить эту проблему, вы можете разбить данные на блоки во время экспорта, а затем воссоединить их, как описано ниже:

1. Создайте временную промежуточную таблицу для столбца Location.

2. Для каждого города разбейте данные расположения на блоки объемом 8000 байт. В результате для каждого города будет создано 1 &ndash; N строк.

3. Чтобы воссоединить эти блоки, с помощью оператора T-SQL [PIVOT](/sql/t-sql/queries/from-using-pivot-and-unpivot) преобразуйте строки в столбцы, а затем сцепите значения столбцов для каждого города.

Сложность состоит в том, что для каждого города будет разное количество строк, в зависимости от размера географических данных. Для использования оператора PIVOT необходимо, чтобы у каждого города было одинаковое количество строк. Для этого запрос T-SQL (сведения о нем см. [здесь][MergeLocation]) выполняет некоторые приемы, чтобы включить строки с пустыми значениями, обеспечивая одинаковое количество столбцов для каждого города после сведения. Результирующий запрос выполняется намного быстрее, чем последовательный циклический перебор каждой строки.

Такой же подход используется для данных изображений.

## <a name="slowly-changing-dimensions"></a>Медленно изменяющиеся измерения

Данные измерений являются относительно статичными, но могут изменяться. Например, продукт может быть переназначен другой категории продуктов. Медленно изменяющиеся измерения можно обрабатывать несколькими способами. Для этого чаще всего используется [тип 2](https://wikipedia.org/wiki/Slowly_changing_dimension#Type_2:_add_new_row), который позволяет добавлять новую запись при каждом изменении измерения. 

Чтобы реализовать такой метод, в таблицу измерений нужно добавить столбцы, которые определяют фактический диапазон дат для определенной записи. Кроме того, нужно продублировать первичные ключи из базы данных-источника, чтобы таблица измерений содержала смоделированные первичные ключи.

На изображении ниже показана таблица Dimension.City. В столбце `WWI City ID` представлены первичные ключи из базы данных-источника. А в столбце `City Key` содержатся смоделированные ключи, созданные при выполнении конвейера ETL. Также обратите внимание, что в таблице есть столбцы `Valid From` и `Valid To`, которые определяют период, в который строка была актуальна. Текущие значения в столбце `Valid To` равны 9999-12-31.

![](./images/city-dimension-table.png)

Преимущество такого подхода заключается в том, что он позволяет хранить данные журнала, которые могут пригодиться для анализа. Но это также означает, что у одной сущности будет несколько строк. Например, ниже показаны записи, для которых в столбце `WWI City ID` указано значение 28561:

![](./images/city-dimension-table-2.png)

Каждый факт Sales необходимо связать с одной строкой в таблице измерения City, соответствующей дате выставления счета. В рамках процесса ETL создайте дополнительный столбец. 

Следующий запрос T-SQL создает временную таблицу, связывающую каждый счет с правильным ключом в столбце City Key таблицы измерений City.

```sql
CREATE TABLE CityHolder
WITH (HEAP , DISTRIBUTION = HASH([WWI Invoice ID]))
AS
SELECT DISTINCT s1.[WWI Invoice ID] AS [WWI Invoice ID],
                c.[City Key] AS [City Key]
    FROM [Integration].[Sale_Staging] s1
    CROSS APPLY (
                SELECT TOP 1 [City Key]
                    FROM [Dimension].[City]
                WHERE [WWI City ID] = s1.[WWI City ID]
                    AND s1.[Last Modified When] > [Valid From]
                    AND s1.[Last Modified When] <= [Valid To]
                ORDER BY [Valid From], [City Key] DESC
                ) c

```

Эта таблица используется для заполнения столбца в таблице фактов Sales:

```sql
UPDATE [Integration].[Sale_Staging]
SET [Integration].[Sale_Staging].[WWI Customer ID] =  CustomerHolder.[WWI Customer ID]
```

Этот столбец позволяет с помощью запроса Power BI найти нужную запись в таблице City для заданного счета на продажу.

## <a name="security-considerations"></a>Вопросы безопасности

Чтобы обеспечить дополнительную защиту ресурсов служб Azure только в своей виртуальной сети, используйте [конечные точки виртуальной сети](/azure/virtual-network/virtual-network-service-endpoints-overview). Это позволит полностью исключить открытый доступ из Интернета к этим ресурсам и разрешить трафик только из виртуальной сети.

В этом случае вам нужно создать виртуальную сеть в Azure и частные конечные точки для служб Azure. В такие службы трафик будет поступать только из виртуальной сети. Доступ к таким службам можно также получить в локальной сети через шлюз.

Следует учитывать следующие ограничения.

- На момент создания этой эталонной архитектуры конечные точки службы поддерживались для службы хранилища Azure и Хранилища данных SQL Azure, но не для служб Azure Analysis Services. Последние сведения о состоянии поддержки см. [здесь](https://azure.microsoft.com/updates/?product=virtual-network). 

- Если для службы хранилища Azure включены конечные точки службы, PolyBase не сможет скопировать данные из службы хранилища в хранилище данных SQL. Эту проблему можно устранить. Дополнительные сведения см. в разделе [Влияние использования конечных точек службы виртуальной сети со службой хранилища Azure](/azure/sql-database/sql-database-vnet-service-endpoint-rule-overview?toc=%2fazure%2fvirtual-network%2ftoc.json#impact-of-using-vnet-service-endpoints-with-azure-storage). 

- Чтобы переместить данные из локального расположения в службу хранилища Azure, нужно добавить в список разрешений общедоступные IP-адреса из локальной среды или ExpressRoute. Дополнительные сведения см. в разделе [Защита служб Azure в виртуальных сетях](/azure/virtual-network/virtual-network-service-endpoints-overview#securing-azure-services-to-virtual-networks).

- Чтобы службы Analysis Services могли считывать данные из хранилища данных SQL, разверните виртуальные машины Windows в виртуальной сети, которая содержит конечную точку службы "Хранилище данных SQL". Установите в этой виртуальной машине [локальный шлюз данных Azure](/azure/analysis-services/analysis-services-gateway). Затем подключите службы Azure Analysis Services к этому шлюзу данных.

## <a name="deploy-the-solution"></a>Развертывание решения

Чтобы выполнить развертывание и запуск эталонной реализации, выполните действия, описанные в [файле сведений на GitHub][github]. Он позволяет развернуть следующее:

  * Виртуальную машину Windows для имитации локального сервера базы данных. Она включает SQL Server 2017 и связанные с ним инструменты, а также Power BI Desktop.
  * Учетная запись хранения Azure, которая обеспечивает хранилище больших двоичных объектов для хранения данных, экспортированных из базы данных SQL Server.
  * Экземпляр хранилища данных SQL Azure.
  * Экземпляр службы Azure Analysis Services.
  * Фабрику данных Azure и конвейер фабрики данных для задания ELT.

[adf]: //azure/data-factory
[github]: https://github.com/mspnp/reference-architectures/tree/master/data/enterprise_bi_sqldw_advanced
[MergeLocation]: https://github.com/mspnp/reference-architectures/blob/master/data/enterprise_bi_sqldw_advanced/azure/sqldw_scripts/city/%5BIntegration%5D.%5BMergeLocation%5D.sql
[wwi]: //sql/sample/world-wide-importers/wide-world-importers-oltp-database

