---
title: Архитектура микрослужб в Службе Azure Kubernetes (AKS)
description: Развертывание архитектуры микрослужб в Службе Azure Kubernetes (AKS)
author: MikeWasson
ms.date: 12/10/2018
ms.openlocfilehash: 9e4b607cd7f5b33bbf08ce3af67dd5d4071ae8ef
ms.sourcegitcommit: bb7fcffbb41e2c26a26f8781df32825eb60df70c
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 12/20/2018
ms.locfileid: "53644246"
---
# <a name="microservices-architecture-on-azure-kubernetes-service-aks"></a><span data-ttu-id="803c2-103">Архитектура микрослужб в Службе Azure Kubernetes (AKS)</span><span class="sxs-lookup"><span data-stu-id="803c2-103">Microservices architecture on Azure Kubernetes Service (AKS)</span></span>

<span data-ttu-id="803c2-104">В этой эталонной архитектуре показано приложение микрослужб, развернутое в Службе Azure Kubernetes (AKS).</span><span class="sxs-lookup"><span data-stu-id="803c2-104">This reference architectures shows a microservices application deployed to Azure Kubernetes Service (AKS).</span></span> <span data-ttu-id="803c2-105">Она демонстрирует основную конфигурацию AKS, которая может быть отправной точкой для большинства развертываний.</span><span class="sxs-lookup"><span data-stu-id="803c2-105">It shows a basic AKS configuration that can be the starting point for most deployments.</span></span> <span data-ttu-id="803c2-106">Дополнительные расширенные параметры, включая расширенные сетевые параметры, будут рассмотрены в отдельной эталонной архитектуре.</span><span class="sxs-lookup"><span data-stu-id="803c2-106">More advanced options, including advanced networking options, will be covered in a separate reference architecture.</span></span>

<span data-ttu-id="803c2-107">В этой статье предполагается, что вы владеете базовыми знаниями о Kubernetes.</span><span class="sxs-lookup"><span data-stu-id="803c2-107">This article assumes basic knowledge of Kubernetes.</span></span> <span data-ttu-id="803c2-108">Статья посвящена главным образом инфраструктуре и вопросам DevOps по запуску архитектуры микрослужб в AKS.</span><span class="sxs-lookup"><span data-stu-id="803c2-108">The article focuses mainly on the infrastructure and DevOps considerations of running a microservices architecture on AKS.</span></span> <span data-ttu-id="803c2-109">Инструкции по проектированию микрослужб с использованием предметно-ориентированного проектирования см. в статье [Проектирование, создание и использование микрослужб в Azure](/azure/architecture/microservices).</span><span class="sxs-lookup"><span data-stu-id="803c2-109">For guidance on how to design microservices from a Domain Driven Design (DDD) perspective, see [Designing, building, and operating microservices on Azure](/azure/architecture/microservices).</span></span>

> [!NOTE]
> <span data-ttu-id="803c2-110">Мы работаем над эталонной реализацией, чтобы дополнить эту статью. Мы планируем опубликовать эту реализацию в начале 2019 года.</span><span class="sxs-lookup"><span data-stu-id="803c2-110">We are working on a reference implementation (RI) to accompany this article, which we expect to publish in early 2019.</span></span> <span data-ttu-id="803c2-111">В эту статью будут добавлены дополнительные рекомендации из этой эталонной реализации.</span><span class="sxs-lookup"><span data-stu-id="803c2-111">This article will be updated to incorporate additional best practices from that RI.</span></span>

![Эталонная архитектура AKS](./_images/aks.png)

## <a name="architecture"></a><span data-ttu-id="803c2-113">Архитектура</span><span class="sxs-lookup"><span data-stu-id="803c2-113">Architecture</span></span>

<span data-ttu-id="803c2-114">Архитектура состоит из следующих компонентов:</span><span class="sxs-lookup"><span data-stu-id="803c2-114">The architecture consists of the following components.</span></span>

<span data-ttu-id="803c2-115">**Служба Azure Kubernetes** (AKS).</span><span class="sxs-lookup"><span data-stu-id="803c2-115">**Azure Kubernetes Service** (AKS).</span></span> <span data-ttu-id="803c2-116">AKS — это служба Azure, которая развертывает управляемый кластер Kubernetes.</span><span class="sxs-lookup"><span data-stu-id="803c2-116">AKS is an Azure service that deploys a managed Kubernetes cluster.</span></span> 

<span data-ttu-id="803c2-117">**Кластер Kubernetes**.</span><span class="sxs-lookup"><span data-stu-id="803c2-117">**Kubernetes cluster**.</span></span> <span data-ttu-id="803c2-118">AKS отвечает за развертывание кластера Kubernetes и за управление главными узлами Kubernetes.</span><span class="sxs-lookup"><span data-stu-id="803c2-118">AKS is responsible for deploying the Kubernetes cluster and for managing the Kubernetes masters.</span></span> <span data-ttu-id="803c2-119">Вы только управляете узлами агентов.</span><span class="sxs-lookup"><span data-stu-id="803c2-119">You only manage the agent nodes.</span></span>

<span data-ttu-id="803c2-120">**Виртуальная сеть**.</span><span class="sxs-lookup"><span data-stu-id="803c2-120">**Virtual network**.</span></span> <span data-ttu-id="803c2-121">По умолчанию AKS создает виртуальную сеть для развертывания узлов агентов.</span><span class="sxs-lookup"><span data-stu-id="803c2-121">By default, AKS creates a virtual network to deploy the agent nodes into.</span></span> <span data-ttu-id="803c2-122">Для более сложных сценариев можно сначала создать виртуальную сеть, которая позволяет управлять такими настройками, как конфигурация подсетей, локальное подключение и IP-адресация.</span><span class="sxs-lookup"><span data-stu-id="803c2-122">For more advanced scenarios, you can create the virtual network first, which lets you control things like how the subnets are configured, on-premises connectivity, and IP addressing.</span></span> <span data-ttu-id="803c2-123">Дополнительные сведения см. в статье [Настройка расширенных параметров сети в Службе Azure Kubernetes (AKS)](/azure/aks/configure-advanced-networking).</span><span class="sxs-lookup"><span data-stu-id="803c2-123">For more information, see [Configure advanced networking in Azure Kubernetes Service (AKS)](/azure/aks/configure-advanced-networking).</span></span>

<span data-ttu-id="803c2-124">**Входящий трафик**.</span><span class="sxs-lookup"><span data-stu-id="803c2-124">**Ingress**.</span></span> <span data-ttu-id="803c2-125">Входящий трафик обеспечивает доступ маршрутов HTTP(S) к службам внутри кластера.</span><span class="sxs-lookup"><span data-stu-id="803c2-125">An ingress exposes HTTP(S) routes to services inside the cluster.</span></span> <span data-ttu-id="803c2-126">Дополнительные сведения см. в разделе [Шлюз API](#api-gateway), приведенном ниже.</span><span class="sxs-lookup"><span data-stu-id="803c2-126">For more information, see the section [API Gateway](#api-gateway) below.</span></span>

<span data-ttu-id="803c2-127">**Внешние хранилища данных**.</span><span class="sxs-lookup"><span data-stu-id="803c2-127">**External data stores**.</span></span> <span data-ttu-id="803c2-128">Состояние микрослужб, как правило, не отслеживается и записывается во внешние хранилища данных, такие как База данных SQL Azure или Cosmos DB.</span><span class="sxs-lookup"><span data-stu-id="803c2-128">Microservices are typically stateless and write state to external data stores, such as Azure SQL Database or Cosmos DB.</span></span>

<span data-ttu-id="803c2-129">**Azure Active Directory**.</span><span class="sxs-lookup"><span data-stu-id="803c2-129">**Azure Active Directory**.</span></span> <span data-ttu-id="803c2-130">АКS использует удостоверение Azure Active Directory (Azure AD) для создания других ресурсов Azure, таких как подсистемы балансировки нагрузки Azure, и управления ими.</span><span class="sxs-lookup"><span data-stu-id="803c2-130">AKS uses an Azure Active Directory (Azure AD) identity to create and manage other Azure resources such as Azure load balancers.</span></span> <span data-ttu-id="803c2-131">Azure AD также рекомендуется применять для проверки подлинности пользователей в клиентских приложениях.</span><span class="sxs-lookup"><span data-stu-id="803c2-131">Azure AD is also recommended for user authentication in client applications.</span></span>

<span data-ttu-id="803c2-132">**Реестр контейнеров Azure**.</span><span class="sxs-lookup"><span data-stu-id="803c2-132">**Azure Container Registry**.</span></span> <span data-ttu-id="803c2-133">В Реестре контейнеров можно хранить закрытые образы Docker, которые развертываются в кластере.</span><span class="sxs-lookup"><span data-stu-id="803c2-133">Use Container Registry to store private Docker images, which are deployed to the cluster.</span></span> <span data-ttu-id="803c2-134">АКS может выполнять аутентификацию в Реестре контейнеров с помощью удостоверения Azure AD.</span><span class="sxs-lookup"><span data-stu-id="803c2-134">AKS can authenticate with Container Registry using its Azure AD identity.</span></span> <span data-ttu-id="803c2-135">Обратите внимание, что для АКS не требуется Реестр контейнеров Azure.</span><span class="sxs-lookup"><span data-stu-id="803c2-135">Note that AKS does not require Azure Container Registry.</span></span> <span data-ttu-id="803c2-136">Можно использовать другие реестры контейнеров, такие как Центр Docker.</span><span class="sxs-lookup"><span data-stu-id="803c2-136">You can use other container registries, such as Docker Hub.</span></span>

<span data-ttu-id="803c2-137">**Azure Pipelines**.</span><span class="sxs-lookup"><span data-stu-id="803c2-137">**Azure Pipelines**.</span></span> <span data-ttu-id="803c2-138">Конвейеры являются частью Azure DevOps Services и выполняют автоматизированные сборки, тесты и развертывания.</span><span class="sxs-lookup"><span data-stu-id="803c2-138">Pipelines is part of Azure DevOps Services and runs automated builds, tests, and deployments.</span></span> <span data-ttu-id="803c2-139">Вы также можете использовать сторонние решения для CI/CD, такие как Jenkins.</span><span class="sxs-lookup"><span data-stu-id="803c2-139">You can also use third-party CI/CD solutions such as Jenkins.</span></span> 

<span data-ttu-id="803c2-140">**Helm**.</span><span class="sxs-lookup"><span data-stu-id="803c2-140">**Helm**.</span></span> <span data-ttu-id="803c2-141">Helm &mdash; это диспетчер пакетов для Kubernetes, который предоставляет способ упаковки объектов Kubernetes в единый элемент, который можно публиковать, развертывать, обновлять и изменять.</span><span class="sxs-lookup"><span data-stu-id="803c2-141">Helm is as a package manager for Kubernetes &mdash; a way to bundle Kubernetes objects into a single unit that you can publish, deploy, version, and update.</span></span>

<span data-ttu-id="803c2-142">**Azure Monitor**.</span><span class="sxs-lookup"><span data-stu-id="803c2-142">**Azure Monitor**.</span></span> <span data-ttu-id="803c2-143">Azure Monitor собирает и хранит метрики и журналы, включая метрики платформы, для служб Azure в решении и телеметрии приложений.</span><span class="sxs-lookup"><span data-stu-id="803c2-143">Azure Monitor collects and stores metrics and logs, including platform metrics for the Azure services in the solution and application telemetry.</span></span> <span data-ttu-id="803c2-144">С помощью этих данных можно отслеживать приложения, настраивать оповещения и панели мониторинга, а также анализировать первопричины сбоев.</span><span class="sxs-lookup"><span data-stu-id="803c2-144">Use this data to monitor the application, set up alerts and dashboards, and perform root cause analysis of failures.</span></span> <span data-ttu-id="803c2-145">Azure Monitor интегрируется с AKS для сбора метрик контроллеров, узлов и контейнеров, а также журналов контейнеров и журналов главных узлов.</span><span class="sxs-lookup"><span data-stu-id="803c2-145">Azure Monitor integrates with AKS to collect metrics from controllers, nodes, and containers, as well as container logs and master node logs.</span></span>

## <a name="design-considerations"></a><span data-ttu-id="803c2-146">Рекомендации по проектированию</span><span class="sxs-lookup"><span data-stu-id="803c2-146">Design considerations</span></span>

<span data-ttu-id="803c2-147">Эта эталонная архитектура ориентирована на архитектуры микрослужб, хотя многие из рекомендуемых методов будут применяться к другим рабочим нагрузкам, выполняемым в AKS.</span><span class="sxs-lookup"><span data-stu-id="803c2-147">This reference architecture is focused on microservices architectures, although many of the recommended practices will apply to other workloads running on AKS.</span></span>

### <a name="microservices"></a><span data-ttu-id="803c2-148">Микрослужбы</span><span class="sxs-lookup"><span data-stu-id="803c2-148">Microservices</span></span>

<span data-ttu-id="803c2-149">Объект Службы Kubernetes предоставляет естественный способ моделирования микрослужб в Kubernetes.</span><span class="sxs-lookup"><span data-stu-id="803c2-149">The Kubernetes Service object is a natural way to model microservices in Kubernetes.</span></span> <span data-ttu-id="803c2-150">Микрослужба — это слабо связанная, независимо развертываемая единица кода.</span><span class="sxs-lookup"><span data-stu-id="803c2-150">A microservice is a loosely coupled, independently deployable unit of code.</span></span> <span data-ttu-id="803c2-151">Микрослужбы обычно взаимодействуют через четко определенные интерфейсы API и могут быть обнаружены с помощью некоторых форм обнаружения служб.</span><span class="sxs-lookup"><span data-stu-id="803c2-151">Microservices typically communicate through well-defined APIs, and are discoverable through some form of service discovery.</span></span> <span data-ttu-id="803c2-152">Объект Службы Kubernetes предоставляет набор возможностей, соответствующих следующим требованиям.</span><span class="sxs-lookup"><span data-stu-id="803c2-152">The Kubernetes Service object provides a set of capabilities that match these requirements:</span></span>

- <span data-ttu-id="803c2-153">IP-адрес.</span><span class="sxs-lookup"><span data-stu-id="803c2-153">IP address.</span></span> <span data-ttu-id="803c2-154">Объект Службы предоставляет статический внутренний IP-адрес для всех групп модулей pod (набор реплик).</span><span class="sxs-lookup"><span data-stu-id="803c2-154">The Service object provides a static internal IP address for a group of pods (ReplicaSet).</span></span> <span data-ttu-id="803c2-155">Когда модули pod создаются или перемещаются, служба всегда доступна по этому внутреннему IP-адресу.</span><span class="sxs-lookup"><span data-stu-id="803c2-155">As pods are created or moved around, the service is always reachable at this internal IP address.</span></span>

- <span data-ttu-id="803c2-156">Балансировка нагрузки.</span><span class="sxs-lookup"><span data-stu-id="803c2-156">Load balancing.</span></span> <span data-ttu-id="803c2-157">Трафик, отправляемый на IP-адрес службы, равномерно распределяется между модулями pod.</span><span class="sxs-lookup"><span data-stu-id="803c2-157">Traffic sent to the service's IP address is load balanced to the pods.</span></span> 

- <span data-ttu-id="803c2-158">Обнаружение служб.</span><span class="sxs-lookup"><span data-stu-id="803c2-158">Service discovery.</span></span> <span data-ttu-id="803c2-159">Служба DNS Kubernetes назначает службам внутренние записи DNS.</span><span class="sxs-lookup"><span data-stu-id="803c2-159">Services are assigned internal DNS entries by the Kubernetes DNS service.</span></span> <span data-ttu-id="803c2-160">Это означает, что шлюз API может вызывать внутреннюю службу по DNS-имени.</span><span class="sxs-lookup"><span data-stu-id="803c2-160">That means the API gateway can call a backend service using the DNS name.</span></span> <span data-ttu-id="803c2-161">Один и тот же механизм может использоваться для связи между службами.</span><span class="sxs-lookup"><span data-stu-id="803c2-161">The same mechanism can be used for service-to-service communication.</span></span> <span data-ttu-id="803c2-162">Записи DNS упорядочены по пространству имен, поэтому если пространства имен соответствуют ограниченным контекстам, то DNS-имя службы будет естественным образом сопоставлено с доменом приложения.</span><span class="sxs-lookup"><span data-stu-id="803c2-162">The DNS entries are organized by namespace, so if your namespaces correspond to bounded contexts, then the DNS name for a service will map naturally to the application domain.</span></span>

<span data-ttu-id="803c2-163">На следующей схеме показаны концептуальные связи между службами и модулями.</span><span class="sxs-lookup"><span data-stu-id="803c2-163">The following diagram show the conceptual relation between services and pods.</span></span> <span data-ttu-id="803c2-164">Фактическое сопоставление с IP-адресами и портами конечных точек осуществляется с помощью kube-proxy, сетевого прокси-сервера Kubernetes.</span><span class="sxs-lookup"><span data-stu-id="803c2-164">The actual mapping to endpoint IP addresses and ports is done by kube-proxy, the Kubernetes network proxy.</span></span>

![Службы и модули pod](./_images/aks-services.png)

### <a name="api-gateway"></a><span data-ttu-id="803c2-166">API Gateway</span><span class="sxs-lookup"><span data-stu-id="803c2-166">API Gateway</span></span>

<span data-ttu-id="803c2-167">*Шлюз API* — это шлюз, который находится между внешними клиентами и микрослужбами.</span><span class="sxs-lookup"><span data-stu-id="803c2-167">An *API gateway* is a gateway that sits between external clients and the microservices.</span></span> <span data-ttu-id="803c2-168">Он выполняет роль обратного прокси-сервера, который перенаправляет запросы от клиентов к микрослужбам.</span><span class="sxs-lookup"><span data-stu-id="803c2-168">It acts as a reverse proxy, routing requests from clients to microservices.</span></span> <span data-ttu-id="803c2-169">Он также может выполнять такие специализированные задачи, как аутентификация, завершение SSL-запросов и ограничение частоты.</span><span class="sxs-lookup"><span data-stu-id="803c2-169">It may also perform various cross-cutting tasks such as authentication, SSL termination, and rate limiting.</span></span> 

<span data-ttu-id="803c2-170">Функциональные возможности, предоставляемые шлюзом, можно группировать следующим образом:</span><span class="sxs-lookup"><span data-stu-id="803c2-170">Functionality provided by a gateway can be grouped as follows:</span></span>

- <span data-ttu-id="803c2-171">[Маршрутизация шлюза.](../../patterns/gateway-routing.md) Маршрутизация клиентских запросов к правильным серверным службам.</span><span class="sxs-lookup"><span data-stu-id="803c2-171">[Gateway Routing](../../patterns/gateway-routing.md): Routing client requests to the right backend services.</span></span> <span data-ttu-id="803c2-172">Шлюз предоставляет одну конечную точку для клиентов и позволяет разделить клиенты и службы.</span><span class="sxs-lookup"><span data-stu-id="803c2-172">This provides a single endpoint for clients, and helps to decouple clients from services.</span></span>

- <span data-ttu-id="803c2-173">[Агрегация шлюза.](../../patterns/gateway-aggregation.md) Объединение нескольких запросов в один, чтобы сократить число вызовов между клиентом и серверной частью.</span><span class="sxs-lookup"><span data-stu-id="803c2-173">[Gateway Aggregation](../../patterns/gateway-aggregation.md): Aggregation of multiple requests into a single request, to reduce chattiness between the client and the backend.</span></span>

- <span data-ttu-id="803c2-174">[Разгрузка шлюза.](../../patterns/gateway-offloading.md)</span><span class="sxs-lookup"><span data-stu-id="803c2-174">[Gateway Offloading](../../patterns/gateway-offloading.md).</span></span> <span data-ttu-id="803c2-175">Шлюз может разгрузить функциональность серверных служб, например завершение подключений SSL, проверка подлинности, помещение IP-адресов в белый список или ограничение частоты клиента (регулирование количества запросов).</span><span class="sxs-lookup"><span data-stu-id="803c2-175">A gateway can offload functionality from the backend services, such as SSL termination, authentication, IP whitelisting, or client rate limiting (throttling).</span></span>

<span data-ttu-id="803c2-176">Шлюзы API представляют собой [общий конструктивный шаблон микрослужб](https://microservices.io/patterns/apigateway.html).</span><span class="sxs-lookup"><span data-stu-id="803c2-176">API gateways are a general [microservices design pattern](https://microservices.io/patterns/apigateway.html).</span></span> <span data-ttu-id="803c2-177">Они могут быть реализованы с использованием ряда различных технологий.</span><span class="sxs-lookup"><span data-stu-id="803c2-177">They can be implemented using a number of different technologies.</span></span> <span data-ttu-id="803c2-178">Вероятно, наиболее распространенной реализацией является развертывание пограничного маршрутизатора или обратного прокси-сервера, например Nginx, HAProxy или Traefik, внутри кластера.</span><span class="sxs-lookup"><span data-stu-id="803c2-178">Probably the most common implementation is to deploy an edge router or reverse proxy, such as Nginx, HAProxy, or Traefik, inside the cluster.</span></span> 

<span data-ttu-id="803c2-179">Другие доступные варианты:</span><span class="sxs-lookup"><span data-stu-id="803c2-179">Other options include:</span></span>

- <span data-ttu-id="803c2-180">Шлюз приложений Azure или служба управления API Azure, которые являются управляемыми службами, расположенными за пределами кластера.</span><span class="sxs-lookup"><span data-stu-id="803c2-180">Azure Application Gateway and/or Azure API-Management, which are both managed services that reside outside of the cluster.</span></span> <span data-ttu-id="803c2-181">Контроллер входящего трафика Шлюза приложений в настоящее время предоставляется в бета-версии.</span><span class="sxs-lookup"><span data-stu-id="803c2-181">An Application Gateway Ingress Controller is currently in beta.</span></span>

- <span data-ttu-id="803c2-182">Прокси-серверы Функций Azure.</span><span class="sxs-lookup"><span data-stu-id="803c2-182">Azure Functions Proxies.</span></span> <span data-ttu-id="803c2-183">Прокси-серверы могут изменять запросы и ответы, а также направлять запросы на основе URL-адреса.</span><span class="sxs-lookup"><span data-stu-id="803c2-183">Proxies can modify requests and responses and route requests based on URL.</span></span>

<span data-ttu-id="803c2-184">Тип ресурса Kubernetes **Входящий трафик** абстрагирует параметры конфигурации для прокси-сервера.</span><span class="sxs-lookup"><span data-stu-id="803c2-184">The Kubernetes **Ingress** resource type abstracts the configuration settings for a proxy server.</span></span> <span data-ttu-id="803c2-185">Он работает совместно с контроллером входящего трафика, который обеспечивает базовую реализацию этого трафика.</span><span class="sxs-lookup"><span data-stu-id="803c2-185">It works in conjunction with an ingress controller, which provides the underlying implementation of the Ingress.</span></span> <span data-ttu-id="803c2-186">Среди прочего предоставляются контроллеры входящего трафика для Nginx, HAProxy, Traefik и Шлюза приложений (предварительная версия).</span><span class="sxs-lookup"><span data-stu-id="803c2-186">There are ingress controllers for Nginx, HAProxy, Traefik, and Application Gateway (preview), among others.</span></span>

<span data-ttu-id="803c2-187">Контроллер входящего трафика обрабатывает настройку прокси-сервера.</span><span class="sxs-lookup"><span data-stu-id="803c2-187">The ingress controller handles configuring the proxy server.</span></span> <span data-ttu-id="803c2-188">Часто это требует сложных файлов конфигурации, которые трудно настроить, если вы не эксперт, так что входной контроллер является хорошей абстракцией.</span><span class="sxs-lookup"><span data-stu-id="803c2-188">Often these require complex configuration files, which can be hard to tune if you aren't an expert, so the ingress controller is a nice abstraction.</span></span> <span data-ttu-id="803c2-189">Кроме того, контроллер входящего трафика имеет доступ к API Kubernetes, поэтому он может принимать интеллектуальные решения о маршрутизации и балансировке нагрузки.</span><span class="sxs-lookup"><span data-stu-id="803c2-189">In addition, the Ingress Controller has access to the Kubernetes API, so it can make intelligent decisions about routing and load balancing.</span></span> <span data-ttu-id="803c2-190">Например, контроллер входящего трафика Nginx обходит прокси-сервер сети kube-proxy.</span><span class="sxs-lookup"><span data-stu-id="803c2-190">For example, the Nginx ingress controller bypasses the kube-proxy network proxy.</span></span>

<span data-ttu-id="803c2-191">С другой стороны, если вам нужен полный контроль над параметрами, вы можете обойти эту абстракцию и настроить прокси-сервер вручную.</span><span class="sxs-lookup"><span data-stu-id="803c2-191">On the other hand, if you need complete control over the settings, you may want to bypass this abstraction and configure the proxy server manually.</span></span> 

<span data-ttu-id="803c2-192">Обратный прокси-сервер является потенциально проблемным местом или единой точкой сбоя, поэтому всегда следует развертывать как минимум две реплики для обеспечения высокого уровня доступности.</span><span class="sxs-lookup"><span data-stu-id="803c2-192">A reverse proxy server is a potential bottleneck or single point of failure, so always deploy at least two replicas for high availability.</span></span>

### <a name="data-storage"></a><span data-ttu-id="803c2-193">Хранилище данных</span><span class="sxs-lookup"><span data-stu-id="803c2-193">Data storage</span></span>

<span data-ttu-id="803c2-194">В архитектуре микрослужб службы не должны совместно использовать хранилище данных.</span><span class="sxs-lookup"><span data-stu-id="803c2-194">In a microservices architecture, services should not share data storage.</span></span> <span data-ttu-id="803c2-195">Каждая служба должна владеть собственными личными данными в отдельном логическом хранилище, чтобы избежать скрытых зависимостей между службами.</span><span class="sxs-lookup"><span data-stu-id="803c2-195">Each service should own its own private data in a separate logical storage, to avoid hidden dependencies among services.</span></span> <span data-ttu-id="803c2-196">Это позволяет устранить непреднамеренную взаимозависимость между службами, которая может возникнуть, когда службы совместно используют одни и те же базовые схемы данных.</span><span class="sxs-lookup"><span data-stu-id="803c2-196">The reason is to avoid unintentional coupling between services, which can happen when services share the same underlying data schemas.</span></span> <span data-ttu-id="803c2-197">Кроме того, когда службы управляют собственными хранилищами данных, они могут использовать правильное хранилище данных для своих конкретных требований.</span><span class="sxs-lookup"><span data-stu-id="803c2-197">Also, when services manage their own data stores, they can use the right data store for their particular requirements.</span></span> <span data-ttu-id="803c2-198">Дополнительные сведения см. в статье [Проектирование микрослужб: рекомендации по работе с данными](/azure/architecture/microservices/data-considerations).</span><span class="sxs-lookup"><span data-stu-id="803c2-198">For more information, see [Designing microservices: Data considerations](/azure/architecture/microservices/data-considerations).</span></span>

<span data-ttu-id="803c2-199">Избегайте хранения постоянных данных в локальном хранилище кластера, так как это связывает данные с узлом.</span><span class="sxs-lookup"><span data-stu-id="803c2-199">Avoid storing persistent data in local cluster storage, because that ties the data to the node.</span></span> <span data-ttu-id="803c2-200">Вот как следует поступить вместо этого:</span><span class="sxs-lookup"><span data-stu-id="803c2-200">Instead,</span></span> 

- <span data-ttu-id="803c2-201">Используйте внешнюю службу, такую как База данных SQL Azure или Cosmos DB, *или*</span><span class="sxs-lookup"><span data-stu-id="803c2-201">Use an external service such as Azure SQL Database or Cosmos DB, *or*</span></span>

- <span data-ttu-id="803c2-202">подключите постоянный том с применением Дисков или Файлов Azure.</span><span class="sxs-lookup"><span data-stu-id="803c2-202">Mount a persistent volume using Azure Disks or Azure Files.</span></span> <span data-ttu-id="803c2-203">Используйте Файлы Azure, если один и тот же том должен быть общим для нескольких модулей.</span><span class="sxs-lookup"><span data-stu-id="803c2-203">Use Azure Files if the same volume needs to be shared by multiple pods.</span></span>

### <a name="namespaces"></a><span data-ttu-id="803c2-204">Пространства имен</span><span class="sxs-lookup"><span data-stu-id="803c2-204">Namespaces</span></span>

<span data-ttu-id="803c2-205">Пространства имен используются для организации служб в кластере.</span><span class="sxs-lookup"><span data-stu-id="803c2-205">Use namespaces to organize services within the cluster.</span></span> <span data-ttu-id="803c2-206">Каждый объект в кластере Kubernetes принадлежит пространству имен.</span><span class="sxs-lookup"><span data-stu-id="803c2-206">Every object in a Kubernetes cluster belongs to a namespace.</span></span> <span data-ttu-id="803c2-207">Когда создается новый объект, он по умолчанию назначается пространству имен `default`.</span><span class="sxs-lookup"><span data-stu-id="803c2-207">By default, when you create a new object, it goes into the `default` namespace.</span></span> <span data-ttu-id="803c2-208">Тем не менее рекомендуется создавать пространства имен, которые являются более описательными. Это позволяет упорядочить ресурсы в кластере.</span><span class="sxs-lookup"><span data-stu-id="803c2-208">But it's a good practice to create namespaces that are more descriptive to help organize the resources in the cluster.</span></span>

<span data-ttu-id="803c2-209">Во-первых, пространства имен помогают предотвратить конфликты имен.</span><span class="sxs-lookup"><span data-stu-id="803c2-209">First, namespaces help prevent naming collisions.</span></span> <span data-ttu-id="803c2-210">Когда несколько команд развертывают микрослужбы в одном кластере, содержащем сотни микрослужб, ими становится трудно управлять, если все они принадлежат одному пространству имен.</span><span class="sxs-lookup"><span data-stu-id="803c2-210">When multiple teams deploy microservices into the same cluster, with possibly hundreds of microservices, it gets hard to manage if they all go into the same namespace.</span></span> <span data-ttu-id="803c2-211">Кроме того, пространства имен позволяют:</span><span class="sxs-lookup"><span data-stu-id="803c2-211">In addition, namespaces allow you to:</span></span>

- <span data-ttu-id="803c2-212">Применять ограничения ресурсов к пространству имен, чтобы общий набор модулей, назначенных этому пространству имен, не превышал квоту ресурсов пространства.</span><span class="sxs-lookup"><span data-stu-id="803c2-212">Apply resource constraints to a namespace, so that the total set of pods assigned to that namespace cannot exceed the resource quota of the namespace.</span></span>

- <span data-ttu-id="803c2-213">Примените политики на уровне пространства имен, включая RBAC и политики безопасности.</span><span class="sxs-lookup"><span data-stu-id="803c2-213">Apply policies at the namespace level, including RBAC and security policies.</span></span>

<span data-ttu-id="803c2-214">Для архитектуры микрослужб организуйте микрослужбы в ограниченных контекстах и создайте пространства имен для каждого контекста.</span><span class="sxs-lookup"><span data-stu-id="803c2-214">For a microservices architecture, considering organizing the microservices into bounded contexts, and creating namespaces for each bounded context.</span></span> <span data-ttu-id="803c2-215">Например, все микрослужбы, связанные с ограниченным контекстом "выполнение заказа", можно поместить в одно и то же пространство имен.</span><span class="sxs-lookup"><span data-stu-id="803c2-215">For example, all microservices related to the "Order Fulfillment" bounded context could go into the same namespace.</span></span> <span data-ttu-id="803c2-216">Кроме того, можно создать пространство имен для каждой группы разработчиков.</span><span class="sxs-lookup"><span data-stu-id="803c2-216">Alternatively, create a namespace for each development team.</span></span>

<span data-ttu-id="803c2-217">Поместите службы программ в отдельное пространство имен.</span><span class="sxs-lookup"><span data-stu-id="803c2-217">Place utility services into their own separate namespace.</span></span> <span data-ttu-id="803c2-218">Например, вы можете развернуть Elasticsearch или Prometheus для мониторинга кластера или Tiller для Helm.</span><span class="sxs-lookup"><span data-stu-id="803c2-218">For example, you might deploy Elasticsearch or Prometheus for cluster monitoring, or Tiller for Helm.</span></span>

## <a name="scalability-considerations"></a><span data-ttu-id="803c2-219">Вопросы масштабируемости</span><span class="sxs-lookup"><span data-stu-id="803c2-219">Scalability considerations</span></span>

<span data-ttu-id="803c2-220">Kubernetes поддерживает горизонтальное масштабирование на двух уровнях:</span><span class="sxs-lookup"><span data-stu-id="803c2-220">Kubernetes supports scale-out at two levels:</span></span>

- <span data-ttu-id="803c2-221">Масштабирование количества модулей, выделенных для развертывания.</span><span class="sxs-lookup"><span data-stu-id="803c2-221">Scale the number of pods allocated to a deployment.</span></span>
- <span data-ttu-id="803c2-222">Масштабирование узлов в кластере для увеличения общего объема вычислительных ресурсов, доступных в кластере.</span><span class="sxs-lookup"><span data-stu-id="803c2-222">Scale the nodes in the cluster, to increase the total compute resources available to the cluster.</span></span>

<span data-ttu-id="803c2-223">Хотя можно масштабировать модули и узлы вручную, рекомендуется использовать автомасштабирование, чтобы свести к минимуму вероятность того, что службы будут испытывать нехватку ресурсов при высокой нагрузке.</span><span class="sxs-lookup"><span data-stu-id="803c2-223">Although you can scale out pods and nodes manually, we recommend using autoscaling, to minimize the chance that services will become resource starved under high load.</span></span> <span data-ttu-id="803c2-224">Стратегия автомасштабирования должна учитывать как модули, так и узлы.</span><span class="sxs-lookup"><span data-stu-id="803c2-224">An autoscaling strategy must take both pods and nodes into account.</span></span> <span data-ttu-id="803c2-225">Если вы просто масштабируете модули, в конечном итоге вы достигнете предела ресурсов на узлах.</span><span class="sxs-lookup"><span data-stu-id="803c2-225">If you just scale out the pods, eventually you will reach the resource limits of the nodes.</span></span> 

### <a name="pod-autoscaling"></a><span data-ttu-id="803c2-226">Автомасштабирование модулей</span><span class="sxs-lookup"><span data-stu-id="803c2-226">Pod autoscaling</span></span>

<span data-ttu-id="803c2-227">Средство горизонтального автомасштабирования pod (HPA) масштабирует модули на основе наблюдаемых метрик ЦП, памяти или пользовательских метрик.</span><span class="sxs-lookup"><span data-stu-id="803c2-227">The Horizontal Pod Autoscaler (HPA) scales pods based on observed CPU, memory, or custom metrics.</span></span> <span data-ttu-id="803c2-228">Чтобы настроить горизонтальное масштабирование модулей, необходимо указать целевую метрику (например, 70 % потребления ЦП), а также минимальное и максимальное число реплик.</span><span class="sxs-lookup"><span data-stu-id="803c2-228">To configure horizontal pod scaling, you specify a target metric (for example, 70% of CPU), and the minimum and maximum number of replicas.</span></span> <span data-ttu-id="803c2-229">Для получения этих чисел необходимо выполнить нагрузочное тестирование служб.</span><span class="sxs-lookup"><span data-stu-id="803c2-229">You should load test your services to derive these numbers.</span></span>

<span data-ttu-id="803c2-230">Побочным эффектом автомасштабирования является то, что модули могут создаваться или исключаться чаще, так как происходят события увеличения и уменьшения масштаба.</span><span class="sxs-lookup"><span data-stu-id="803c2-230">A side-effect of autoscaling is that pods may be created or evicted more frequently, as scale-out and scale-in events happen.</span></span> <span data-ttu-id="803c2-231">Чтобы уменьшить это влияние:</span><span class="sxs-lookup"><span data-stu-id="803c2-231">To mitigate the effects of this:</span></span>

- <span data-ttu-id="803c2-232">Используйте пробы готовности, чтобы сообщить Kubernetes, когда новый модуль будет готов принять трафик.</span><span class="sxs-lookup"><span data-stu-id="803c2-232">Use readiness probes to let Kubernetes know when a new pod is ready to accept traffic.</span></span>
- <span data-ttu-id="803c2-233">Используйте бюджеты неработоспособности, чтобы ограничить количество модулей, которые могут быть исключены из службы за раз.</span><span class="sxs-lookup"><span data-stu-id="803c2-233">Use pod disruption budgets to limit how many pods can be evicted from a service at a time.</span></span>

### <a name="cluster-autoscaling"></a><span data-ttu-id="803c2-234">Автоматическое масштабирование кластера</span><span class="sxs-lookup"><span data-stu-id="803c2-234">Cluster autoscaling</span></span>

<span data-ttu-id="803c2-235">Средство автомасштабирования кластера масштабирует количество узлов.</span><span class="sxs-lookup"><span data-stu-id="803c2-235">The cluster autoscaler scales the number of nodes.</span></span> <span data-ttu-id="803c2-236">Если модули не могут быть запланированы из-за ограничений ресурсов, это средство будет подготавливать больше узлов.</span><span class="sxs-lookup"><span data-stu-id="803c2-236">If pods can't be scheduled because of resource constraints, the cluster autoscaler will provision more nodes.</span></span>  <span data-ttu-id="803c2-237">(Примечание. Интеграция между AKS и средством автомасштабирования кластера в настоящее время предоставляется в предварительной версии.)</span><span class="sxs-lookup"><span data-stu-id="803c2-237">(Note: Integration between AKS and the cluster autoscaler is currently in preview.)</span></span>

<span data-ttu-id="803c2-238">В то время как HPA отслеживает фактически потребляемые ресурсы или другие метрики из запущенных модулей, средство автомасштабирования кластера подготавливает узлы для модулей, которые еще не запланированы.</span><span class="sxs-lookup"><span data-stu-id="803c2-238">Whereas HPA looks at actual resources consumed or other metrics from running pods, the cluster autoscaler is provisioning nodes for pods that aren't scheduled yet.</span></span> <span data-ttu-id="803c2-239">Поэтому оно рассматривает запрошенные ресурсы, как указано в спецификации развертывания модулей pod Kubernetes.</span><span class="sxs-lookup"><span data-stu-id="803c2-239">Therefore, it looks at the requested resources, as specified in the Kubernetes pod spec for a deployment.</span></span> <span data-ttu-id="803c2-240">Используйте нагрузочное тестирование для точной настройки этих значений.</span><span class="sxs-lookup"><span data-stu-id="803c2-240">Use load testing to fine-tune these values.</span></span>

<span data-ttu-id="803c2-241">После создания кластера нельзя изменить размер виртуальной машины, поэтому необходимо выполнить начальное планирование емкости, чтобы выбрать подходящий размер виртуальной машины для узлов агентов во время создания кластера.</span><span class="sxs-lookup"><span data-stu-id="803c2-241">You can't change the VM size after you create the cluster, so you should do some initial capacity planning to choose an appropriate VM size for the agent nodes when you create the cluster.</span></span> 

## <a name="availability-considerations"></a><span data-ttu-id="803c2-242">Вопросы доступности</span><span class="sxs-lookup"><span data-stu-id="803c2-242">Availability considerations</span></span>

### <a name="health-probes"></a><span data-ttu-id="803c2-243">Пробы работоспособности.</span><span class="sxs-lookup"><span data-stu-id="803c2-243">Health probes</span></span>

<span data-ttu-id="803c2-244">Kubernetes определяет два типа проб работоспособности, доступных для модуля:</span><span class="sxs-lookup"><span data-stu-id="803c2-244">Kubernetes defines two types of health probe that a pod can expose:</span></span>

- <span data-ttu-id="803c2-245">Проба готовности: сообщает Kubernetes, готов ли модуль к приему запросов.</span><span class="sxs-lookup"><span data-stu-id="803c2-245">Readiness probe: Tells Kubernetes whether the pod is ready to accept requests.</span></span>

- <span data-ttu-id="803c2-246">Проба активности: сообщает Kubernetes, следует ли удалить модуль и запустить новый экземпляр.</span><span class="sxs-lookup"><span data-stu-id="803c2-246">Liveness probe: Tells Kubernetes whether a pod should be removed and a new instance started.</span></span>

<span data-ttu-id="803c2-247">При рассмотрении проб нужно помнить о принципах работы службы в Kubernetes.</span><span class="sxs-lookup"><span data-stu-id="803c2-247">When thinking about probes, it's useful to recall how a service works in Kubernetes.</span></span> <span data-ttu-id="803c2-248">Служба имеет селектор метки, который соответствует набору модулей (ноль или больше).</span><span class="sxs-lookup"><span data-stu-id="803c2-248">A service has a label selector that matches a set of (zero or more) pods.</span></span> <span data-ttu-id="803c2-249">Kubernetes распределяет нагрузку трафика между модулями, которые совпадают с селектором.</span><span class="sxs-lookup"><span data-stu-id="803c2-249">Kubernetes load balances traffic to the pods that match the selector.</span></span> <span data-ttu-id="803c2-250">Только модули, которые запущены успешно и являются работоспособными, получают трафик.</span><span class="sxs-lookup"><span data-stu-id="803c2-250">Only pods that started successfully and are healthy receive traffic.</span></span> <span data-ttu-id="803c2-251">Если контейнер аварийно завершает работу, Kubernetes завершает работу модуля и планирует замену.</span><span class="sxs-lookup"><span data-stu-id="803c2-251">If a container crashes, Kubernetes kills the pod and schedules a replacement.</span></span>

<span data-ttu-id="803c2-252">Иногда модуль может быть не готов к получению трафика, даже если он был запущен успешно.</span><span class="sxs-lookup"><span data-stu-id="803c2-252">Sometimes, a pod may not be ready to receive traffic, even though the pod started successfully.</span></span> <span data-ttu-id="803c2-253">Например, могут быть задачи инициализации, когда приложение, работающее в контейнере, загружает данные в память или считывает данные конфигурации.</span><span class="sxs-lookup"><span data-stu-id="803c2-253">For example, there may be initialization tasks, where the application running in the container loads things into memory or reads configuration data.</span></span> <span data-ttu-id="803c2-254">Чтобы указать, что модуль исправен, но не готов к получению трафика, определите пробу готовности.</span><span class="sxs-lookup"><span data-stu-id="803c2-254">To indicate that a pod is healthy but not ready to receive traffic, define a readiness probe.</span></span> 

<span data-ttu-id="803c2-255">Пробы активности обрабатывают случай, когда модуль все еще работает, но не работоспособен и должен быть перезапущен.</span><span class="sxs-lookup"><span data-stu-id="803c2-255">Liveness probes handle the case where a pod is still running, but is unhealthy and should be recycled.</span></span> <span data-ttu-id="803c2-256">Например, предположим, что контейнер обслуживает HTTP-запросы, но по какой-то причине зависает.</span><span class="sxs-lookup"><span data-stu-id="803c2-256">For example, suppose that a container is serving HTTP requests but hangs for some reason.</span></span> <span data-ttu-id="803c2-257">Контейнер не завершит работу аварийно, но он перестанет обрабатывать любые запросы.</span><span class="sxs-lookup"><span data-stu-id="803c2-257">The container doesn't crash, but it has stopped serving any requests.</span></span> <span data-ttu-id="803c2-258">Если вы определите пробу активности HTTP, проба перестанет отвечать на запросы, что укажет Kubernetes на необходимость перезагрузки модуля.</span><span class="sxs-lookup"><span data-stu-id="803c2-258">If you define an HTTP liveness probe, the probe will stop responding and that informs Kubernetes to restart the pod.</span></span>

<span data-ttu-id="803c2-259">Вот некоторые соображения при проектировании проб:</span><span class="sxs-lookup"><span data-stu-id="803c2-259">Here are some considerations when designing probes:</span></span>

- <span data-ttu-id="803c2-260">Если код имеет длительное время запуска, существует опасность, что проба активности сообщит об ошибке до завершения запуска.</span><span class="sxs-lookup"><span data-stu-id="803c2-260">If your code has a long startup time, there is a danger that a liveness probe will report failure before the startup completes.</span></span> <span data-ttu-id="803c2-261">Чтобы избежать этого, используйте параметр initialDelaySeconds, который задерживает запуск пробы.</span><span class="sxs-lookup"><span data-stu-id="803c2-261">To prevent this, use the initialDelaySeconds setting, which delays the probe from starting.</span></span>

- <span data-ttu-id="803c2-262">Проба активности не помогает, если только перезапуск модуля не восстановит его работоспособное состояние.</span><span class="sxs-lookup"><span data-stu-id="803c2-262">A liveness probe doesn't help unless restarting the pod is likely to restore it to a healthy state.</span></span> <span data-ttu-id="803c2-263">Пробу активности можно использовать для предотвращения утечек памяти или неожиданных взаимоблокировок, но нет смысла перезапускать модуль, который немедленно завершится сбоем.</span><span class="sxs-lookup"><span data-stu-id="803c2-263">You can use a liveness probe to mitigate against memory leaks or unexpected deadlocks, but there's no point in restarting a pod that's going to immediately fail again.</span></span>

- <span data-ttu-id="803c2-264">Иногда пробы готовности используются для проверки зависимых служб.</span><span class="sxs-lookup"><span data-stu-id="803c2-264">Sometimes readiness probes are used to check dependent services.</span></span> <span data-ttu-id="803c2-265">Например, если модуль имеет зависимость от базы данных, проба активности может проверить подключение к базе данных.</span><span class="sxs-lookup"><span data-stu-id="803c2-265">For example, if a pod has a dependency on a database, the liveness probe might check the database connection.</span></span> <span data-ttu-id="803c2-266">Однако такой подход может вызывать неожиданные проблемы.</span><span class="sxs-lookup"><span data-stu-id="803c2-266">However, this approach can create unexpected problems.</span></span> <span data-ttu-id="803c2-267">Внешняя служба может быть временно недоступна по какой-либо причине.</span><span class="sxs-lookup"><span data-stu-id="803c2-267">An external service might be temporarily unavailable for some reason.</span></span> <span data-ttu-id="803c2-268">В результате проба готовности завершится сбоем для всех модулей в службе, что приведет к их удалению из подсистемы балансировки нагрузки и, таким образом, к созданию каскадных сбоев вышестоящих ресурсов.</span><span class="sxs-lookup"><span data-stu-id="803c2-268">That will cause the readiness probe to fail for all the pods in your service, causing all of them to be removed from load balancing, and thus creating cascading failures upstream.</span></span> <span data-ttu-id="803c2-269">Лучшим подходом является реализация обработки повторных попыток в службе, чтобы служба могла корректно восстанавливаться после временных сбоев.</span><span class="sxs-lookup"><span data-stu-id="803c2-269">A better approach is to implement retry handling within your service, so that your service can recover correctly from transient failures.</span></span>

### <a name="resource-constraints"></a><span data-ttu-id="803c2-270">Ограничения ресурсов</span><span class="sxs-lookup"><span data-stu-id="803c2-270">Resource constraints</span></span>

<span data-ttu-id="803c2-271">Состязание ресурсов может повлиять на доступность службы.</span><span class="sxs-lookup"><span data-stu-id="803c2-271">Resource contention can affect the availability of a service.</span></span> <span data-ttu-id="803c2-272">Определите ограничения ресурсов для контейнеров, чтобы один контейнер не мог перегружать ресурсы кластера (память и ЦП).</span><span class="sxs-lookup"><span data-stu-id="803c2-272">Define resource constraints for containers, so that a single container cannot overwhelm the cluster resources (memory and CPU).</span></span> <span data-ttu-id="803c2-273">Для ресурсов, не являющихся контейнерами, таких как потоки или сетевые подключения, рассмотрите возможность использования [шаблона отсеков](/azure/architecture/patterns/bulkhead), чтобы изолировать ресурсы.</span><span class="sxs-lookup"><span data-stu-id="803c2-273">For non-container resources, such as threads or network connections, consider using the [Bulkhead Pattern](/azure/architecture/patterns/bulkhead) to isolate resources.</span></span>

<span data-ttu-id="803c2-274">Используйте квоты ресурсов, чтобы ограничить общий объем ресурсов, разрешенных для пространства имен.</span><span class="sxs-lookup"><span data-stu-id="803c2-274">Use resource quotas to limit the total resources allowed for a namespace.</span></span> <span data-ttu-id="803c2-275">Таким образом, внешний интерфейс не может ограничивать ресурсы для серверных служб или наоборот.</span><span class="sxs-lookup"><span data-stu-id="803c2-275">That way, the front end can't starve the backend services for resources or vice-versa.</span></span>

## <a name="security-considerations"></a><span data-ttu-id="803c2-276">Вопросы безопасности</span><span class="sxs-lookup"><span data-stu-id="803c2-276">Security considerations</span></span>

### <a name="role-based-access-control-rbac"></a><span data-ttu-id="803c2-277">управление доступом на основе ролей (RBAC).</span><span class="sxs-lookup"><span data-stu-id="803c2-277">Role based access control (RBAC)</span></span>

<span data-ttu-id="803c2-278">Kubernetes и Azure имеют механизмы управления доступом на основе ролей (RBAC).</span><span class="sxs-lookup"><span data-stu-id="803c2-278">Kubernetes and Azure both have mechanisms for role-based access control (RBAC):</span></span>

- <span data-ttu-id="803c2-279">Azure RBAC контролирует доступ к ресурсам в Azure, включая возможность создания новых ресурсов.</span><span class="sxs-lookup"><span data-stu-id="803c2-279">Azure RBAC controls access to resources in Azure, including the ability to create new Azure resources.</span></span> <span data-ttu-id="803c2-280">Разрешения могут назначаться пользователям, группам или субъектам-службам.</span><span class="sxs-lookup"><span data-stu-id="803c2-280">Permissions can be assigned to users, groups, or service principals.</span></span> <span data-ttu-id="803c2-281">(Субъект-служба является удостоверением безопасности, используемым приложениями.)</span><span class="sxs-lookup"><span data-stu-id="803c2-281">(A service principal is a security identity used by applications.)</span></span>

- <span data-ttu-id="803c2-282">Kubernetes RBAC контролирует разрешения для API Kubernetes.</span><span class="sxs-lookup"><span data-stu-id="803c2-282">Kubernetes RBAC controls permissions to the Kubernetes API.</span></span> <span data-ttu-id="803c2-283">Например, создание и перечисление модулей — это действия, которые могут быть авторизованы (или запрещены) для пользователя с помощью RBAC.</span><span class="sxs-lookup"><span data-stu-id="803c2-283">For example, creating pods and listing pods are actions that can be authorized (or denied) to a user through RBAC.</span></span> <span data-ttu-id="803c2-284">Чтобы назначить пользователям разрешения Kubernetes, создайте *роли* и *привязки ролей*:</span><span class="sxs-lookup"><span data-stu-id="803c2-284">To assign Kubernetes permissions to users, you create *roles* and *role bindings*:</span></span>

  - <span data-ttu-id="803c2-285">Роль — это набор разрешений, применяемых в пространстве имен.</span><span class="sxs-lookup"><span data-stu-id="803c2-285">A Role is a set of permissions that apply within a namespace.</span></span> <span data-ttu-id="803c2-286">Разрешения определяются как команды (получение, обновление, создание, удаление), относящиеся к ресурсам (модули, развертывания и т. д.).</span><span class="sxs-lookup"><span data-stu-id="803c2-286">Permissions are defined as verbs (get, update, create, delete) on resources (pods, deployments, etc.).</span></span>

  - <span data-ttu-id="803c2-287">RoleBinding назначает пользователям или группам роль.</span><span class="sxs-lookup"><span data-stu-id="803c2-287">A RoleBinding assigns users or groups to a Role.</span></span>

  - <span data-ttu-id="803c2-288">Существует также объект ClusterRole, который похож на роль, но применяется ко всему кластеру во всех пространствах имен.</span><span class="sxs-lookup"><span data-stu-id="803c2-288">There is also a ClusterRole object, which is like a Role but applies to the entire cluster, across all namespaces.</span></span> <span data-ttu-id="803c2-289">Чтобы назначить ClusterRole пользователям или группам, создайте ClusterRoleBinding.</span><span class="sxs-lookup"><span data-stu-id="803c2-289">To assign users or groups to a ClusterRole, create a ClusterRoleBinding.</span></span>

<span data-ttu-id="803c2-290">AKS интегрирует оба этих механизма RBAC.</span><span class="sxs-lookup"><span data-stu-id="803c2-290">AKS integrates these two RBAC mechanisms.</span></span> <span data-ttu-id="803c2-291">При создании кластера AKS можно настроить его на проверку подлинности пользователя с помощью Azure AD.</span><span class="sxs-lookup"><span data-stu-id="803c2-291">When you create an AKS cluster, you can configure it to use Azure AD for user authentication.</span></span> <span data-ttu-id="803c2-292">Дополнительные сведения о том, как выполнить такую настройку, см. в статье [Интеграция Azure Active Directory со Службой Azure Kubernetes](/azure/aks/aad-integration).</span><span class="sxs-lookup"><span data-stu-id="803c2-292">For details on how to set this up, see [Integrate Azure Active Directory with Azure Kubernetes Service](/azure/aks/aad-integration).</span></span>

<span data-ttu-id="803c2-293">Когда это будет настроено, пользователь, который хочет получить доступ к API Kubernetes (например, через kubectl), должен войти, используя учетные данные Azure AD.</span><span class="sxs-lookup"><span data-stu-id="803c2-293">Once this is configured, a user who wants to access the Kubernetes API (for example, through kubectl) must sign in using their Azure AD credentials.</span></span>

<span data-ttu-id="803c2-294">По умолчанию пользователь Azure AD не имеет доступа к кластеру.</span><span class="sxs-lookup"><span data-stu-id="803c2-294">By default, an Azure AD user has no access to the cluster.</span></span> <span data-ttu-id="803c2-295">Чтобы предоставить доступ, администратор кластера создает RoleBindings, которые ссылаются на пользователей или группы Azure AD.</span><span class="sxs-lookup"><span data-stu-id="803c2-295">To grant access, the cluster administrator creates RoleBindings that refer to Azure AD users or groups.</span></span> <span data-ttu-id="803c2-296">Если у пользователя нет разрешений для конкретной операции, она завершится сбоем.</span><span class="sxs-lookup"><span data-stu-id="803c2-296">If a user doesn't have permissions for a particular operation, it will fail.</span></span>

<span data-ttu-id="803c2-297">Если у пользователей нет доступа по умолчанию, как администратор кластера получает разрешение на создание привязки ролей?</span><span class="sxs-lookup"><span data-stu-id="803c2-297">If users have no access by default, how does the cluster admin have permission to create the role bindings in the first place?</span></span> <span data-ttu-id="803c2-298">На самом деле кластер AKS имеет два типа учетных данных для вызова сервера API Kubernetes: пользователь кластера и администратор кластера. Учетные данные администратора кластера предоставляют полный доступ к кластеру.</span><span class="sxs-lookup"><span data-stu-id="803c2-298">An AKS cluster actually has two types of credentials for calling the Kubernetes API server: cluster user and cluster admin. The cluster admin credentials grant full access to the cluster.</span></span> <span data-ttu-id="803c2-299">Команда Azure CLI `az aks get-credentials --admin` загружает учетные данные администратора кластера и сохраняет их в файле kubeconfig.</span><span class="sxs-lookup"><span data-stu-id="803c2-299">The Azure CLI command `az aks get-credentials --admin` downloads the cluster admin credentials and saves them into your kubeconfig file.</span></span> <span data-ttu-id="803c2-300">Администратор кластера может использовать этот файл для создания ролей и привязок ролей.</span><span class="sxs-lookup"><span data-stu-id="803c2-300">The cluster administrator can use this kubeconfig to create roles and role bindings.</span></span>

<span data-ttu-id="803c2-301">Так как учетные данные администратора кластера имеют такие разрешения, используйте Azure RBAC для ограничения доступа к ним:</span><span class="sxs-lookup"><span data-stu-id="803c2-301">Because the cluster admin credentials are so powerful, use Azure RBAC to restrict access to them:</span></span>

- <span data-ttu-id="803c2-302">"Роль администратора кластера службы Azure Kubernetes" имеет разрешение на скачивание учетных данных администратора кластера.</span><span class="sxs-lookup"><span data-stu-id="803c2-302">The "Azure Kubernetes Service Cluster Admin Role" has permission to download the cluster admin credentials.</span></span> <span data-ttu-id="803c2-303">Эта роль должна быть назначена только администраторам кластера.</span><span class="sxs-lookup"><span data-stu-id="803c2-303">Only cluster administrators should be assigned to this role.</span></span>

- <span data-ttu-id="803c2-304">"Роль пользователя кластера службы Azure Kubernetes" имеет разрешение на скачивание учетных данных пользователя кластера.</span><span class="sxs-lookup"><span data-stu-id="803c2-304">The "Azure Kubernetes Service Cluster User Role" has permission to download the cluster user credentials.</span></span> <span data-ttu-id="803c2-305">Пользователям, не являющимся администраторами, может быть назначена эта роль.</span><span class="sxs-lookup"><span data-stu-id="803c2-305">Non-admin users can be assigned to this role.</span></span> <span data-ttu-id="803c2-306">Эта роль не дает никаких особых разрешений на использование ресурсов Kubernetes в кластере &mdash; она просто позволяет пользователю подключаться к серверу API.</span><span class="sxs-lookup"><span data-stu-id="803c2-306">This role does not give any particular permissions on Kubernetes resources inside the cluster &mdash; it just allows a user to connect to the API server.</span></span> 

<span data-ttu-id="803c2-307">При определении политик RBAC (как Kubernetes, так и Azure) следует учитывать роли в организации:</span><span class="sxs-lookup"><span data-stu-id="803c2-307">When you define your RBAC policies (both Kubernetes and Azure), think about the roles in your organization:</span></span>

- <span data-ttu-id="803c2-308">Кто может создавать или удалять кластер AKS и загружать учетные данные администратора?</span><span class="sxs-lookup"><span data-stu-id="803c2-308">Who can create or delete an AKS cluster and download the admin credentials?</span></span>
- <span data-ttu-id="803c2-309">Кто может администрировать кластер?</span><span class="sxs-lookup"><span data-stu-id="803c2-309">Who can administer a cluster?</span></span>
- <span data-ttu-id="803c2-310">Кто может создавать или обновлять ресурсы в пространстве имен?</span><span class="sxs-lookup"><span data-stu-id="803c2-310">Who can create or update resources within a namespace?</span></span>

<span data-ttu-id="803c2-311">Рекомендуется назначать разрешения Kubernetes RBAC по пространству имен, используя роли и RoleBindings, а не ClusterRoles и ClusterRoleBindings.</span><span class="sxs-lookup"><span data-stu-id="803c2-311">It's a good practice to scope Kubernetes RBAC permissions by namespace, using Roles and RoleBindings, rather than ClusterRoles and ClusterRoleBindings.</span></span>

<span data-ttu-id="803c2-312">Наконец, возникает вопрос о том, какие разрешения необходимы кластеру AKS для создания и администрирования ресурсов Azure, таких как подсистемы балансировки нагрузки, сеть или хранилище.</span><span class="sxs-lookup"><span data-stu-id="803c2-312">Finally, there is the question of what permissions the AKS cluster has to create and manage Azure resources, such as load balancers, networking, or storage.</span></span> <span data-ttu-id="803c2-313">Для проверки подлинности в интерфейсах API Azure в кластере используется субъект-служба Azure AD.</span><span class="sxs-lookup"><span data-stu-id="803c2-313">To authenticate itself with Azure APIs, the cluster uses an Azure AD service principal.</span></span> <span data-ttu-id="803c2-314">Если субъект-служба не указан при создании кластера, он создается автоматически.</span><span class="sxs-lookup"><span data-stu-id="803c2-314">If you don't specify a service principal when you create the cluster, one is created automatically.</span></span> <span data-ttu-id="803c2-315">Однако рекомендуется сначала создать субъект-службу и назначить ему минимальные разрешения RBAC.</span><span class="sxs-lookup"><span data-stu-id="803c2-315">However, it's a good security practice to create the service principal first and assign the minimal RBAC permissions to it.</span></span> <span data-ttu-id="803c2-316">Дополнительные сведения см. в статье [Субъекты-службы со службой Azure Kubernetes](/azure/aks/kubernetes-service-principal).</span><span class="sxs-lookup"><span data-stu-id="803c2-316">For more information, see [Service principals with Azure Kubernetes Service](/azure/aks/kubernetes-service-principal).</span></span>

### <a name="secrets-management-and-application-credentials"></a><span data-ttu-id="803c2-317">Управление секретами и учетные данные приложения</span><span class="sxs-lookup"><span data-stu-id="803c2-317">Secrets management and application credentials</span></span>

<span data-ttu-id="803c2-318">Приложениям и службам часто требуются учетные данные, позволяющие им подключаться к внешним службам, таким как служба хранилища Azure или База данных SQL.</span><span class="sxs-lookup"><span data-stu-id="803c2-318">Applications and services often need credentials that allow them to connect to external services such as Azure Storage or SQL Database.</span></span> <span data-ttu-id="803c2-319">Задача состоит в том, чтобы сохранить эти учетные данные безопасными и предотвратить их утечку.</span><span class="sxs-lookup"><span data-stu-id="803c2-319">The challenge is to keep these credentials safe and not leak them.</span></span> 

<span data-ttu-id="803c2-320">Для ресурсов Azure следует использовать управляемые удостоверения.</span><span class="sxs-lookup"><span data-stu-id="803c2-320">For Azure resources, one option is to use managed identities.</span></span> <span data-ttu-id="803c2-321">Идея управляемого удостоверения заключается в том, что приложение или служба имеет удостоверение, хранящееся в Azure AD, и использует его для проверки подлинности в службе Azure.</span><span class="sxs-lookup"><span data-stu-id="803c2-321">The idea of a managed identity is that an application or service has an identity stored in Azure AD, and uses this identity to authenticate with an Azure service.</span></span> <span data-ttu-id="803c2-322">Приложение или служба имеет субъект-службу, созданный для него в Azure AD, и выполняет проверку подлинности с помощью токенов OAuth 2.0.</span><span class="sxs-lookup"><span data-stu-id="803c2-322">The application or service has a Service Principal created for it in Azure AD, and authenticates using OAuth 2.0 tokens.</span></span> <span data-ttu-id="803c2-323">Выполняющийся процесс вызывает адрес localhost для получения токена.</span><span class="sxs-lookup"><span data-stu-id="803c2-323">The executing process calls a localhost address to get the token.</span></span> <span data-ttu-id="803c2-324">Таким образом, вам не нужно хранить пароли или строки подключения.</span><span class="sxs-lookup"><span data-stu-id="803c2-324">That way, you don't need to store any passwords or connection strings.</span></span> <span data-ttu-id="803c2-325">Управляемые удостоверения можно использовать в AKS путем присвоения их отдельным модулям с помощью проекта [aad-pod-identity](https://github.com/Azure/aad-pod-identity).</span><span class="sxs-lookup"><span data-stu-id="803c2-325">You can use managed identities in AKS by assigning identities to individual pods, using the [aad-pod-identity](https://github.com/Azure/aad-pod-identity) project.</span></span>

<span data-ttu-id="803c2-326">В настоящее время не все службы Azure поддерживают проверку подлинности с помощью управляемых удостоверений.</span><span class="sxs-lookup"><span data-stu-id="803c2-326">Currently, not all Azure services support authentication using managed identities.</span></span> <span data-ttu-id="803c2-327">Список см. в статье [Службы с поддержкой управляемых удостоверений для ресурсов Azure](/azure/active-directory/managed-identities-azure-resources/services-support-msi).</span><span class="sxs-lookup"><span data-stu-id="803c2-327">For a list, see [Azure services that support Azure AD authentication](/azure/active-directory/managed-identities-azure-resources/services-support-msi).</span></span>

<span data-ttu-id="803c2-328">Даже при использовании управляемых удостоверений, вероятно, потребуется хранить некоторые учетные данные или другие секреты приложений для служб Azure, которые не поддерживают управляемые удостоверения, сторонние службы, ключи API и т. д.</span><span class="sxs-lookup"><span data-stu-id="803c2-328">Even with managed identities, you'll probably need to store some credentials or other application secrets, whether for Azure services that don't support managed identities, third-party services, API keys, and so on.</span></span> <span data-ttu-id="803c2-329">Ниже приведены несколько вариантов безопасного хранения секретов:</span><span class="sxs-lookup"><span data-stu-id="803c2-329">Here are some options for storing secrets securely:</span></span>

- <span data-ttu-id="803c2-330">Azure Key Vault.</span><span class="sxs-lookup"><span data-stu-id="803c2-330">Azure Key Vault.</span></span> <span data-ttu-id="803c2-331">В AKS можно подключить один или несколько секретов из Key Vault в качестве тома.</span><span class="sxs-lookup"><span data-stu-id="803c2-331">In AKS, you can mount one or more secrets from Key Vault as a volume.</span></span> <span data-ttu-id="803c2-332">Том считывает секреты из хранилища ключей.</span><span class="sxs-lookup"><span data-stu-id="803c2-332">The volume reads the secrets from Key Vault.</span></span> <span data-ttu-id="803c2-333">Модуль может считывать секреты так же, как обычный том.</span><span class="sxs-lookup"><span data-stu-id="803c2-333">The pod can then read the secrets just like a regular volume.</span></span> <span data-ttu-id="803c2-334">Дополнительные сведения см. в проекте [Kubernetes-KeyVault-FlexVolume](https://github.com/Azure/kubernetes-keyvault-flexvol) на GitHub.</span><span class="sxs-lookup"><span data-stu-id="803c2-334">For more information, see the [Kubernetes-KeyVault-FlexVolume](https://github.com/Azure/kubernetes-keyvault-flexvol) project on GitHub.</span></span>

    <span data-ttu-id="803c2-335">Модуль проходит проверку подлинности с помощью удостоверения модуля (описанного выше) или с помощью субъекта-службы Azure AD вместе с секретом клиента.</span><span class="sxs-lookup"><span data-stu-id="803c2-335">The pod authenticates itself by using either a pod identity (described above) or by using an Azure AD Service Principal along with a client secret.</span></span> <span data-ttu-id="803c2-336">Рекомендуется использовать удостоверения модулей, потому что в этом случае секрет клиента не требуется.</span><span class="sxs-lookup"><span data-stu-id="803c2-336">Using pod identities is recommended because the client secret isn't needed in that case.</span></span> 

- <span data-ttu-id="803c2-337">Хранилище HashiCorp.</span><span class="sxs-lookup"><span data-stu-id="803c2-337">HashiCorp Vault.</span></span> <span data-ttu-id="803c2-338">Приложения Kubernetes могут проходить аутентификацию в хранилище HashiCorp с помощью управляемых удостоверений Azure AD.</span><span class="sxs-lookup"><span data-stu-id="803c2-338">Kubernetes applications can authenticate with HashiCorp Vault using Azure AD managed identities.</span></span> <span data-ttu-id="803c2-339">Дополнительные сведения см. в статье [HashiCorp Vault speaks Azure Active Directory](https://open.microsoft.com/2018/04/10/scaling-tips-hashicorp-vault-azure-active-directory/) (Использование хранилища HashiCorp в Azure Active Directory).</span><span class="sxs-lookup"><span data-stu-id="803c2-339">See [HashiCorp Vault speaks Azure Active Directory](https://open.microsoft.com/2018/04/10/scaling-tips-hashicorp-vault-azure-active-directory/).</span></span> <span data-ttu-id="803c2-340">Можно развернуть хранилище в Kubernetes, но рекомендуется запускать его в отдельном выделенном кластере из кластера приложений.</span><span class="sxs-lookup"><span data-stu-id="803c2-340">You can deploy Vault itself to Kubernetes, but it's recommend to run it in a separate dedicated cluster from your application cluster.</span></span> 

- <span data-ttu-id="803c2-341">Секреты Kubernetes.</span><span class="sxs-lookup"><span data-stu-id="803c2-341">Kubernetes secrets.</span></span> <span data-ttu-id="803c2-342">Можно также просто использовать секреты Kubernetes.</span><span class="sxs-lookup"><span data-stu-id="803c2-342">Another option is simply to use Kubernetes secrets.</span></span> <span data-ttu-id="803c2-343">Этот вариант настроить проще всего, но он имеет некоторые проблемы.</span><span class="sxs-lookup"><span data-stu-id="803c2-343">This option is the easiest to configure but has some challenges.</span></span> <span data-ttu-id="803c2-344">Секреты хранятся в хранилище etcd, которое является распределенным хранилищем "ключ — значение".</span><span class="sxs-lookup"><span data-stu-id="803c2-344">Secrets are stored in etcd, which is a distributed key-value store.</span></span> <span data-ttu-id="803c2-345">AKS [шифрует неактивные данные etcd](https://github.com/Azure/kubernetes-kms#azure-kubernetes-service-aks).</span><span class="sxs-lookup"><span data-stu-id="803c2-345">AKS [encrypts etcd at rest](https://github.com/Azure/kubernetes-kms#azure-kubernetes-service-aks).</span></span> <span data-ttu-id="803c2-346">Ключами шифрования управляет корпорация Майкрософт.</span><span class="sxs-lookup"><span data-stu-id="803c2-346">Microsoft manages the encryption keys.</span></span>

<span data-ttu-id="803c2-347">Использование таких хранилищ, как HashiCorp или Azure Key Vault, предоставляет ряд преимуществ:</span><span class="sxs-lookup"><span data-stu-id="803c2-347">Using a system like HashiCorp Vault or Azure Key Vault provides several advantages, such as:</span></span>

- <span data-ttu-id="803c2-348">централизованное управление секретами;</span><span class="sxs-lookup"><span data-stu-id="803c2-348">Centralized control of secrets.</span></span>
- <span data-ttu-id="803c2-349">все секреты шифруются в неактивном состоянии;</span><span class="sxs-lookup"><span data-stu-id="803c2-349">Ensuring that all secrets are encrypted at rest.</span></span>
- <span data-ttu-id="803c2-350">централизованное управление ключами;</span><span class="sxs-lookup"><span data-stu-id="803c2-350">Centralized key management.</span></span>
- <span data-ttu-id="803c2-351">управление доступом к секретам;</span><span class="sxs-lookup"><span data-stu-id="803c2-351">Access control of secrets.</span></span>
- <span data-ttu-id="803c2-352">Аудит</span><span class="sxs-lookup"><span data-stu-id="803c2-352">Auditing</span></span>

### <a name="pod-and-container-security"></a><span data-ttu-id="803c2-353">Безопасность контейнеров и модулей</span><span class="sxs-lookup"><span data-stu-id="803c2-353">Pod and container security</span></span>

<span data-ttu-id="803c2-354">Ниже приведены некоторые рекомендации по обеспечению безопасности ваших модулей и контейнеров (этот список не является исчерпывающим).</span><span class="sxs-lookup"><span data-stu-id="803c2-354">This list is certainly not exhaustive, but here are some recommended practices for securing your pods and containers:</span></span> 

<span data-ttu-id="803c2-355">Не запускайте контейнеры в привилегированном режиме.</span><span class="sxs-lookup"><span data-stu-id="803c2-355">Don't run containers in privileged mode.</span></span> <span data-ttu-id="803c2-356">Привилегированный режим предоставляет контейнерам доступ ко всем устройствам на узле.</span><span class="sxs-lookup"><span data-stu-id="803c2-356">Privileged mode gives a container access to all devices on the host.</span></span> <span data-ttu-id="803c2-357">Можно задать политику безопасности модулей, чтобы запретить выполнение контейнеров в привилегированном режиме.</span><span class="sxs-lookup"><span data-stu-id="803c2-357">You can set Pod Security Policy to disallow containers from running in privileged mode.</span></span> 

<span data-ttu-id="803c2-358">По возможности избегайте запуска процессов с правами привилегированного пользователя внутри контейнеров.</span><span class="sxs-lookup"><span data-stu-id="803c2-358">When possible, avoid running processes as root inside containers.</span></span> <span data-ttu-id="803c2-359">Контейнеры не обеспечивают полную изоляцию с точки зрения безопасности, поэтому лучше запускать контейнерный процесс с правами непривилегированного пользователя.</span><span class="sxs-lookup"><span data-stu-id="803c2-359">Containers do not provide complete isolation from a security standpoint, so it's better to run a container process as a non-privileged user.</span></span> 

<span data-ttu-id="803c2-360">Храните образы в доверенном частном реестре, например в Реестре контейнеров Azure или в доверенном реестре Docker.</span><span class="sxs-lookup"><span data-stu-id="803c2-360">Store images in a trusted private registry, such as Azure Container Registry or Docker Trusted Registry.</span></span> <span data-ttu-id="803c2-361">Используйте проверяющий веб-перехватчик допуска в Kubernetes, чтобы модули могли извлекать образы только из доверенного реестра.</span><span class="sxs-lookup"><span data-stu-id="803c2-361">Use a validating admission webhook in Kubernetes to ensure that pods can only pull images from the trusted registry.</span></span>

<span data-ttu-id="803c2-362">Сканируйте образы на наличие известных уязвимостей с помощью таких решений, как Twistlock и Aqua, которые доступны в Azure Marketplace.</span><span class="sxs-lookup"><span data-stu-id="803c2-362">Scan images for known vulnerabilities, using a scanning solution such as Twistlock and Aqua, which are available through the Azure Marketplace.</span></span>

<span data-ttu-id="803c2-363">Автоматизируйте исправления образов с помощью Задач ACR, компонента Реестра контейнеров Azure.</span><span class="sxs-lookup"><span data-stu-id="803c2-363">Automate image patching using ACR Tasks, a feature of Azure Container Registry.</span></span> <span data-ttu-id="803c2-364">Образ контейнера строится из уровней.</span><span class="sxs-lookup"><span data-stu-id="803c2-364">A container image is built up from layers.</span></span> <span data-ttu-id="803c2-365">Базовые уровни включают образ ОС и образы платформы приложений, такие как ASP.NET Core или Node.js.</span><span class="sxs-lookup"><span data-stu-id="803c2-365">The base layers include the OS image and application framework images, such as ASP.NET Core or Node.js.</span></span> <span data-ttu-id="803c2-366">Базовые образы обычно создаются на уровень выше от разработчиков приложений и поддерживаются другими администраторами проектов.</span><span class="sxs-lookup"><span data-stu-id="803c2-366">The base images are typically created upstream from the application developers, and are maintained by other project maintainers.</span></span> <span data-ttu-id="803c2-367">Если эти образы исправляются на вышестоящем уровне, важно обновлять, тестировать и повторно развертывать собственные образы, чтобы не осталось известных уязвимостей безопасности.</span><span class="sxs-lookup"><span data-stu-id="803c2-367">When these images are patched upstream, it's important to update, test, and redeploy your own images, so that you don't leave any known security vulnerabilities.</span></span> <span data-ttu-id="803c2-368">Задачи ACR могут помочь автоматизировать этот процесс.</span><span class="sxs-lookup"><span data-stu-id="803c2-368">ACR Tasks can help to automate this process.</span></span>

## <a name="deployment-cicd-considerations"></a><span data-ttu-id="803c2-369">Рекомендации по развертыванию (CI/CD)</span><span class="sxs-lookup"><span data-stu-id="803c2-369">Deployment (CI/CD) considerations</span></span>

<span data-ttu-id="803c2-370">Ниже приведены некоторые цели надежного процесса CI/CD для архитектуры микрослужб:</span><span class="sxs-lookup"><span data-stu-id="803c2-370">Here are some goals of a robust CI/CD process for a microservices architecture:</span></span>

- <span data-ttu-id="803c2-371">Каждая команда может создавать и развертывать службы, которыми она владеет независимо, не влияя на другие команды и не нарушая их работу.</span><span class="sxs-lookup"><span data-stu-id="803c2-371">Each team can build and deploy the services that it owns independently, without affecting or disrupting other teams.</span></span>

- <span data-ttu-id="803c2-372">Перед развертыванием новой версии службы в рабочей среде она развертывается в средах разработки, тестирования и контроля качества для проверки.</span><span class="sxs-lookup"><span data-stu-id="803c2-372">Before a new version of a service is deployed to production, it gets deployed to dev/test/QA environments for validation.</span></span> <span data-ttu-id="803c2-373">Шлюзы качества применяются на каждом этапе.</span><span class="sxs-lookup"><span data-stu-id="803c2-373">Quality gates are enforced at each stage.</span></span>

- <span data-ttu-id="803c2-374">Новая версия службы может развертываться параллельно с предыдущей версией.</span><span class="sxs-lookup"><span data-stu-id="803c2-374">A new version of a service can be deployed side-by-side with the previous version.</span></span>

- <span data-ttu-id="803c2-375">Заданы достаточные политики контроля доступа.</span><span class="sxs-lookup"><span data-stu-id="803c2-375">Sufficient access control policies are in place.</span></span>

- <span data-ttu-id="803c2-376">Образам контейнеров, развернутым в рабочей среде, можно доверять.</span><span class="sxs-lookup"><span data-stu-id="803c2-376">You can trust the container images that are deployed to production.</span></span>

### <a name="isolation-of-environments"></a><span data-ttu-id="803c2-377">Изоляция сред</span><span class="sxs-lookup"><span data-stu-id="803c2-377">Isolation of environments</span></span>

<span data-ttu-id="803c2-378">У вас будет несколько сред, в которых вы развертываете службы, включая среды для разработки, тестирования проверки сборки, интеграционного и нагрузочного тестирования и, наконец, рабочую среду.</span><span class="sxs-lookup"><span data-stu-id="803c2-378">You will have multiple environments where you deploy services, including environments for development, smoke testing, integration testing, load testing, and finally production.</span></span> <span data-ttu-id="803c2-379">Эти среды нуждаются в некоторой степени изоляции.</span><span class="sxs-lookup"><span data-stu-id="803c2-379">These environments need some level of isolation.</span></span> <span data-ttu-id="803c2-380">В Kubernetes у вас есть выбор между физической и логической изоляцией.</span><span class="sxs-lookup"><span data-stu-id="803c2-380">In Kubernetes, you have a choice between physical isolation and logical isolation.</span></span> <span data-ttu-id="803c2-381">Физическая изоляция означает развертывание в отдельных кластерах.</span><span class="sxs-lookup"><span data-stu-id="803c2-381">Physical isolation means deploying to separate clusters.</span></span> <span data-ttu-id="803c2-382">При логической изоляции используются пространства имен и политики, как описано выше.</span><span class="sxs-lookup"><span data-stu-id="803c2-382">Logical isolation makes use of namespaces and policies, as described earlier.</span></span>

<span data-ttu-id="803c2-383">Рекомендуется создать выделенный рабочий кластер вместе с отдельным кластером для сред разработки и тестирования.</span><span class="sxs-lookup"><span data-stu-id="803c2-383">Our recommendation is to create a dedicated production cluster along with a separate cluster for your dev/test environments.</span></span> <span data-ttu-id="803c2-384">Используйте логическую изоляцию для разделения сред в кластере разработки и тестирования.</span><span class="sxs-lookup"><span data-stu-id="803c2-384">Use logical isolation to separate environments within the dev/test cluster.</span></span> <span data-ttu-id="803c2-385">У служб, развернутых в кластере разработки и тестирования, никогда не должно быть доступа к хранилищам данных, в которых хранятся бизнес-данные.</span><span class="sxs-lookup"><span data-stu-id="803c2-385">Services deployed to the dev/test cluster should never have access to data stores that hold business data.</span></span> 

### <a name="helm"></a><span data-ttu-id="803c2-386">Helm</span><span class="sxs-lookup"><span data-stu-id="803c2-386">Helm</span></span>

<span data-ttu-id="803c2-387">С помощью Helm можно управлять созданием и развертыванием служб.</span><span class="sxs-lookup"><span data-stu-id="803c2-387">Consider using Helm to manage building and deploying services.</span></span> <span data-ttu-id="803c2-388">Некоторые возможности Helm, которые помогают с CI/CD:</span><span class="sxs-lookup"><span data-stu-id="803c2-388">Some of the features of Helm that help with CI/CD include:</span></span>

- <span data-ttu-id="803c2-389">Упорядочение всех объектов Kubernetes для определенной микрослужбы в одну диаграмму Helm.</span><span class="sxs-lookup"><span data-stu-id="803c2-389">Organizing all of the Kubernetes objects for a particular microservice into a single Helm chart.</span></span>
- <span data-ttu-id="803c2-390">Развертывание диаграммы с помощью одной команды Helm, а не ряда команд kubectl.</span><span class="sxs-lookup"><span data-stu-id="803c2-390">Deploying the chart as a single helm command, rather than a series of kubectl commands.</span></span>
- <span data-ttu-id="803c2-391">Отслеживание обновлений и исправлений с использованием семантических версий, а также возможность отката к предыдущей версии.</span><span class="sxs-lookup"><span data-stu-id="803c2-391">Tracking updates and revisions, using semantic versioning, along with the ability to roll back to a previous version.</span></span>
- <span data-ttu-id="803c2-392">Использование шаблонов во избежание дублирования данных, таких как метки и селекторы, в нескольких файлах.</span><span class="sxs-lookup"><span data-stu-id="803c2-392">The use of templates to avoid duplicating information, such as labels and selectors, across many files.</span></span>
- <span data-ttu-id="803c2-393">Управление зависимостями между диаграммами.</span><span class="sxs-lookup"><span data-stu-id="803c2-393">Managing dependencies between charts.</span></span>
- <span data-ttu-id="803c2-394">Публикация диаграмм в репозитории Helm, например в Реестре контейнеров Azure, и их интеграция с конвейером сборки.</span><span class="sxs-lookup"><span data-stu-id="803c2-394">Publishing charts to a Helm repository, such as Azure Container Registry, and integrating them with the build pipeline.</span></span>

<span data-ttu-id="803c2-395">Дополнительные сведения об использовании Реестра контейнеров Helm см. в статье [Использование Реестра контейнеров Azure в качестве репозитория Helm для диаграмм приложения](/azure/container-registry/container-registry-helm-repos).</span><span class="sxs-lookup"><span data-stu-id="803c2-395">For more information about using Container Registry as a Helm repository, see [Use Azure Container Registry as a Helm repository for your application charts](/azure/container-registry/container-registry-helm-repos).</span></span>

### <a name="cicd-workflow"></a><span data-ttu-id="803c2-396">Рабочий процесс CI/CD</span><span class="sxs-lookup"><span data-stu-id="803c2-396">CI/CD workflow</span></span>

<span data-ttu-id="803c2-397">Прежде чем создавать рабочий процесс CI/CD, необходимо знать, как будет структурирована база кода и как ею управлять.</span><span class="sxs-lookup"><span data-stu-id="803c2-397">Before creating a CI/CD workflow, you must know how the code base will be structured and managed.</span></span>

- <span data-ttu-id="803c2-398">Работает команда в отдельном или в едином репозитории?</span><span class="sxs-lookup"><span data-stu-id="803c2-398">Do teams work in separate respositories or in a monorepo (single respository)?</span></span>
- <span data-ttu-id="803c2-399">Что такое стратегия создания ветви?</span><span class="sxs-lookup"><span data-stu-id="803c2-399">What is your branching strategy?</span></span>
- <span data-ttu-id="803c2-400">Кто может отправлять код в рабочую среду?</span><span class="sxs-lookup"><span data-stu-id="803c2-400">Who can push code to production?</span></span> <span data-ttu-id="803c2-401">Существует ли роль менеджера выпуска?</span><span class="sxs-lookup"><span data-stu-id="803c2-401">Is there a release manager role?</span></span>

<span data-ttu-id="803c2-402">Подход единого репозитория получил приоритет, но есть определенные преимущества и недостатки.</span><span class="sxs-lookup"><span data-stu-id="803c2-402">The monorepo approach has been gaining favor but there are advantages and disadvantages to both.</span></span>

| &nbsp; | <span data-ttu-id="803c2-403">Единый репозиторий</span><span class="sxs-lookup"><span data-stu-id="803c2-403">Monorepo</span></span> | <span data-ttu-id="803c2-404">Несколько репозиториев</span><span class="sxs-lookup"><span data-stu-id="803c2-404">Multiple repos</span></span> |
|--------|----------|----------------|
| <span data-ttu-id="803c2-405">**Преимущества**</span><span class="sxs-lookup"><span data-stu-id="803c2-405">**Advantages**</span></span> | <span data-ttu-id="803c2-406">Общий доступ к коду</span><span class="sxs-lookup"><span data-stu-id="803c2-406">Code sharing</span></span><br/><span data-ttu-id="803c2-407">Простой способ стандартизировать код и средства</span><span class="sxs-lookup"><span data-stu-id="803c2-407">Easier to standardize code and tooling</span></span><br/><span data-ttu-id="803c2-408">Простой рефакторинг кода</span><span class="sxs-lookup"><span data-stu-id="803c2-408">Easier to refactor code</span></span><br/><span data-ttu-id="803c2-409">Обнаруживаемость – единое представление кода</span><span class="sxs-lookup"><span data-stu-id="803c2-409">Discoverability - single view of the code</span></span><br/> | <span data-ttu-id="803c2-410">Очистка владельца в команде</span><span class="sxs-lookup"><span data-stu-id="803c2-410">Clear ownership per team</span></span><br/><span data-ttu-id="803c2-411">Потенциально меньше конфликтов слияния</span><span class="sxs-lookup"><span data-stu-id="803c2-411">Potentially fewer merge conflicts</span></span><br/><span data-ttu-id="803c2-412">Помогает применить разделение микрослужб</span><span class="sxs-lookup"><span data-stu-id="803c2-412">Helps to enforce decoupling of microservices</span></span> |
| <span data-ttu-id="803c2-413">**Сложности**</span><span class="sxs-lookup"><span data-stu-id="803c2-413">**Challenges**</span></span> | <span data-ttu-id="803c2-414">Изменение совместного использования кода может повлиять на несколько микрослужб</span><span class="sxs-lookup"><span data-stu-id="803c2-414">Changes to shared code can affect multiple microservices</span></span><br/><span data-ttu-id="803c2-415">Большая вероятность конфликтов слияния</span><span class="sxs-lookup"><span data-stu-id="803c2-415">Greater potential for merge conflicts</span></span><br/><span data-ttu-id="803c2-416">Инструментарий необходимо масштабировать в большую базу кода</span><span class="sxs-lookup"><span data-stu-id="803c2-416">Tooling must scale to a large code base</span></span><br/><span data-ttu-id="803c2-417">управление доступом;</span><span class="sxs-lookup"><span data-stu-id="803c2-417">Access control</span></span><br/><span data-ttu-id="803c2-418">Более сложный процесс развертывания</span><span class="sxs-lookup"><span data-stu-id="803c2-418">More complex deployment process</span></span> | <span data-ttu-id="803c2-419">Труднее совместно использовать код</span><span class="sxs-lookup"><span data-stu-id="803c2-419">Harder to share code</span></span><br/><span data-ttu-id="803c2-420">Труднее применять стандарты кодирования</span><span class="sxs-lookup"><span data-stu-id="803c2-420">Harder to enforce coding standards</span></span><br/><span data-ttu-id="803c2-421">Управление зависимостями</span><span class="sxs-lookup"><span data-stu-id="803c2-421">Dependency management</span></span><br/><span data-ttu-id="803c2-422">Диффузная база кода, низкая обнаруживаемость</span><span class="sxs-lookup"><span data-stu-id="803c2-422">Diffuse code base, poor discoverability</span></span><br/><span data-ttu-id="803c2-423">Отсутствие общей инфраструктуры</span><span class="sxs-lookup"><span data-stu-id="803c2-423">Lack of shared infrastructure</span></span>

<span data-ttu-id="803c2-424">В этом разделе рассматриваются возможные рабочие процессы CI/CD, исходя из следующих предположений.</span><span class="sxs-lookup"><span data-stu-id="803c2-424">In this section, we present a possible CI/CD workflow, based on the following assumptions:</span></span>

- <span data-ttu-id="803c2-425">Репозиторий кода – единый репозиторий с упорядоченными папками микрослужб.</span><span class="sxs-lookup"><span data-stu-id="803c2-425">The code repository is monorepo, with folders organized by microservice.</span></span>
- <span data-ttu-id="803c2-426">Стратегия создания ветви команды основывается на [разработке на основе магистрали](https://trunkbaseddevelopment.com/).</span><span class="sxs-lookup"><span data-stu-id="803c2-426">The team's branching strategy is based on [trunk-based development](https://trunkbaseddevelopment.com/).</span></span>
- <span data-ttu-id="803c2-427">Команды используют [Azure Pipelines](/azure/devops/pipelines) для запуска процесса CI/CD.</span><span class="sxs-lookup"><span data-stu-id="803c2-427">The team uses [Azure Pipelines](/azure/devops/pipelines) to run the CI/CD process.</span></span>
- <span data-ttu-id="803c2-428">В Реестре контейнеров Azure команды используют [пространства имен](/azure/container-registry/container-registry-best-practices#repository-namespaces) для изоляции образов, утвержденных для рабочей среды на основе по-прежнему тестируемых образов.</span><span class="sxs-lookup"><span data-stu-id="803c2-428">The team uses [namespaces](/azure/container-registry/container-registry-best-practices#repository-namespaces) in Azure Container Registry to isolate images that are approved for production from images that are still being tested.</span></span>

<span data-ttu-id="803c2-429">В этом примере разработчик работает над микрослужбой, которая называется "Служба доставки".</span><span class="sxs-lookup"><span data-stu-id="803c2-429">In this example, a developer is working on a microservice called Delivery Service.</span></span> <span data-ttu-id="803c2-430">(Имя из эталонной реализации, описанной [здесь](../../microservices/index.md#the-drone-delivery-application).) При разработке новых компонентов разработчик проверяет код в ветви компонентов.</span><span class="sxs-lookup"><span data-stu-id="803c2-430">(The name comes from the reference implementation described [here](../../microservices/index.md#the-drone-delivery-application).) While developing a new feature, the developer checks code into a feature branch.</span></span>

![Рабочий процесс CI/CD](./_images/aks-cicd-1.png)

<span data-ttu-id="803c2-432">Отправка, зафиксированная в этой ветви, активирует сборку CI для микрослужбы.</span><span class="sxs-lookup"><span data-stu-id="803c2-432">Pushing commits to this branch tiggers a CI build for the microservice.</span></span> <span data-ttu-id="803c2-433">По соглашению компоненты ветвей носят имя `feature/*`.</span><span class="sxs-lookup"><span data-stu-id="803c2-433">By convention, feature branches are named `feature/*`.</span></span> <span data-ttu-id="803c2-434">[Файл определения сборки](/azure/devops/pipelines/yaml-schema) включает триггер, который фильтруется по имени ветви и пути источника.</span><span class="sxs-lookup"><span data-stu-id="803c2-434">The [build definition file](/azure/devops/pipelines/yaml-schema) includes a trigger that filters by the branch name and the source path.</span></span> <span data-ttu-id="803c2-435">Благодаря этому подходу каждая команда может иметь свой собственный конвейер сборки.</span><span class="sxs-lookup"><span data-stu-id="803c2-435">Using this approach, each team can have its own build pipeline.</span></span>

```yaml
trigger:
  batch: true
  branches:
    include:
    - master
    - feature/*

    exclude:
    - feature/experimental/*

  paths:
     include:
     - /src/shipping/delivery/
```

<span data-ttu-id="803c2-436">На этом этапе в рабочем процессе сборка CI выполняет некоторые минимальные проверки кода.</span><span class="sxs-lookup"><span data-stu-id="803c2-436">At this point in the workflow, the CI build runs some minimal code verification:</span></span>

1. <span data-ttu-id="803c2-437">Компиляция кода</span><span class="sxs-lookup"><span data-stu-id="803c2-437">Build code</span></span>
1. <span data-ttu-id="803c2-438">Выполнение модульных тестов</span><span class="sxs-lookup"><span data-stu-id="803c2-438">Run unit tests</span></span>

<span data-ttu-id="803c2-439">Здесь идея состоит в том, чтобы сборка занимала немного времени и разработчик смог получить отзывы быстрее.</span><span class="sxs-lookup"><span data-stu-id="803c2-439">The idea here is to keep the build times short so the developer can get quick feedback.</span></span> <span data-ttu-id="803c2-440">Разработчик открывает PR, когда компонент готов к объединению с главной ветвью.</span><span class="sxs-lookup"><span data-stu-id="803c2-440">When the feature is ready to merge into master, the developer opens a PR.</span></span> <span data-ttu-id="803c2-441">Этим активируется другая сборка CI, которая выполняет некоторые дополнительные проверки.</span><span class="sxs-lookup"><span data-stu-id="803c2-441">This triggers another CI build that performs some additional checks:</span></span>

1. <span data-ttu-id="803c2-442">Компиляция кода</span><span class="sxs-lookup"><span data-stu-id="803c2-442">Build code</span></span>
1. <span data-ttu-id="803c2-443">Выполнение модульных тестов</span><span class="sxs-lookup"><span data-stu-id="803c2-443">Run unit tests</span></span>
1. <span data-ttu-id="803c2-444">Создание образа контейнера среды выполнения</span><span class="sxs-lookup"><span data-stu-id="803c2-444">Build the runtime container image</span></span>
1. <span data-ttu-id="803c2-445">Запуск сканирований уязвимостей в образе</span><span class="sxs-lookup"><span data-stu-id="803c2-445">Run vulnerability scans on the image</span></span>

![Рабочий процесс CI/CD](./_images/aks-cicd-2.png)

> [!NOTE]
> <span data-ttu-id="803c2-447">В Azure Repos можно определить [политики](/azure/devops/repos/git/branch-policies) для защиты ветвей.</span><span class="sxs-lookup"><span data-stu-id="803c2-447">In Azure Repos, you can define [policies](/azure/devops/repos/git/branch-policies) to protect branches.</span></span> <span data-ttu-id="803c2-448">Например, для слияния в главную ветвь политика может требовать успешную сборку CI, а также утверждение от утверждающего.</span><span class="sxs-lookup"><span data-stu-id="803c2-448">For example, the policy could require a successful CI build plus a sign-off from an approver in order to merge into master.</span></span>

<span data-ttu-id="803c2-449">На некотором этапе группа готова к развертыванию новой версии службы доставки.</span><span class="sxs-lookup"><span data-stu-id="803c2-449">At some point, the team is ready to deploy a new version of the Delivery service.</span></span> <span data-ttu-id="803c2-450">Чтобы сделать это, менеджер по выпускам создает ветвь с главной ветви с таким именем шаблона: `release/<microservice name>/<semver>`.</span><span class="sxs-lookup"><span data-stu-id="803c2-450">To do so, the release manager creates a branch from master with this naming pattern: `release/<microservice name>/<semver>`.</span></span> <span data-ttu-id="803c2-451">Например, `release/delivery/v1.0.2`.</span><span class="sxs-lookup"><span data-stu-id="803c2-451">For example, `release/delivery/v1.0.2`.</span></span>
<span data-ttu-id="803c2-452">Этим активируется полная сборка CI, которая выполняет все предыдущие действия, а также:</span><span class="sxs-lookup"><span data-stu-id="803c2-452">This triggers a full CI build that runs all the previous steps plus:</span></span>

1. <span data-ttu-id="803c2-453">Передает образ Docker в Реестр контейнеров Azure.</span><span class="sxs-lookup"><span data-stu-id="803c2-453">Push the Docker image to Azure Container Registry.</span></span> <span data-ttu-id="803c2-454">Образ отмечен номером версии, взятым из имени ветви.</span><span class="sxs-lookup"><span data-stu-id="803c2-454">The image is tagged with the version number taken from the branch name.</span></span>
2. <span data-ttu-id="803c2-455">Запускает `helm package`, чтобы упаковать диаграмму Helm.</span><span class="sxs-lookup"><span data-stu-id="803c2-455">Run `helm package` to package the Helm chart</span></span>
3. <span data-ttu-id="803c2-456">Передает пакет Helm в реестр контейнеров, запустив `az acr helm push`.</span><span class="sxs-lookup"><span data-stu-id="803c2-456">Push the Helm package to Container Registry by running `az acr helm push`.</span></span>

<span data-ttu-id="803c2-457">При условии, что эта сборка завершается успешно, она запускает процесс развертывания с помощью [конвейера выпуска](/azure/devops/pipelines/release/what-is-release-management) Azure Pipelines.</span><span class="sxs-lookup"><span data-stu-id="803c2-457">Assuming this build succeeds, it triggers a deployment process using an Azure Pipelines [release pipeline](/azure/devops/pipelines/release/what-is-release-management).</span></span> <span data-ttu-id="803c2-458">Конвейер данных выполняет приведенные ниже действия.</span><span class="sxs-lookup"><span data-stu-id="803c2-458">This pipeline</span></span>

1. <span data-ttu-id="803c2-459">Запускает `helm upgrade` для развертывания диаграммы Helm в среду контроля качества.</span><span class="sxs-lookup"><span data-stu-id="803c2-459">Run `helm upgrade` to deploy the Helm chart to a QA environment.</span></span>
1. <span data-ttu-id="803c2-460">Утверждающий утверждает, прежде чем пакет переместится в рабочую среду.</span><span class="sxs-lookup"><span data-stu-id="803c2-460">An approver signs off before the package moves to production.</span></span> <span data-ttu-id="803c2-461">Дополнительные сведения см. в статье [Release deployment control using approvals](/azure/devops/pipelines/release/approvals/approvals) (Управление развертыванием выпуска с помощью утверждений).</span><span class="sxs-lookup"><span data-stu-id="803c2-461">See [Release deployment control using approvals](/azure/devops/pipelines/release/approvals/approvals).</span></span>
1. <span data-ttu-id="803c2-462">Повторно маркирует образ Docker для пространства имен рабочей среды в Реестре контейнеров Azure.</span><span class="sxs-lookup"><span data-stu-id="803c2-462">Re-tag the Docker image for the production namespace in Azure Container Registry.</span></span> <span data-ttu-id="803c2-463">Например, если текущий маркер – `myrepo.azurecr.io/delivery:v1.0.2`, рабочим маркером будет `reponame.azurecr.io/prod/delivery:v1.0.2`.</span><span class="sxs-lookup"><span data-stu-id="803c2-463">For example, if the current tag is `myrepo.azurecr.io/delivery:v1.0.2`, the production tag is `reponame.azurecr.io/prod/delivery:v1.0.2`.</span></span>
1. <span data-ttu-id="803c2-464">Запускает `helm upgrade` для развертывания диаграммы Helm в рабочую среду.</span><span class="sxs-lookup"><span data-stu-id="803c2-464">Run `helm upgrade` to deploy the Helm chart to the production environment.</span></span>

![Рабочий процесс CI/CD](./_images/aks-cicd-3.png)

<span data-ttu-id="803c2-466">Важно помнить, что даже в едином репозитории эти задачи могут распространяться на отдельные микрослужбы таким образом, чтобы команды могли с высокой скоростью совершать развертывание.</span><span class="sxs-lookup"><span data-stu-id="803c2-466">It's important to remember that even in a monorepo, these tasks can be scoped to individual microservices, so that teams can deploy with high velocity.</span></span> <span data-ttu-id="803c2-467">Требуется выполнение некоторых действий вручную: утверждение PR, создание ветви выпуска и утверждение развертывания в рабочем кластере.</span><span class="sxs-lookup"><span data-stu-id="803c2-467">There are some manual steps in the process: Approving PRs, creating release branches, and approving deployments into the production cluster.</span></span> <span data-ttu-id="803c2-468">Эти действия выполняются вручную политикой &mdash; и могут быть полностью автоматизированы в соответствии с предпочтениями организации.</span><span class="sxs-lookup"><span data-stu-id="803c2-468">These steps are manual by policy &mdash; they could be completely automated if the organization prefers.</span></span>
