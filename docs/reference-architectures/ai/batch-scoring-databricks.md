---
title: Пакетная оценка моделей Spark в Azure Databricks
description: Создайте масштабируемое решение для пакетной оценки моделей классификации Apache Spark по расписанию с использованием Azure Databricks.
author: njray
ms.date: 02/07/2019
ms.topic: reference-architecture
ms.service: architecture-center
ms.subservice: reference-architecture
ms.custom: azcat-ai
ms.openlocfilehash: 1b6f10edf098ed8d9fa14c16de113fc765372835
ms.sourcegitcommit: a68f248402c598f9d25bc1dc62f27a6a934ff001
ms.translationtype: HT
ms.contentlocale: ru-RU
ms.lasthandoff: 02/08/2019
ms.locfileid: "55903276"
---
# <a name="batch-scoring-of-spark-models-on-azure-databricks"></a><span data-ttu-id="0ace9-103">Пакетная оценка моделей Spark в Azure Databricks</span><span class="sxs-lookup"><span data-stu-id="0ace9-103">Batch scoring of Spark models on Azure Databricks</span></span>

<span data-ttu-id="0ace9-104">Эта эталонная архитектура позволяет создать масштабируемое решение для пакетной оценки модели классификации Apache Spark по расписанию с помощью Azure Databricks, оптимизированной для Azure аналитической платформы на основе Apache Spark.</span><span class="sxs-lookup"><span data-stu-id="0ace9-104">This reference architecture shows how to build a scalable solution for batch scoring an Apache Spark classification model on a schedule using Azure Databricks, an Apache Spark-based analytics platform optimized for Azure.</span></span> <span data-ttu-id="0ace9-105">Вы можете использовать это решение в качестве шаблона, применимого и к другим сценариям.</span><span class="sxs-lookup"><span data-stu-id="0ace9-105">The solution can be used as a template that can be generalized to other scenarios.</span></span>

<span data-ttu-id="0ace9-106">Ссылку на реализацию этой архитектуры можно найти на сайте  [GitHub][github].</span><span class="sxs-lookup"><span data-stu-id="0ace9-106">A reference implementation for this architecture is available on [GitHub][github].</span></span>

![Пакетная оценка моделей Spark в Azure Databricks](./_images/batch-scoring-spark.png)

<span data-ttu-id="0ace9-108">**Сценарий**. Предприятие из отрасли, требующей интенсивного использования ресурсов, намерено сократить расходы и время простоя в связи с непредвиденными физическими сбоями.</span><span class="sxs-lookup"><span data-stu-id="0ace9-108">**Scenario**: A business in an asset-heavy industry wants to minimize the costs and downtime associated with unexpected mechanical failures.</span></span> <span data-ttu-id="0ace9-109">Используя данные Интернета, полученные со своих компьютеров, они могут создавать модели прогнозного обслуживания.</span><span class="sxs-lookup"><span data-stu-id="0ace9-109">Using IoT data collected from their machines, they can create a predictive maintenance model.</span></span> <span data-ttu-id="0ace9-110">Эта модель позволит организации обслуживать и ремонтировать компоненты до того, как они выйдут из строя.</span><span class="sxs-lookup"><span data-stu-id="0ace9-110">This model enables the business to maintain components proactively and repair them before they fail.</span></span> <span data-ttu-id="0ace9-111">Повысив эффективность использования физических компонентов, они смогут снизить расходы и сократить время простоя.</span><span class="sxs-lookup"><span data-stu-id="0ace9-111">By maximizing mechanical component use, they can control costs and reduce downtime.</span></span>

<span data-ttu-id="0ace9-112">Модель прогнозного обслуживания собирает данные с компьютеров и сохраняет зарегистрированные данные о сбоях компонентов.</span><span class="sxs-lookup"><span data-stu-id="0ace9-112">A predictive maintenance model collects data from the machines and retains historical examples of component failures.</span></span> <span data-ttu-id="0ace9-113">Эту модель затем можно применить для отслеживания текущего состояния компонентов и прогнозировать на ее основе вероятность сбоя компонентов в ближайшем будущем.</span><span class="sxs-lookup"><span data-stu-id="0ace9-113">The model can then be used to monitor the current state of the components and predict if a given component will fail in the near future.</span></span> <span data-ttu-id="0ace9-114">Распространенные варианты использования и подходы к моделированию см. в [руководстве по использованию Azure ИИ для создания решений прогнозного обслуживания][ai-guide].</span><span class="sxs-lookup"><span data-stu-id="0ace9-114">For common use cases and modeling approaches, see [Azure AI guide for predictive maintenance solutions][ai-guide].</span></span>

<span data-ttu-id="0ace9-115">Эта эталонная архитектура предназначена для рабочих нагрузок, которые активируются при получении новых данных от физических компонентов.</span><span class="sxs-lookup"><span data-stu-id="0ace9-115">This reference architecture is designed for workloads that are triggered by the presence of new data from the component machines.</span></span> <span data-ttu-id="0ace9-116">Обработка предусматривает указанные ниже действия.</span><span class="sxs-lookup"><span data-stu-id="0ace9-116">Processing involves the following steps:</span></span>

1. <span data-ttu-id="0ace9-117">Прием данных из внешнего хранилища данных в хранилище данных Azure Databricks.</span><span class="sxs-lookup"><span data-stu-id="0ace9-117">Ingest the data from the external data store onto an Azure Databricks data store.</span></span>

2. <span data-ttu-id="0ace9-118">Обучение модели машинного обучения путем преобразования данных в обучающий набор данных с последующим созданием модели Spark MLlib.</span><span class="sxs-lookup"><span data-stu-id="0ace9-118">Train a machine learning model by transforming the data into a training data set, then building a Spark MLlib model.</span></span> <span data-ttu-id="0ace9-119">MLlib содержит распространенные алгоритмы машинного обучения и служебные программы, оптимизированные для использования поддерживаемых в Spark возможностей масштабируемости данных.</span><span class="sxs-lookup"><span data-stu-id="0ace9-119">MLlib consists of most common machine learning algorithms and utilities optimized to take advantage of Spark data scalability capabilities.</span></span>

3. <span data-ttu-id="0ace9-120">Применение обученной модели для прогнозирования (классификация) сбоев компонентов путем преобразования данных в оценочный набор данных.</span><span class="sxs-lookup"><span data-stu-id="0ace9-120">Apply the trained model to predict (classify) component failures by transforming the data into a scoring data set.</span></span> <span data-ttu-id="0ace9-121">Оценка данных с помощью модели Spark MLLib.</span><span class="sxs-lookup"><span data-stu-id="0ace9-121">Score the data with the Spark MLLib model.</span></span>

4. <span data-ttu-id="0ace9-122">Сохранение результатов в хранилище данных Databricks для использования после обработки.</span><span class="sxs-lookup"><span data-stu-id="0ace9-122">Store results on the Databricks data store for post-processing consumption.</span></span>

<span data-ttu-id="0ace9-123">Для каждой из этих задач на  [GitHub][github] доступны готовые записные книжки.</span><span class="sxs-lookup"><span data-stu-id="0ace9-123">Notebooks are provided on [GitHub][github] to perform each of these tasks.</span></span>

## <a name="architecture"></a><span data-ttu-id="0ace9-124">Архитектура</span><span class="sxs-lookup"><span data-stu-id="0ace9-124">Architecture</span></span>

<span data-ttu-id="0ace9-125">Архитектура определяет поток данных, который полностью содержится в [Azure Databricks][databricks] на основе набора последовательно выполняемых [записных книжек][notebooks].</span><span class="sxs-lookup"><span data-stu-id="0ace9-125">The architecture defines a data flow that is entirely contained within [Azure Databricks][databricks] based on a set of sequentially executed [notebooks][notebooks].</span></span> <span data-ttu-id="0ace9-126">Она содержит следующие компоненты.</span><span class="sxs-lookup"><span data-stu-id="0ace9-126">It consists of the following components:</span></span>

<span data-ttu-id="0ace9-127">**[Файлы данных][github]**.</span><span class="sxs-lookup"><span data-stu-id="0ace9-127">**[Data files][github]**.</span></span> <span data-ttu-id="0ace9-128">Эталонная реализация использует имитированный набор данных, содержащийся в пяти файлах статических данных.</span><span class="sxs-lookup"><span data-stu-id="0ace9-128">The reference implementation uses a simulated data set contained in five static data files.</span></span>

<span data-ttu-id="0ace9-129">**[Прием][notebooks]**.</span><span class="sxs-lookup"><span data-stu-id="0ace9-129">**[Ingestion][notebooks]**.</span></span> <span data-ttu-id="0ace9-130">Записная книжка для приема данных скачивает входные файлы данных в коллекцию наборов данных Databricks.</span><span class="sxs-lookup"><span data-stu-id="0ace9-130">The data ingestion notebook downloads the input data files into a collection of Databricks data sets.</span></span> <span data-ttu-id="0ace9-131">В реальном сценарии поток данных с устройств Интернета вещей будет поступать в хранилище, доступное для Databricks, например экземпляр Azure SQL Server или хранилище BLOB-объектов Azure.</span><span class="sxs-lookup"><span data-stu-id="0ace9-131">In a real-world scenario, data from IoT devices would stream onto Databricks-accessible storage such as Azure SQL Server or Azure Blob storage.</span></span> <span data-ttu-id="0ace9-132">Databricks поддерживает несколько [источников данных][data-sources].</span><span class="sxs-lookup"><span data-stu-id="0ace9-132">Databricks supports multiple [data sources][data-sources].</span></span>

<span data-ttu-id="0ace9-133">**Конвейер обучения**.</span><span class="sxs-lookup"><span data-stu-id="0ace9-133">**Training pipeline**.</span></span> <span data-ttu-id="0ace9-134">Эта записная книжка выполняет записную книжку проектирования признаков, чтобы на основе полученных данных создать набор аналитических данных.</span><span class="sxs-lookup"><span data-stu-id="0ace9-134">This notebook executes the feature engineering notebook to create an analysis data set from the ingested data.</span></span> <span data-ttu-id="0ace9-135">Затем выполняется записная книжка создания модели, которая обучает модель машинного обучения с помощью масштабируемой библиотеки машинного обучения [Apache Spark MLlib][mllib].</span><span class="sxs-lookup"><span data-stu-id="0ace9-135">It then executes a model building notebook that trains the machine learning model using the [Apache Spark MLlib][mllib] scalable machine learning library.</span></span>

<span data-ttu-id="0ace9-136">**Конвейер оценки**.</span><span class="sxs-lookup"><span data-stu-id="0ace9-136">**Scoring pipeline**.</span></span> <span data-ttu-id="0ace9-137">Эта записная книжка выполняет записную книжку проектирования признаков, чтобы на основе полученных данных создать набор оценочных данных, и записную книжку оценки.</span><span class="sxs-lookup"><span data-stu-id="0ace9-137">This notebook executes the feature engineering notebook to create scoring data set from the ingested data and executes the scoring notebook.</span></span> <span data-ttu-id="0ace9-138">Записная книжка оценки использует обученную модель [Spark MLlib][mllib-spark] для создания прогнозов для наблюдений в оценочном наборе данных.</span><span class="sxs-lookup"><span data-stu-id="0ace9-138">The scoring notebook uses the trained [Spark MLlib][mllib-spark] model to generate predictions for the observations in the scoring data set.</span></span> <span data-ttu-id="0ace9-139">Эти прогнозы сохраняются в хранилище результатов, для которого создается новый набор данных в хранилище данных Databricks.</span><span class="sxs-lookup"><span data-stu-id="0ace9-139">The predictions are stored in the results store, a new data set on the Databricks data store.</span></span>

<span data-ttu-id="0ace9-140">**Планировщик**.</span><span class="sxs-lookup"><span data-stu-id="0ace9-140">**Scheduler**.</span></span> <span data-ttu-id="0ace9-141">Запланированное [задание][job] Databricks выполняет оценку с использованием модели Spark в пакетном режиме.</span><span class="sxs-lookup"><span data-stu-id="0ace9-141">A scheduled Databricks [job][job] handles batch scoring with the Spark model.</span></span> <span data-ttu-id="0ace9-142">Это задание выполняет записную книжку конвейера оценки, передавая переменные аргументы через параметры записной книжки, а также предоставляя сведения для создания оценочного набора данных и определения расположения для хранения результирующего набора данных.</span><span class="sxs-lookup"><span data-stu-id="0ace9-142">The job executes the scoring pipeline notebook, passing variable arguments through notebook parameters to specify the details for constructing the scoring data set and where to store the results data set.</span></span>

<span data-ttu-id="0ace9-143">Этот сценарий реализован в виде последовательности конвейера.</span><span class="sxs-lookup"><span data-stu-id="0ace9-143">The scenario is constructed as a pipeline flow.</span></span> <span data-ttu-id="0ace9-144">Каждая записная книжка оптимизирована для выполнения в пакетном режиме любой из операций: прием, проектирование признаков, создание модели и оценка модели.</span><span class="sxs-lookup"><span data-stu-id="0ace9-144">Each notebook is optimized to perform in a batch setting for each of the operations: ingestion, feature engineering, model building, and model scorings.</span></span> <span data-ttu-id="0ace9-145">Для этой цели записная книжка проектирования признаков создает большой набор данных, который используется в любой из операций: обучение, калибровка, тестирование и оценка.</span><span class="sxs-lookup"><span data-stu-id="0ace9-145">To accomplish this, the feature engineering notebook is designed to generate a general data set for any of the training, calibration, testing, or scoring operations.</span></span> <span data-ttu-id="0ace9-146">В нашем примере для этих операций используется стратегия темпорального разбиения, то есть параметры записной книжки применяются для фильтрации по заданному диапазону дат.</span><span class="sxs-lookup"><span data-stu-id="0ace9-146">In this scenario, we use a temporal split strategy for these operations, so the notebook parameters are used to set date-range filtering.</span></span>

<span data-ttu-id="0ace9-147">Так как этот сценарий создает конвейер пакетного выполнения, мы предоставляем набор дополнительных записных книжек для просмотра выходных данных, возвращаемых записными книжками конвейера.</span><span class="sxs-lookup"><span data-stu-id="0ace9-147">Because the scenario creates a batch pipeline, we provide a set of optional examination notebooks to explore the output of the pipeline notebooks.</span></span> <span data-ttu-id="0ace9-148">Их можно найти в репозитории GitHub:</span><span class="sxs-lookup"><span data-stu-id="0ace9-148">You can find these in the GitHub repository:</span></span>

- `1a_raw-data_exploring`
- `2a_feature_exploration`
- `2b_model_testing`
- `3b_model_scoring_evaluation`

## <a name="recommendations"></a><span data-ttu-id="0ace9-149">Рекомендации</span><span class="sxs-lookup"><span data-stu-id="0ace9-149">Recommendations</span></span>

<span data-ttu-id="0ace9-150">Платформа Databricks настроена так, чтобы можно было загружать и развертывать обученные модели для создания прогнозов с использованием новых данных.</span><span class="sxs-lookup"><span data-stu-id="0ace9-150">Databricks is set up so you can load and deploy your trained models to make predictions with new data.</span></span> <span data-ttu-id="0ace9-151">Мы использовали для этого примера Databricks из-за следующих дополнительных преимуществ:</span><span class="sxs-lookup"><span data-stu-id="0ace9-151">We used Databricks for this scenario because it provides these additional advantages:</span></span>

- <span data-ttu-id="0ace9-152">поддержка единого входа с использованием учетных данных Azure Active Directory;</span><span class="sxs-lookup"><span data-stu-id="0ace9-152">Single sign-on support using Azure Active Directory credentials.</span></span>
- <span data-ttu-id="0ace9-153">планировщик заданий для выполнения заданий рабочих конвейеров;</span><span class="sxs-lookup"><span data-stu-id="0ace9-153">Job scheduler to execute jobs for production pipelines.</span></span>
- <span data-ttu-id="0ace9-154">полностью интерактивная записная книжка с поддержкой совместной работы, панелей мониторинга и интерфейсов REST API;</span><span class="sxs-lookup"><span data-stu-id="0ace9-154">Fully interactive notebook with collaboration, dashboards, REST APIs.</span></span>
- <span data-ttu-id="0ace9-155">неограниченное количество кластеров с возможностью масштабирования до любого размера;</span><span class="sxs-lookup"><span data-stu-id="0ace9-155">Unlimited clusters that can scale to any size.</span></span>
- <span data-ttu-id="0ace9-156">широкий набор средств безопасности, управление доступом на основе ролей и журналы аудита.</span><span class="sxs-lookup"><span data-stu-id="0ace9-156">Advanced security, role-based access controls, and audit logs.</span></span>

<span data-ttu-id="0ace9-157">Для взаимодействия с платформой Azure Databricks откройте интерфейс [рабочей области][workspace] Databricks в браузере или [интерфейсе командной строки][cli] (CLI).</span><span class="sxs-lookup"><span data-stu-id="0ace9-157">To interact with the Azure Databricks service, use the Databricks [Workspace][workspace] interface in a web browser or the [command-line interface][cli] (CLI).</span></span> <span data-ttu-id="0ace9-158">Доступ к Databricks с помощью CLI возможен с любой платформы, которая поддерживает Python версий 2.7.9–3.6.</span><span class="sxs-lookup"><span data-stu-id="0ace9-158">Access the Databricks CLI from any platform that supports Python 2.7.9 to 3.6.</span></span>

<span data-ttu-id="0ace9-159">Эталонная реализация использует [записные книжки][notebooks] для последовательного выполнения задач.</span><span class="sxs-lookup"><span data-stu-id="0ace9-159">The reference implementation uses [notebooks][notebooks] to execute tasks in sequence.</span></span> <span data-ttu-id="0ace9-160">Каждая записная книжка сохраняет промежуточные артефакты данных (наборы данных для обучения, тестирования, оценки и итоговые результаты) в то же хранилище данных, где расположены входные данные.</span><span class="sxs-lookup"><span data-stu-id="0ace9-160">Each notebook stores intermediate data artifacts (training, test, scoring, or results data sets) to the same data store as the input data.</span></span> <span data-ttu-id="0ace9-161">Это сделано для того, чтобы вам было проще применить эти данные для вашего варианта использования.</span><span class="sxs-lookup"><span data-stu-id="0ace9-161">The goal is to make it easy for you to use it as needed in your particular use case.</span></span> <span data-ttu-id="0ace9-162">На практике вы будете подключать источник данных к экземпляру Azure Databricks, чтобы записные книжки считывали данные непосредственно из хранилища и записывали в него результаты.</span><span class="sxs-lookup"><span data-stu-id="0ace9-162">In practice, you would connect your data source to your Azure Databricks instance for the notebooks to read and write directly back into your storage.</span></span>

<span data-ttu-id="0ace9-163">При желании вы можете отслеживать выполнение заданий с помощью пользовательского интерфейса Databricks, хранилища данных или Databricks [CLI][cli].</span><span class="sxs-lookup"><span data-stu-id="0ace9-163">You can monitor job execution through the Databricks user interface, the data store, or the Databricks [CLI][cli] as necessary.</span></span> <span data-ttu-id="0ace9-164">Для мониторинга кластера доступны [журнал событий][log] и другие [метрики][metrics], предлагаемые Databricks.</span><span class="sxs-lookup"><span data-stu-id="0ace9-164">Monitor the cluster using the [event log][log] and other [metrics][metrics] that Databricks provides.</span></span>

## <a name="performance-considerations"></a><span data-ttu-id="0ace9-165">Рекомендации по производительности</span><span class="sxs-lookup"><span data-stu-id="0ace9-165">Performance considerations</span></span>

<span data-ttu-id="0ace9-166">Кластер Azure Databricks по умолчанию применяет автоматическое масштабирование, то есть во время выполнения Databricks динамически перераспределяет рабочие роли с учетом характеристик конкретного задания.</span><span class="sxs-lookup"><span data-stu-id="0ace9-166">An Azure Databricks cluster enables autoscaling by default so that during runtime, Databricks dynamically reallocates workers to account for the characteristics of your job.</span></span> <span data-ttu-id="0ace9-167">Возможно, некоторым частям конвейера потребуется больше вычислительных ресурсов, чем другим.</span><span class="sxs-lookup"><span data-stu-id="0ace9-167">Certain parts of your pipeline may be more computationally demanding than others.</span></span> <span data-ttu-id="0ace9-168">Databricks добавляет во время этих этапов задания дополнительные рабочие роли, а затем удаляет их по мере необходимости.</span><span class="sxs-lookup"><span data-stu-id="0ace9-168">Databricks adds additional workers during these phases of your job (and removes them when they’re no longer needed).</span></span> <span data-ttu-id="0ace9-169">Автоматическое масштабирование позволяет максимизировать [использование кластера][cluster], ведь вам не придется подготавливать кластер в соответствии с рабочей нагрузкой.</span><span class="sxs-lookup"><span data-stu-id="0ace9-169">Autoscaling makes it easier to achieve high [cluster utilization][cluster], because you don’t need to provision the cluster to match a workload.</span></span>

<span data-ttu-id="0ace9-170">Кроме того, [Фабрика данных Azure][adf] с Azure Databricks позволяет разрабатывать более сложные конвейеры с выполнением по расписанию.</span><span class="sxs-lookup"><span data-stu-id="0ace9-170">Additionally, more complex scheduled pipelines can be developed by using [Azure Data Factory][adf] with Azure Databricks.</span></span>

## <a name="storage-considerations"></a><span data-ttu-id="0ace9-171">Рекомендации по работе с хранилищем</span><span class="sxs-lookup"><span data-stu-id="0ace9-171">Storage considerations</span></span>

<span data-ttu-id="0ace9-172">В этой эталонной реализации данные хранятся непосредственно в хранилище Databricks, чтобы упростить систему.</span><span class="sxs-lookup"><span data-stu-id="0ace9-172">In this reference implementation, the data is stored directly within Databricks storage for simplicity.</span></span> <span data-ttu-id="0ace9-173">Для реальной работы данные можно хранить в облачном хранилище данных, например [хранилище BLOB-объектов Azure][blob].</span><span class="sxs-lookup"><span data-stu-id="0ace9-173">In a production setting, however, the data can be stored on cloud data storage such as [Azure Blob Storage][blob].</span></span> <span data-ttu-id="0ace9-174">Также [Databricks][databricks-connect] поддерживает Azure Data Lake Store, Хранилище данных SQL Azure, Azure Cosmos DB, Apache Kafka и Hadoop.</span><span class="sxs-lookup"><span data-stu-id="0ace9-174">[Databricks][databricks-connect] also supports Azure Data Lake Store, Azure SQL Data Warehouse, Azure Cosmos DB, Apache Kafka, and Hadoop.</span></span>

## <a name="cost-considerations"></a><span data-ttu-id="0ace9-175">Рекомендации по стоимости</span><span class="sxs-lookup"><span data-stu-id="0ace9-175">Cost considerations</span></span>

<span data-ttu-id="0ace9-176">Azure Databricks — это предложение Spark ценовой категории "Премиум" с соответствующими ценами.</span><span class="sxs-lookup"><span data-stu-id="0ace9-176">Azure Databricks is a premium Spark offering with an associated cost.</span></span> <span data-ttu-id="0ace9-177">Кроме того, существуют [ценовые категории][pricing] Databricks "Стандартный" и "Премиум".</span><span class="sxs-lookup"><span data-stu-id="0ace9-177">In addition, there are standard and premium Databricks [pricing tiers][pricing].</span></span>

<span data-ttu-id="0ace9-178">Для нашего примера вполне достаточно ценовой категории "Стандартный".</span><span class="sxs-lookup"><span data-stu-id="0ace9-178">For this scenario, the standard pricing tier is sufficient.</span></span> <span data-ttu-id="0ace9-179">Но если для вашего приложения требуются автоматическое масштабирование кластеров для обработки больших рабочих нагрузок или интерактивные панели мониторинга Databricks, вам потребуется уровень "Премиум" с соответствующими ценами.</span><span class="sxs-lookup"><span data-stu-id="0ace9-179">However, if your specific application requires automatically scaling clusters to handle larger workloads or interactive Databricks dashboards, the premium level could increase costs further.</span></span>

<span data-ttu-id="0ace9-180">Записные книжки этого решения можно запустить на любой платформе на базе Spark с минимальными изменениями — нужно лишь удалить специализированные пакеты для Databricks.</span><span class="sxs-lookup"><span data-stu-id="0ace9-180">The solution notebooks can run on any Spark-based platform with minimal edits to remove the Databricks-specific packages.</span></span> <span data-ttu-id="0ace9-181">Вы можете изучить аналогичные решения для других платформ Azure.</span><span class="sxs-lookup"><span data-stu-id="0ace9-181">See the following similar solutions for various Azure platforms:</span></span>

- <span data-ttu-id="0ace9-182">[Записная книжка Python для Студии машинного обучения Azure][python-aml]</span><span class="sxs-lookup"><span data-stu-id="0ace9-182">[Python on Azure Machine Learning Studio][python-aml]</span></span>
- <span data-ttu-id="0ace9-183">[Службы R в SQL Server][sql-r]</span><span class="sxs-lookup"><span data-stu-id="0ace9-183">[SQL Server R services][sql-r]</span></span>
- <span data-ttu-id="0ace9-184">[PySpark для Виртуальной машины для обработки и анализа данных в Azure][py-dvsm]</span><span class="sxs-lookup"><span data-stu-id="0ace9-184">[PySpark on an Azure Data Science Virtual Machine][py-dvsm]</span></span>

## <a name="deploy-the-solution"></a><span data-ttu-id="0ace9-185">Развертывание решения</span><span class="sxs-lookup"><span data-stu-id="0ace9-185">Deploy the solution</span></span>

<span data-ttu-id="0ace9-186">Для развертывания этой эталонной архитектуры выполните действия, описанные в репозитории  [GitHub][github], чтобы создать масштабируемое решение для оценки моделей Spark в Azure Databricks в пакетном режиме.</span><span class="sxs-lookup"><span data-stu-id="0ace9-186">To deploy this reference architecture, follow the steps described in the [GitHub][github] repository to build a scalable solution for scoring Spark models in batch on Azure Databricks.</span></span>

## <a name="related-architectures"></a><span data-ttu-id="0ace9-187">Связанные архитектуры</span><span class="sxs-lookup"><span data-stu-id="0ace9-187">Related architectures</span></span>

<span data-ttu-id="0ace9-188">Мы разработали эталонную архитектуру, которая использует Spark для создания [систем рекомендаций в режиме реального времени][recommendation] с автономными предварительно вычисленными оценками.</span><span class="sxs-lookup"><span data-stu-id="0ace9-188">We have also built a reference architecture that uses Spark for building [real-time recommendation systems][recommendation] with offline, pre-computed scores.</span></span> <span data-ttu-id="0ace9-189">Такие системы рекомендации — распространенный пример применения пакетной обработки для оценок.</span><span class="sxs-lookup"><span data-stu-id="0ace9-189">These recommendation systems are common scenarios where scores are batch-processed.</span></span>

[adf]: https://azure.microsoft.com/blog/operationalize-azure-databricks-notebooks-using-data-factory/
[ai-guide]: /azure/machine-learning/team-data-science-process/cortana-analytics-playbook-predictive-maintenance
[blob]: https://docs.databricks.com/spark/latest/data-sources/azure/azure-storage.html
[cli]: https://docs.databricks.com/user-guide/dev-tools/databricks-cli.html
[cluster]: https://docs.azuredatabricks.net/user-guide/clusters/sizing.html
[databricks]: /azure/azure-databricks/
[databricks-connect]: /azure/azure-databricks/databricks-connect-to-data-sources
[data-sources]: https://docs.databricks.com/spark/latest/data-sources/index.html
[github]: https://github.com/Azure/BatchSparkScoringPredictiveMaintenance
[job]: https://docs.databricks.com/user-guide/jobs.html
[log]: https://docs.databricks.com/user-guide/clusters/event-log.html
[metrics]: https://docs.databricks.com/user-guide/clusters/metrics.html
[mllib]: https://docs.databricks.com/spark/latest/mllib/index.html
[mllib-spark]: https://docs.databricks.com/spark/latest/mllib/index.html#apache-spark-mllib
[notebooks]: https://docs.databricks.com/user-guide/notebooks/index.html
[pricing]: https://azure.microsoft.com/en-us/pricing/details/databricks/
[python-aml]: https://gallery.azure.ai/Notebook/Predictive-Maintenance-Modelling-Guide-Python-Notebook-1
[py-dvsm]: https://gallery.azure.ai/Tutorial/Predictive-Maintenance-using-PySpark
[recommendation]: /azure/architecture/reference-architectures/ai/real-time-recommendation
[sql-r]: https://gallery.azure.ai/Tutorial/Predictive-Maintenance-Modeling-Guide-using-SQL-R-Services-1
[workspace]: https://docs.databricks.com/user-guide/workspace.html
